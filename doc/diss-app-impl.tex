\section{Analysis of cycle cancelling} \label{appendix:impl-cc-analysis}

\subsubsection{Correctness} \label{appendix:impl-cc-analysis:correctness}

I will show that, if the algorithm terminates, it produces the correct result. \\

\begin{lemma} \label{lemma:cycle-cancelling-invariant}
    Immediately before each iteration of the loop, $\mathbf{x}$ is a feasible solution.
\end{lemma} 
\begin{proof}
    For the base case, $\mathbf{x}$ is feasible after initialisation, by correctness of the maximum-flow algorithm used.
    
    For the inductive case, suppose $\mathbf{x}$ is feasible immediately prior to an iteration of the loop body. The body pushes flow along a cycle. This maintains feasibility: the excess at the vertices along the cycles remain zero, with any increase in the flow leaving a vertex being counterbalanced by an equal increase in the flow entering that vertex.
\end{proof}

\cccorrectness*
\begin{proof}
By \cref{lemma:cycle-cancelling-invariant}, $\mathbf{x}$ is a feasible solution upon termination. The algorithm only terminates when no negative-cost directed cycles exist. It follows by \cref{thm:optimality-neg-cycle} that $\mathbf{x}$ is optimal.
\end{proof}

\subsubsection{Termination and asymptotic complexity} \label{appendix:impl-cc-analysis:complexity}

I will now show that the algorithm always terminates, and provide a bound on its complexity.\\

\begin{lemma} \label{lemma:cycle-cancelling-termination}
    The algorithm terminates within $O(mCU)$ iterations\footnotemark.
    \footnotetext{See \cref{sec:prep-flow-complexity} for a definition of $m$, $C$, $U$ and other variables used in complexity analysis.}
\end{lemma}
\begin{proof}
    Each iteration of the algorithm identifies a cycle $w$, of cost $c < 0$, and pushes $\delta = \min_{(i,j) \in w} r_{ij}$ units of flow along the cycle. Note $\delta > 0$, otherwise the cycle would not exist in the residual network. 
    
    The objective function value changes by $c\delta$.  By \cref{assumption:integrality}, $c$ and $\delta$ must both be integral. So as $c < 0$ and $\delta > 0$, it follows $c \leq -1$ and $\delta \geq 1$ so $c\delta \leq -1$. That is, the cost is decreased by at least one unit each iteration.
    
    The number of iterations is thus bounded by cost of the initial feasible flow. $mCU$ is an upper bound on the cost of any flow, hence the result.
\end{proof}

\cccomplexity*
\begin{proof}
    Note that Bellman-Ford runs in $O(nm)$ time. Augmenting flow along the cycle is of cost linear in the length of the cycle, and so is certainly $O(n)$. Thus each iteration runs in $O(nm)$. By \cref{lemma:cycle-cancelling-termination}, it follows the complexity of the algorithm is $O(nm^2CU)$.
\end{proof}

\section{Successive shortest path} \label{appendix:impl-ssp}

\subsection{Terminating Djikstra's algorithm early} \label{appendix:impl-ssp:partial-djikstra}

Djikstra's algorithm is said to have \emph{permanently labelled} a vertex $i$ when it extracts $i$ from the heap. At this point, Djikstra has found a shortest path from $s$ to $i$.

I modified the successive shortest path algorithm to terminate Djikstra as soon as it permanently labels a deficit vertex $l$. Although this does not affect asymptotic complexity, it may considerably improve performance in practice.\\

\begin{lemma} \label{lemma:ssp-preserve-triangle}
    Define:
    \[
    d'_{i}=\begin{cases}
    d_{i} & \text{if $i$ permanently labelled}\\
    d_{l} & \text{otherwise}
    \end{cases}
    \]
    Suppose the triangle equality holds on $\mathbf{d}$, that is:
    \begin{equation} \label{eq:djikstra-triangle-assumption}
    \forall(i,j)\in E_{\mathbf{x}}\cdot d_j \leq d_i + c^{\boldsymbol{\pi}}_{ij}
    \end{equation}
    Then it also holds on $\mathbf{d}'$:
    \[\forall(i,j)\in E_{\mathbf{x}}\cdot d'_j \leq d'_i + c^{\boldsymbol{\pi}}_{ij}\]
\end{lemma}
\begin{proof}
    When Djikstra's algorithm is terminated early, the only shortest path distances known are those to permanently labelled vertex. But vertices are labelled in ascending order of their shortest path distance. As $l$ is permanently labelled, it follows that for any unlabelled vertex $i$:
    \begin{equation} \label{eq:ssp-djikstra-unlabelled}
    d_l \leq d_i
    \end{equation}
    But $l$ is the last vertex to be labelled, so it follows that for any permanently labelled vertex $i$:
    \begin{equation} \label{eq:ssp-djikstra-labelled}
    d_i \leq d_l
    \end{equation}
    
    Now, let $(i,j) \in E_{\mathbf{x}}$. It remains to prove $d'_j \leq d'_i + c^{\boldsymbol{\pi}}_{ij}$, for which there are four possible cases.
    
    \paragraph{$i$ and $j$ permanently labelled} $d'_i = d_i$ and $d'_j = d_j$, so result follows by \cref{eq:djikstra-triangle-assumption}.
    
    \paragraph{$i$ and $j$ not labelled} $d'_i = d_l = d'_j$, so result follows by non-negativity of reduced costs $c^{\boldsymbol{\pi}}_{ij}$.
    
    \paragraph{$i$ permanently labelled, $j$ not} $d'_j = d_l$ by definition, and $d_l \leq d_j$ by \cref{eq:ssp-djikstra-unlabelled}, so $d'_j \leq d_j$. By definition $d'_i = d_i$, so it follows by \cref{eq:djikstra-triangle-assumption} that $d'_j \leq d'_i + c^{\boldsymbol{\pi}}_{ij}$.
    
    \paragraph{$i$ not labelled, $j$ permanently labelled} By definition, $d'_j = d_j$. By \cref{eq:ssp-djikstra-labelled}, $d_j \leq d_l$, so $d'_j \leq d_l$. By definition, $d'_i = d_l$, so $d'_j \leq d'_i$. Result follows by non-negativity of $c^{\boldsymbol{\pi}}_{ij}$.
\end{proof}

\begin{lemma}
    Recall \cref{lemma:ssp-reduced-costs}. Let us redefine:
    {\normalfont
        \[\boldsymbol{\pi}'_{i}=\begin{cases}
        \boldsymbol{\pi}_{i}-d_{i} & \textrm{if $i$ permanently labelled;}\\
        \boldsymbol{\pi}_{i}-d_{l} & \textrm{otherwise.}
        \end{cases}\]}\noindent
    The original result (a) still holds. The result (b) holds along the shortest path from $s$ to $l$\footnotemark.
    \footnotetext{Note that this is all that is needed for the correctness of the algorithm, as this is the only path along which we augment flow.}
\end{lemma}
\begin{proof}
    The original proof for the lemma~\cite[lemma~9.11]{Ahuja:1993} uses the triangle inequality stated in \cref{eq:djikstra-triangle-assumption}. 
    
    By \cref{lemma:ssp-preserve-triangle}, $\mathbf{d}'$ also satisfies the triangle equality. The original proof for (a) thus still holds, as it makes no further assumptions on $\mathbf{d}'$.
    
    As for (b), every vertex $i$ along the shortest path from $s$ to $l$ has been permanently labelled, and so $d'_i = d_i$. Hence the original proof still holds along this path.
\end{proof}

Any constant shift in the potential for every vertex will leave reduced costs unchanged, so $\boldsymbol{\pi}'$ may equivalently be defined as:

\[\boldsymbol{\pi}'_{i}=\begin{cases}
\boldsymbol{\pi}_{i}-d_i+d_l & \text{if $i$ permanently labelled;}\\
\boldsymbol{\pi}_{i} & \text{otherwise.}
\end{cases}\]

This is computationally more efficient, as it reduces the number of elements of $\boldsymbol{\pi}$ that must be updated.

\subsection{Correctness analysis} \label{appendix:impl-ssp:analysis-correctness}

\Cref{lemma:ssp-reduced-costs,cor:ssp-reduced-costs} prove properties of the algorithm, allowing \cref{thm:ssp-invariant} to show that reduced cost optimality is maintained as an invariant. Correctness of the algorithm follows in \cref{cor:ssp-correctness} by using the terminating condition of the algorithm.\\

\begin{lemma} \label{lemma:ssp-reduced-costs}
    Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions of \cref{eq:optimality-reduced-cost} with respect to potentials $\boldsymbol{\pi}$. Let $\mathbf{d}$ represent the shortest path distances in the residual network $G_{\mathbf{x}}$ from a vertex $s \in V$ to all other vertices, with respect to the reduced costs $c^{\boldsymbol{\pi}}_{ij}$. Then:
    
    \begin{enumerate}[label=(\alph*)]
        \item $\mathbf{x}$ also satisfies reduced cost optimality conditions with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
        \item The reduced costs $c^{\boldsymbol{\pi}'}_{ij}$ are zero for all arcs $(i,j)$ in the shortest-path tree rooted at $s \in V$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    See Ahuja, \textit{et al.}~\cite[lemma~9.11]{Ahuja:1993}.
\end{proof}

\begin{cor} \label{cor:ssp-reduced-costs}
    Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions, with respect to some potentials $\boldsymbol{\pi}$. Let $\mathbf{x}'$ denote the pseudoflow obtained from $\mathbf{x}$ by sending flow along a shortest path from vertex $s$ to some other vertex $k \in V$. Then $\mathbf{x}'$ also satisfies the reduced cost optimality conditions, with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
\end{cor}
\begin{proof} (Adapted from Ahuja, \textit{et al.}~\cite[lemma~9.12]{Ahuja:1993})
    
    By \cref{lemma:ssp-reduced-costs}(a), $\left(\mathbf{x},\boldsymbol{\pi'}\right)$ satisfies the reduced cost optimality conditions.
    
    Pushing flow along an arc $(i,j) \in G_{\mathbf{x}}$ might add its reversal $(j,i)$ to the residual network. Let $P$ be a shortest path from $s$ to $k$. By \cref{lemma:ssp-reduced-costs}(b), it follows that any arc $(i,j) \in P$ has $c^{\boldsymbol{\pi}'}_{ij} = 0$. So $c^{\boldsymbol{\pi}'}_{ji} = 0$. Thus any arcs are added to the residual network by augmenting flow along $P$ have a zero reduced cost, and so still satisfy the reduced cost optimality conditions of \cref{eq:optimality-reduced-cost}.
\end{proof}

\sspinvariant*
\begin{proof} (Induction)
    
    For the base case, note that $(\mathbf{0},\mathbf{0})$ satisfies reduced cost optimality. $G_{\boldsymbol{0}} = G$ holds, i.e.\ the residual and original network are the same. Moreover, all arc costs $c_{ij}$ are non-negative (by \cref{assumption:non-negative-arc-costs}) and so the reduced costs $c^{\boldsymbol{0}}_{ij}=c_{ij}$ are also non-negative. Thus reduced cost optimality holds.
    
    Now, assume that reduced cost optimality holds immediately prior to execution of the loop body. The body computes the shortest path distances $\mathbf{d}$ from a vertex $s$, and updates $\boldsymbol{\pi}$ to become $\boldsymbol{\pi'}$ as defined in \cref{lemma:ssp-reduced-costs}. It then pushes flow along a shortest path from $s$ to another vertex, yielding a new flow of the same form as $\mathbf{x'}$ in \cref{cor:ssp-reduced-costs}. It follows by \cref{cor:ssp-reduced-costs} that $(\mathbf{x}',\boldsymbol{\pi}')$ satisfies reduced cost optimality at the end of the loop body. Hence, the inductive hypothesis continues to hold.
\end{proof}

\sspcorrectness*
\begin{proof}
    The algorithm terminates when the mass balance constraints of \cref{eq:mass-balance-constraints} are satisfied. At this point, the solution $\mathbf{x}$ is feasible (see \cref{sec:prep-flow-pseudo}). 
    
    By \cref{thm:ssp-invariant}, we know the algorithm maintains the invariant that $\mathbf{x}$ satisfies reduced cost optimality. 
    
    It follows that $\mathbf{x}$ is both optimal and a feasible flow upon termination, so $\mathbf{x}$ is a solution to the minimum-cost flow problem.
\end{proof}

\section{Heuristics for cost scaling} \label{appendix:impl-csheuristics}
% Proofread: 1 minor edits
Goldberg has proposed a number of heuristics to improve the real-world performance of his cost scaling algorithm~\cite{Goldberg:1997}, described in \cref{sec:impl-cost-scaling}. These have been found to result in considerable real-world improvements in efficiency~\cite{Bunnagel:1998,KiralyKovacs:2012}. Note that their effectiveness depends on the problem instance. Moreover, several of the heuristics such as arc fixing and potential update are highly sensitive to parameter settings or other implementation choices.

\subsection{Potential refinement} \label{appendix:impl-csheuristics-potential-refinement}
The $\textproc{Refine}(\mathbf{x},\boldsymbol{\pi},\epsilon)$ routine is guaranteed to produce an $\epsilon$-optimal flow. However, it may also be $\epsilon'$-optimal for $\epsilon' < \epsilon$. This heuristic decreases $\epsilon$ while modifying the potentials $\boldsymbol{\pi}$, without changing the flow $\mathbf{x}$. This has been found to yield a 40\% performance improvement~\cite{Bunnagel:1998}.

\subsection{Push lookahead}
Before performing $\textproc{Push}(i,j)$, this heuristic checks whether $j$ is a deficit vertex ($e_j < 0$) or if $j$ has an outgoing admissible arc. If so, the \textproc{Push} operation proceeds.

Otherwise, the \textproc{Push} operation is aborted, and \textproc{Relabel} is performed instead. Were the \textproc{Push} operation to be performed, the flow would get `stuck' at vertex $j$, and end up being pushed back to $i$. This heuristic has been found to significantly reduce the number of \textproc{Push} operations performed~\cite{Goldberg:1997}.

\subsection{Arc fixing}
The algorithm examines a large number of arcs, sometimes unnecessarily. It can be proved that for any arc $(i,j)$ with reduced cost satisfying $\left|c^{\boldsymbol{\pi}}_{ij}\right| > 2n\epsilon$, the flow $x_{ij}$ is never again modified. These arcs can safely be \emph{fixed}: removed from the adjacency list examined by the algorithm. 

This heuristic takes things a step further, \emph{speculatively} fixing arcs whenever $\left|c^{\boldsymbol{\pi}}_{ij}\right|$ is above some threshold $\beta$. Speculatively fixed arcs may still need to have their flow updated, but this is unlikely. Consequently, the algorithm examines these arcs very infrequently, unfixing arcs found to violate reduced cost optimality conditions.

\subsection{Potential update}
The \textproc{Relabel} operation given in \cref{algo:cost-scaling-operations} updates the potential at a single vertex. This heuristic is based around an alternative \emph{set relabel} operation, which updates potentials at many vertices at once.

Regular relabel operations are still used, with set relabel called periodically. The optimum frequency is dependent on both the problem and implementation, but calling set relabel every $O(n)$ regular relabels has been found to be a good rule of thumb~\cite{Goldberg:1997}.

This heuristic improves performance when used on its own, but has little effect when used in conjunction with potential refinement and push lookahead described above. In fact, it may even \emph{harm} performance in some cases, although it has been found to be more beneficial the larger the network instance~\cite{Bunnagel:1998}.