\section{Cycle cancelling} \label{appendix:impl-cc}

\subsection{Algorithm description}

Cycle cancelling iteratively reduces the cost of the solution by sending flow along negative-cost cycles. This is inspired by the negative cycle optimality conditions, stated in \cref{thm:optimality-neg-cycle}:\\

\optimalitynegcycle*

\begin{algorithm}
    \caption{Cycle cancelling}
    \label{algo:cycle-cancelling}
    \begin{algorithmic}[1]
        \State $\mathbf{x}\gets $ result of maximum-flow algorithm \Comment{establishes x feasible}
        \While{$G_\mathbf{x}$ contains a negative cost cycle}
        \State identify negative cost cycle $W$ \Comment{e.g. using Bellman-Ford}
        \Let{$\delta$}{$\min_{(i,j) \in W} r_{ij}$}
        \State augment $\delta$ units of flow along cycle $W$
        \EndWhile{}
    \end{algorithmic}
\end{algorithm}

The algorithm is initialised with a feasible flow $\mathbf{x}$. This may be found by any maximum-flow algorithm, such as Ford-Fulkerson~\cite{FordFulkerson:1956}. The feasibility of the solution $\mathbf{x}$ is maintained at all times, and its cost is successively reduced. 

Each iteration of the algorithm identifies a directed cycle of negative cost in the residual network $G_\mathbf{x}$. Note negative cycles can occur despite \cref{assumption:non-negative-arc-costs} of non-negative arc costs, as reverse arcs in the residual network have the opposite sign to forward arcs.

Negative cost cycles are \emph{cancelled} by pushing as much flow as possible along the cycle. This causes it to `drop out' of the residual network: one or more arcs along the cycle become saturated, and so are no longer present in the residual network. Sending flow along a negative cost cycle reduces the cost of the solution, bringing it closer to optimality. The algorithm terminates when no negative cost directed cycles remain, guaranteeing optimality by \cref{thm:optimality-neg-cycle}.

%SOMEDAY: Difficult to justify, better way of saying it?
Note this generic version of the algorithm does not specify \emph{how} negative cycles are to be identified. I used the well-known Bellman-Ford~\cite[p.~651]{CLRS:2009} algorithm, although other more efficient methods exist. However, cycle cancelling was only implemented in order to have a known-working algorithm early on in development. Even the fastest variants are too slow for practical purposes.

\subsubsection{Correctness} \label{appendix:impl-cc-analysis:correctness}

I will show that, if the algorithm terminates, it produces the correct result. \\

\begin{lemma} \label{lemma:cycle-cancelling-invariant}
    Immediately before each iteration of the loop, $\mathbf{x}$ is a feasible solution.
\end{lemma} 
\begin{proof}
    For the base case, $\mathbf{x}$ is feasible after initialisation, by correctness of the maximum-flow algorithm used.
    
    For the inductive case, suppose $\mathbf{x}$ is feasible immediately prior to an iteration of the loop body. The body pushes flow along a cycle. This maintains feasibility: the excess at the vertices along the cycles remain zero, with any increase in the flow leaving a vertex being counterbalanced by an equal increase in the flow entering that vertex.
\end{proof}

\begin{thm}[Correctness of cycle cancelling]
    \label{thm:cycle-cancelling-correctness}
    Upon termination, $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{thm}
\begin{proof}
    By \cref{lemma:cycle-cancelling-invariant}, $\mathbf{x}$ is a feasible solution upon termination. The algorithm only terminates when no negative-cost directed cycles exist. It follows by \cref{thm:optimality-neg-cycle} that $\mathbf{x}$ is optimal.
\end{proof}

\subsubsection{Termination and asymptotic complexity} \label{appendix:impl-cc-analysis:complexity}

I will now show that the algorithm always terminates, and provide a bound on its complexity.\\

\begin{lemma} \label{lemma:cycle-cancelling-termination}
    The algorithm terminates within $O(mCU)$ iterations\footnotemark.
    \footnotetext{See \cref{sec:prep-flow-complexity} for a definition of $m$, $C$, $U$ and other variables used in complexity analysis.}
\end{lemma}
\begin{proof}
    Each iteration of the algorithm identifies a cycle $w$, of cost $c < 0$, and pushes $\delta = \min_{(i,j) \in w} r_{ij}$ units of flow along the cycle. Note $\delta > 0$, otherwise the cycle would not exist in the residual network. 
    
    The objective function value changes by $c\delta$.  By \cref{assumption:integrality}, $c$ and $\delta$ must both be integral. So as $c < 0$ and $\delta > 0$, it follows $c \leq -1$ and $\delta \geq 1$ so $c\delta \leq -1$. That is, the cost is decreased by at least one unit each iteration.
    
    The number of iterations is thus bounded by cost of the initial feasible flow. $mCU$ is an upper bound on the cost of any flow, hence the result.
\end{proof}

\begin{thm}[Asymptotic complexity of cycle cancelling]
    \label{thm:cycle-cancelling-complexity}
    The asymptotic complexity is $O(nm^2CU)$.
\end{thm}
\begin{proof}
    Note that Bellman-Ford runs in $O(nm)$ time. Augmenting flow along the cycle is of cost linear in the length of the cycle, and so is certainly $O(n)$. Thus each iteration runs in $O(nm)$. By \cref{lemma:cycle-cancelling-termination}, it follows the complexity of the algorithm is $O(nm^2CU)$.
\end{proof}

\begin{remark}
    On networks produced by flow schedulers, the asymptotic complexity is $O(n^3CU)$ by \cref{lemma:network-num-arcs}.
\end{remark}

\section{Successive shortest path} \label{appendix:impl-ssp}

\subsection{Correctness analysis} \label{appendix:impl-ssp:analysis-correctness}

\Cref{lemma:ssp-reduced-costs,cor:ssp-reduced-costs} prove properties of the algorithm, allowing \cref{thm:ssp-invariant} to show that reduced cost optimality is maintained as an invariant. Correctness of the algorithm follows in \cref{cor:ssp-correctness} by using the terminating condition of the algorithm.\\

\begin{lemma} \label{lemma:ssp-reduced-costs}
    Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions of \cref{eq:optimality-reduced-cost} with respect to potentials $\boldsymbol{\pi}$. Let $\mathbf{d}$ represent the shortest path distances in the residual network $G_{\mathbf{x}}$ from a vertex $s \in V$ to all other vertices, with respect to the reduced costs $c^{\boldsymbol{\pi}}_{ij}$. Then:
    
    \begin{enumerate}[label=(\alph*)]
        \item $\mathbf{x}$ also satisfies reduced cost optimality conditions with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
        \item The reduced costs $c^{\boldsymbol{\pi}'}_{ij}$ are zero for all arcs $(i,j)$ in the shortest-path tree rooted at $s \in V$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    See Ahuja, \textit{et al.}~\cite[lemma~9.11]{Ahuja:1993}.
\end{proof}

\begin{cor} \label{cor:ssp-reduced-costs}
    Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions, with respect to some potentials $\boldsymbol{\pi}$. Let $\mathbf{x}'$ denote the pseudoflow obtained from $\mathbf{x}$ by sending flow along a shortest path from vertex $s$ to some other vertex $k \in V$. Then $\mathbf{x}'$ also satisfies the reduced cost optimality conditions, with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
\end{cor}
\begin{proof} (Adapted from Ahuja, \textit{et al.}~\cite[lemma~9.12]{Ahuja:1993})
    
    By \cref{lemma:ssp-reduced-costs}(a), $\left(\mathbf{x},\boldsymbol{\pi'}\right)$ satisfies the reduced cost optimality conditions.
    
    Pushing flow along an arc $(i,j) \in G_{\mathbf{x}}$ might add its reversal $(j,i)$ to the residual network. Let $P$ be a shortest path from $s$ to $k$. By \cref{lemma:ssp-reduced-costs}(b), it follows that any arc $(i,j) \in P$ has $c^{\boldsymbol{\pi}'}_{ij} = 0$. So $c^{\boldsymbol{\pi}'}_{ji} = 0$. Thus any arcs are added to the residual network by augmenting flow along $P$ have a zero reduced cost, and so still satisfy the reduced cost optimality conditions of \cref{eq:optimality-reduced-cost}.
\end{proof}

\sspinvariant*
\begin{proof} (Induction)
    
    For the base case, note that $(\mathbf{0},\mathbf{0})$ satisfies reduced cost optimality. $G_{\boldsymbol{0}} = G$ holds, i.e.\ the residual and original network are the same. Moreover, all arc costs $c_{ij}$ are non-negative (by \cref{assumption:non-negative-arc-costs}) and so the reduced costs $c^{\boldsymbol{0}}_{ij}=c_{ij}$ are also non-negative. Thus reduced cost optimality holds.
    
    Now, assume that reduced cost optimality holds immediately prior to execution of the loop body. The body computes the shortest path distances $\mathbf{d}$ from a vertex $s$, and updates $\boldsymbol{\pi}$ to become $\boldsymbol{\pi'}$ as defined in \cref{lemma:ssp-reduced-costs}. It then pushes flow along a shortest path from $s$ to another vertex, yielding a new flow of the same form as $\mathbf{x'}$ in \cref{cor:ssp-reduced-costs}. It follows by \cref{cor:ssp-reduced-costs} that $(\mathbf{x}',\boldsymbol{\pi}')$ satisfies reduced cost optimality at the end of the loop body. Hence, the inductive hypothesis continues to hold.
\end{proof}

\sspcorrectness*
\begin{proof}
    The algorithm terminates when the mass balance constraints of \cref{eq:mass-balance-constraints} are satisfied. At this point, the solution $\mathbf{x}$ is feasible (see \cref{sec:prep-flow-pseudo}). 
    
    By \cref{thm:ssp-invariant}, we know the algorithm maintains the invariant that $\mathbf{x}$ satisfies reduced cost optimality. 
    
    It follows that $\mathbf{x}$ is both optimal and a feasible flow upon termination, so $\mathbf{x}$ is a solution to the minimum-cost flow problem.
\end{proof}

\subsection{Terminating Djikstra's algorithm early} \label{appendix:impl-ssp:partial-djikstra}

Djikstra's algorithm is said to have \emph{permanently labelled} a vertex $i$ when it extracts $i$ from the heap. At this point, Djikstra has found a shortest path from $s$ to $i$.

I modified the successive shortest path algorithm to terminate Djikstra as soon as it permanently labels a deficit vertex $l$. Although this does not affect asymptotic complexity, it may considerably improve performance in practice.\\

\begin{lemma} \label{lemma:ssp-preserve-triangle}
    Define:
    \[
    d'_{i}=\begin{cases}
    d_{i} & \text{if $i$ permanently labelled}\\
    d_{l} & \text{otherwise}
    \end{cases}
    \]
    Suppose the triangle equality holds on $\mathbf{d}$, that is:
    \begin{equation} \label{eq:djikstra-triangle-assumption}
    \forall(i,j)\in E_{\mathbf{x}}\cdot d_j \leq d_i + c^{\boldsymbol{\pi}}_{ij}
    \end{equation}
    Then it also holds on $\mathbf{d}'$:
    \[\forall(i,j)\in E_{\mathbf{x}}\cdot d'_j \leq d'_i + c^{\boldsymbol{\pi}}_{ij}\]
\end{lemma}
\begin{proof}
    When Djikstra's algorithm is terminated early, the only shortest path distances known are those to permanently labelled vertex. But vertices are labelled in ascending order of their shortest path distance. As $l$ is permanently labelled, it follows that for any unlabelled vertex $i$:
    \begin{equation} \label{eq:ssp-djikstra-unlabelled}
    d_l \leq d_i
    \end{equation}
    But $l$ is the last vertex to be labelled, so it follows that for any permanently labelled vertex $i$:
    \begin{equation} \label{eq:ssp-djikstra-labelled}
    d_i \leq d_l
    \end{equation}
    
    Now, let $(i,j) \in E_{\mathbf{x}}$. It remains to prove $d'_j \leq d'_i + c^{\boldsymbol{\pi}}_{ij}$, for which there are four possible cases.
    
    \paragraph{$i$ and $j$ permanently labelled} $d'_i = d_i$ and $d'_j = d_j$, so result follows by \cref{eq:djikstra-triangle-assumption}.
    
    \paragraph{$i$ and $j$ not labelled} $d'_i = d_l = d'_j$, so result follows by non-negativity of reduced costs $c^{\boldsymbol{\pi}}_{ij}$.
    
    \paragraph{$i$ permanently labelled, $j$ not} $d'_j = d_l$ by definition, and $d_l \leq d_j$ by \cref{eq:ssp-djikstra-unlabelled}, so $d'_j \leq d_j$. By definition $d'_i = d_i$, so it follows by \cref{eq:djikstra-triangle-assumption} that $d'_j \leq d'_i + c^{\boldsymbol{\pi}}_{ij}$.
    
    \paragraph{$i$ not labelled, $j$ permanently labelled} By definition, $d'_j = d_j$. By \cref{eq:ssp-djikstra-labelled}, $d_j \leq d_l$, so $d'_j \leq d_l$. By definition, $d'_i = d_l$, so $d'_j \leq d'_i$. Result follows by non-negativity of $c^{\boldsymbol{\pi}}_{ij}$.
\end{proof}

\begin{lemma}
    Recall \cref{lemma:ssp-reduced-costs}. Let us redefine:
    {\normalfont
        \[\boldsymbol{\pi}'_{i}=\begin{cases}
        \boldsymbol{\pi}_{i}-d_{i} & \textrm{if $i$ permanently labelled;}\\
        \boldsymbol{\pi}_{i}-d_{l} & \textrm{otherwise.}
        \end{cases}\]}\noindent
    The original result (a) still holds. The result (b) holds along the shortest path from $s$ to $l$\footnotemark.
    \footnotetext{Note that this is all that is needed for the correctness of the algorithm, as this is the only path along which we augment flow.}
\end{lemma}
\begin{proof}
    The original proof for the lemma~\cite[lemma~9.11]{Ahuja:1993} uses the triangle inequality stated in \cref{eq:djikstra-triangle-assumption}. 
    
    By \cref{lemma:ssp-preserve-triangle}, $\mathbf{d}'$ also satisfies the triangle equality. The original proof for (a) thus still holds, as it makes no further assumptions on $\mathbf{d}'$.
    
    As for (b), every vertex $i$ along the shortest path from $s$ to $l$ has been permanently labelled, and so $d'_i = d_i$. Hence the original proof still holds along this path.
\end{proof}

Any constant shift in the potential for every vertex will leave reduced costs unchanged, so $\boldsymbol{\pi}'$ may equivalently be defined as:

\[\boldsymbol{\pi}'_{i}=\begin{cases}
\boldsymbol{\pi}_{i}-d_i+d_l & \text{if $i$ permanently labelled;}\\
\boldsymbol{\pi}_{i} & \text{otherwise.}
\end{cases}\]

This is computationally more efficient, as it reduces the number of elements of $\boldsymbol{\pi}$ that must be updated.



\section{Correctness analysis of relaxation} \label{appendix:impl-relaxation-correctness}

First, I show \textproc{UpdatePotentials} and \textproc{AugmentFlow} preserve reduced cost optimality. Using these lemmas, I show that reduced cost optimality is maintained as an invariant throughout the algorithm. It follows that, if the algorithm terminates, then $\mathbf{x}$ is a solution to the minimum-cost flow problem. I also prove some other useful properties of \textproc{UpdatePotentials} and \textproc{AugmentFlow}.\\

% SOMEDAY: This proof is a little bit confusing. You're talking about sums inside a minimisation. Also, you're saying w(pi) is unchanged by changing x: well, of course it is, it has to be as w isn't a function of x! I know wht you mean, but this could perhaps be restated to be explicit. 
\begin{lemma}[Correctness of \textproc{UpdatePotentials}] \label{lemma:relax-correctness-updatepotentials}
    Let $e(S) > r(\boldsymbol{\pi},S)$. Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality conditions. Then, after executing $\textproc{UpdatePotentials}$, the new pseudoflow and vertex potentials $\left(\mathbf{x},\boldsymbol{\pi}\right)$ continue to satisfy reduced cost optimality conditions, and $w(\boldsymbol{\pi}') > w(\boldsymbol{\pi})$.
\end{lemma}
\begin{proof} (Adapted from Ahuja, \textit{et al.}~\cite[p.~334]{Ahuja:1993})
    
    Arcs with a reduced cost of zero are saturated by \crefrange{algo:relaxation-update-potentials:saturate-loop}{algo:relaxation-update-potentials:saturate-cmd}, and drop out of the residual network as a result. This preserves reduced cost optimality: the flow on arcs with a reduced cost of zero is arbitrary. Moreover, it leaves $w(\boldsymbol{\pi})$ unchanged. Recall the formulation of $w(\boldsymbol{\pi})$ given in \cref{eq:relax-obj-fun-balance}:
    \[w(\boldsymbol{\pi})=\min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}^{\boldsymbol{\pi}}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}\right]\]
    The second sum is unchanged, as potentials $\boldsymbol{\pi}$ are unchanged. The first sum is also unchanged, as $x'_{ij}$ differs from $x_{ij}$ only when the reduced costs are $c_{ij}^{\boldsymbol{\pi}}=0$.
    
    Note that there is now $r(\boldsymbol{\pi},S)$ more flow leaving vertices in $S$, so the tree excess is now:
    \[e'(S) = e(S) - r(\boldsymbol{\pi},S)\]
    By the precondition, $e'(S)$ is (strictly) positive.
    
    After \crefrange{algo:relaxation-update-potentials:saturate-loop}{algo:relaxation-update-potentials:saturate-cmd}, all arcs in the residual network crossing the cut $\left(S,\overline{S}\right)$ have positive reduced cost\footnotemark. Let $\alpha > 0$ be the minimal such remaining reduced cost.
    \footnotetext{Since none had negative reduced cost, by assumption of reduced cost optimality on entering the procedure.}
    
    $\boldsymbol{\pi}'$ is now obtained from $\boldsymbol{\pi}$ by increasing the potential of every vertex $i \in S$ in the tree by $\alpha$. Recall the formulation of the objective function in terms of costs and excesses, given in \cref{eq:relax-obj-fun-excess}:
    \[w\left(\boldsymbol{\pi}\right)=\min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\boldsymbol{\pi}_{i}e_{i}\right]\]
    The first sum is unchanged: modifying the potentials $\boldsymbol{\pi}$ does not change the original arc cost $c_{ij}$ or flow $x_{ij}$. For the second sum:
    \begin{align*}
    \sum_{i\in V}\pi'_{i}e'_{i}= & \sum_{i\in S}\left(\pi_{i}+\alpha\right)e'_{i}+\sum_{i\in\overline{S}}\pi_{i}e'_{i}\\
    = &\:\alpha\sum_{i\in S}e'_{i}+\sum_{i\in V}\pi_{i}e'_{i}\\
    = &\:\alpha e'(S)+\sum_{i\in V}\pi_{i}e'_{i}
    \end{align*}
    Since $w\left(\boldsymbol{\pi}\right)$ is unchanged after updating $\mathbf{x}$, it follows:
    \[w\left(\boldsymbol{\pi}'\right)=w(\boldsymbol{\pi})+\alpha e'(S)\]
    $\alpha > 0$ and $e'(S) > 0$ have already been shown; by \cref{assumption:integrality}, it follows that $\alpha \geq 1 $ and $e'(S) \geq 1$ by. Thus $\alpha e'(S) \geq 1$, so $w\left(\boldsymbol{\pi}'\right) > w\left(\boldsymbol{\pi}\right)$.
    
    It remains to show that updating potentials maintains reduced cost optimality. Note that increasing the potentials of vertices in $S$ by $\alpha$ decreases the reduced cost of arcs in $\left(S,\overline{S}\right)$ by $\alpha$, increases the reduced cost of arcs in $\left(\overline{S},S\right)$ by $\alpha$ and leaves the reduced cost of other arcs unchanged.
    
    Prior to updating potentials, all arcs in the residual network had (strictly) positive reduced costs. Consequently, increasing the reduced cost cannot violate reduced cost optimality\footnotemark, but decreasing the reduced cost might. Consequently, before updating the potentials, $c_{ij}^{\boldsymbol{\pi}} \geq \alpha$ for all $(i,j)\in\left(S,\overline{S}\right)\:\mbox{with}\:r_{ij}>0$. Hence, after the potential update, $c_{ij}^{\boldsymbol{\pi}'}\geq 0$ for these arcs.
    \footnotetext{Note increasing the reduced cost from zero to a positive number could violate optimality, but this case has been excluded.}
\end{proof}

\begin{lemma}[Correctness of \textproc{AugmentFlow}] \label{lemma:relax-correctness-augmentflow}
    Let $e(S) \leq r(\boldsymbol{\pi},S)$, and $e_t < 0$. Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality conditions. Then, after executing \textproc{AugmentFlow}, there is a new pseudoflow $\mathbf{x}'$ and $\left(\mathbf{x}',\boldsymbol{\pi}\right)$ still satisfy reduced cost optimality conditions. Moreover, under $\mathbf{x}'$ the excess at vertex $s$ and deficit at vertex $t$ decreases, without changing the excess/deficit at any other vertex.
\end{lemma}
\begin{proof} (Adapted from Ahuja, \textit{et al.}~\cite[p.~336]{Ahuja:1993})
    
    \Cref{algo:relaxation-augment-flow:find-P} finds a shortest path $P$ from the excess vertex $s$ to a deficit vertex $t$. \Crefrange{algo:relaxation-augment-flow:compute-delta}{algo:relaxation-augment-flow:augment-path} then send as much flow as possible along path $P$, subject to:
    \begin{enumerate}
        \item satisfying the capacity constraints at each arc on $P$, and
        \item ensuring $e_s \geq 0$ and $e_t \leq 0$.
    \end{enumerate}
    The restriction on $e_s$ and $e_t$ ensures that the feasibility of the solution is improved, by ensuring the unmet supply/demmand $|e_i|$ monotonically decreases for all $i \in V$\footnotemark.
    \footnotetext{If the algorithm were allowed to `overshoot' and turn $s$ into a deficit vertex or $t$ into an excess, this property might not hold.}
    
    Since an equal amount of flow is sent on each arc in $P$, the excess at vertices other than the start $s$ and end $t$ of path $P$ are unchanged.
\end{proof}

\relaxcorrectness*
\begin{proof}
    For the same reason as given in \cref{thm:ssp-invariant}, the initial values of $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality conditions. \textproc{AugmentFlow} and \textproc{UpdatePotentials} are only invoked when their preconditions are satisfied, and so by \cref{lemma:relax-correctness-augmentflow,lemma:relax-correctness-updatepotentials} it follows that reduced cost optimality is maintained. Moreover, the main algorithm given in \cref{algo:relaxation} does not update $\mathbf{x}$ or $\boldsymbol{\pi}$ except via calls to $\textproc{AugmentFlows}$ and $\textproc{UpdatePotentials}$. So, reduced cost optimality is maintained as an invariant throughout the algorithm.
    
    The algorithm terminates when the mass balance constraints are satisfied, so $\mathbf{x}$ is feasible. Thus upon termination, $\mathbf{x}$ is feasible and satisfies reduced cost optimality conditions. Therefore, $\mathbf{x}$ is a solution to the minimum-cost flow problem. 
\end{proof}

\section{Cost scaling} \label{appendix:impl-cs}

\subsection{Correctness analysis} \label{appendix:impl-cs:correctness}

I show that \textproc{Push} and \textproc{Relabel} preserve $\epsilon$-optimality. I then prove the correctness of \textproc{Refine} from these results, with correctness of the cost scaling algorithm following as a corollary.\\

\begin{lemma}[Correctness of \textproc{Push}] \label{lemma:cost-scaling-push-correctness}
    Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ be $\epsilon$-optimal, and the precondition for $\textproc{Push}(i,j)$ hold: $e_i > 0$, $(i,j) \in E_{\mathbf{x}}$ and $c^{\boldsymbol{\pi}}_{ij} < 0$. Then $\left(\mathbf{x},\boldsymbol{\pi}\right)$ continues to be $\epsilon$-optimal after $\textproc{Push}$.
\end{lemma}
\begin{proof}
    $\textproc{Push}(i,j)$ increases the flow on arc $(i,j)$. By assumption, $(i,j)$ satisfied $\epsilon$-optimality prior to \textproc{Push}. $(i,j)$ may drop out of the residual network after increasing the flow, but this cannot violate optimality. If $(i,j)$ remains in the residual network, $(i,j)$ continues to satisfy $\epsilon$-optimality, since the reduced cost is unchanged.
    
    However, sending flow along $(i,j)$ could add arc $(j,i)$ to the residual network. By the precondition $c^{\boldsymbol{\pi}}_{ij} < 0$, it follows $c^{\boldsymbol{\pi}}_{ji} > 0 \geq -\epsilon$. Thus $(j,i)$ satisfies $\epsilon$-optimality.
    
    No other changes are made which could affect the $\epsilon$-optimality conditions given in \cref{eq:epsilon-optimality}, so \textproc{Push} preserves $\epsilon$-optimality.
\end{proof}

\begin{lemma}[Correctness of \textproc{Relabel}] \label{lemma:cost-scaling-relabel-correctness}
    Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ be $\epsilon$-optimal, and the precondition for $\textproc{Relabel}(i)$ hold: $e_i > 0$ and $\forall(i,j) \in E_{\mathbf{x}} \cdot c^{\boldsymbol{\pi}}_{ij} \geq 0$.
    
    Let $\boldsymbol{\pi}'$ denote the potentials after \textproc{Relabel}. Then $\pi'_i\ \geq \pi_i + \epsilon$ and $\pi'_j = \pi_j$ for $j \neq i$. Moreover, $\left(\mathbf{x},\boldsymbol{\pi}'\right)$ continues to be $\epsilon$-optimal.
\end{lemma}
\begin{proof} (Adapted from Goldberg and Tarjan~\cite[lemma~5.2]{Goldberg:1987})
    
    By the precondition, $\forall(i,j) \in E_{\mathbf{x}} \cdot c^{\boldsymbol{\pi}}_{ij} \geq 0$. Substituting for the definition of reduced costs in \cref{eq:reduced-costs} gives $\forall(i,j) \in E_{\mathbf{x}} \cdot c_{ij} + \pi_j \geq \pi_i$. Thus:
    \[\pi'_i = \min \set{\pi_j + c_{ij} + \epsilon | (i,j) \in E_{\mathbf{x}}} \geq \pi_i + \epsilon\]
    \textproc{Relabel} does not modify any other components of $\boldsymbol{\pi}$, so $\pi'_j = \pi_j$ for $j \neq i$.
    
    Increasing $\pi_i$ has the effect of decreasing the reduced cost $c^{\boldsymbol{\pi}}_{ij}$ of outgoing arcs $(i,j)$, increasing $c^{\boldsymbol{\pi}}_{ji}$ for incoming arcs $(j,i)$ and leaving the reduced cost of other arcs unchanged. Increasing $c^{\boldsymbol{\pi}}_{ji}$ cannot violate $\epsilon$-optimality, but decreasing $c^{\boldsymbol{\pi}}_{ij}$ might. However, for any $(v,w) \in E_{\mathbf{x}}$:
    \[c_{vw} + \pi_w - \min \left\{\pi_j + c_{ij} \::\: (i,j) \in E_{\mathbf{x}}\right\} \geq 0\]
    Thus by definition of $\boldsymbol{\pi}'$, and taking out the constant $\epsilon$:
    \[c_{vw} + \pi_w - \pi'_i \geq -\epsilon\]
    And so $c_{vw}^{\boldsymbol{\pi}'} \geq -\epsilon$, as required for $\epsilon$-optimality.
\end{proof}

\begin{lemma}[Correctness of \textproc{Refine}]
    \label{lemma:cost-scaling-refine-correctness}
    Let the precondition for $\textproc{Refine}$ hold: $\mathbf{x}$ is a pseudoflow. Then upon termination of \textproc{Refine}, the postcondition holds: $\left(\mathbf{x},\boldsymbol{\pi}\right)$ is $\epsilon$-optimal.
\end{lemma}
\begin{proof} (Adapted from Goldberg and Tarjan~\cite[theorem~5.4]{Goldberg:1987})
    The initial flow after \crefrange{algo:cost-scaling-generic-refine:start-init}{algo:cost-scaling-generic-refine:end-init} of \cref{algo:cost-scaling-generic-refine} is $0$-optimal (and so certainly $\epsilon$-optimal). $\epsilon$-optimality is preserved by subsequent \textproc{Push} and \textproc{Relabel} operations by \cref{lemma:cost-scaling-push-correctness,lemma:cost-scaling-relabel-correctness}. Hence $\epsilon$-optimality is maintained as an invariant.
    
    Given that \textproc{Refine} has terminated, the mass balance constraints must be satisfied by the loop condition on \cref{algo:cost-scaling-generic-refine:start-loop}. Thus $\mathbf{x}$ must also be a flow upon termination.
\end{proof}

\costscalingcorrectness*
\begin{proof}
    After each iteration of the algorithm, $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfies $\epsilon$-optimality by \cref{lemma:cost-scaling-refine-correctness}. Upon termination, $\epsilon < 1/n$. So by \cref{thm:epsilon-optimality-optimal}, $\mathbf{x}$ is an optimal solution.
\end{proof}

\subsection{Termination and complexity analysis} \label{appendix:impl-cs:analysis}

\begin{lemma} \label{lemma:cost-scaling-operations-complexity}
    The basic operations given in \cref{algo:cost-scaling-operations} have the following time complexities:
    \begin{enumerate}[label=(\alph*)]
        \item $\textproc{Push}(i,j)$ runs in $O(1)$ time.
        \item $\textproc{Relabel}(i)$ runs in $O\left(\left|\mathrm{Adj}(i)\right|\right)$ time, that is linear in the number of adjacent arcs.
    \end{enumerate}
\end{lemma}
\begin{proof}
    Immediate from inspection of \cref{algo:cost-scaling-operations}.
\end{proof}

\begin{defn}
    An invocation of $\textproc{Push}(i,j)$ is said to be \emph{saturating} if $r_{ij} = 0$ after the operation; otherwise, it is nonsaturating.
\end{defn}

\begin{remark}
    Note a push operation is saturating if and only if $e_i \geq r_{ij}$ prior to calling \textproc{Push}.\\
\end{remark}

\begin{lemma} \label{lemma:cost-scaling-number-operations}
    Within an invocation of \textproc{Refine}, the following upper bounds apply to the number of times each basic operation is performed:
    \begin{enumerate}[label=(\alph*)]
        \item $O(n^2)$ \textproc{Relabel} operations.
        \item $O(nm)$ saturating \textproc{Push} operations.
        \item $O(n^2m)$ nonsaturating \textproc{Push} operations.
    \end{enumerate}
\end{lemma}
\begin{proof}
    See Goldberg and Tarjan~\cite[lemma~6.3, lemma~6.4, lemma~6.7]{Goldberg:1987}.
\end{proof}

\costscalingrefinecomplexity*
\begin{proof}
    Immediate from \cref{lemma:cost-scaling-operations-complexity,lemma:cost-scaling-number-operations}.
    %The result follows by \cref{lemma:cost-scaling-operations-complexity,lemma:cost-scaling-number-operations}. \textproc{Relabel} is executed at most $O(n^2)$ times, with each operation having a cost of $O\left(\left|\mathrm{Adj}(i)\right|\right) = O(n)$, contributing a total cost of $O(n^3)$. There are up to $O(nm)$ saturating and $O(n^2m)$ nonsaturating \textproc{Push} operations. Each has a cost of $O(1)$, and so \textproc{Push} contributes a total cost of $O(n^2m)$. Hence the basic operations executed by \textproc{Refine} contribute a complexity of $O(n^3 + n^2m) = O(n^2m)$.
\end{proof}

\begin{remark}
    It is not possible to prove a bound on the complexity of the generic refine routine given in \cref{algo:cost-scaling-generic-refine}, as it depends on how vertices and arcs are chosen on how excess vertices are selected on \cref{algo:cost-scaling-generic-refine:select-vertex,algo:cost-scaling-generic-refine:call-push}. Bounds on specific implementations of refine are given in \cref{sec:impl-cost-scaling-implementations}.\\
\end{remark}

\costscalinggenericcomplexity*
\begin{proof} (Adapted from Goldberg and Tarjan~\cite[theorem~4.1]{Goldberg:1987})
    
    The algorithm terminates once $\epsilon < 1/n$, which takes place after $\log_2\left(\frac{C}{1/n}\right) = \log_2 (nC)$ iterations of \crefrange{algo:cost-scaling:start-loop}{algo:cost-scaling:end-loop}. The loop condition on \cref{algo:cost-scaling:start-loop} and the assignment on \cref{algo:cost-scaling:update-epsilon} are both $O(1)$, so the dominant cost of each iteration is the execution of \textproc{Refine}. \Crefrange{algo:cost-scaling:start-loop}{algo:cost-scaling:end-loop} thus contribute a cost of $O\left(R(n,m)\lg(nC)\right)$. This dominates the initialisation on \crefrange{algo:cost-scaling:start-loop}{algo:cost-scaling:end-loop}, and thus is the overall cost of the algorithm.
\end{proof}


\subsection{Heuristics} \label{appendix:impl-csheuristics}
% Proofread: 1 minor edits
Goldberg has proposed a number of heuristics to improve the real-world performance of his cost scaling algorithm~\cite{Goldberg:1997}, described in \cref{sec:impl-cost-scaling}. These have been found to result in considerable real-world improvements in efficiency~\cite{Bunnagel:1998,KiralyKovacs:2012}. Note that their effectiveness depends on the problem instance. Moreover, several of the heuristics such as arc fixing and potential update are highly sensitive to parameter settings or other implementation choices.

\subsubsection{Potential refinement} \label{appendix:impl-csheuristics-potential-refinement}
The $\textproc{Refine}(\mathbf{x},\boldsymbol{\pi},\epsilon)$ routine is guaranteed to produce an $\epsilon$-optimal flow. However, it may also be $\epsilon'$-optimal for $\epsilon' < \epsilon$. This heuristic decreases $\epsilon$ while modifying the potentials $\boldsymbol{\pi}$, without changing the flow $\mathbf{x}$. This has been found to yield a 40\% performance improvement~\cite{Bunnagel:1998}.

\subsubsection{Push lookahead}
Before performing $\textproc{Push}(i,j)$, this heuristic checks whether $j$ is a deficit vertex ($e_j < 0$) or if $j$ has an outgoing admissible arc. If so, the \textproc{Push} operation proceeds.

Otherwise, the \textproc{Push} operation is aborted, and \textproc{Relabel} is performed instead. Were the \textproc{Push} operation to be performed, the flow would get `stuck' at vertex $j$, and end up being pushed back to $i$. This heuristic has been found to significantly reduce the number of \textproc{Push} operations performed~\cite{Goldberg:1997}.

\subsubsection{Arc fixing}
The algorithm examines a large number of arcs, sometimes unnecessarily. It can be proved that for any arc $(i,j)$ with reduced cost satisfying $\left|c^{\boldsymbol{\pi}}_{ij}\right| > 2n\epsilon$, the flow $x_{ij}$ is never again modified. These arcs can safely be \emph{fixed}: removed from the adjacency list examined by the algorithm. 

This heuristic takes things a step further, \emph{speculatively} fixing arcs whenever $\left|c^{\boldsymbol{\pi}}_{ij}\right|$ is above some threshold $\beta$. Speculatively fixed arcs may still need to have their flow updated, but this is unlikely. Consequently, the algorithm examines these arcs very infrequently, unfixing arcs found to violate reduced cost optimality conditions.

\subsubsection{Potential update}
The \textproc{Relabel} operation given in \cref{algo:cost-scaling-operations} updates the potential at a single vertex. This heuristic is based around an alternative \emph{set relabel} operation, which updates potentials at many vertices at once.

Regular relabel operations are still used, with set relabel called periodically. The optimum frequency is dependent on both the problem and implementation, but calling set relabel every $O(n)$ regular relabels has been found to be a good rule of thumb~\cite{Goldberg:1997}.

This heuristic improves performance when used on its own, but has little effect when used in conjunction with potential refinement and push lookahead described above. In fact, it may even \emph{harm} performance in some cases, although it has been found to be more beneficial the larger the network instance~\cite{Bunnagel:1998}.