\chapter{Evaluation} \label{chap:eval}

% Proofread: None

\section{Project requirements} 

\begin{table}
    \centering
    % eliminate section prefix
    \crefformats{chapter,section,subsection}{#2#1#3}
    \crefnames{chapter,section,subsection}{}{}
    \begin{tabular}{clcccc}
        \textbf{\#} & \textbf{Deliverable} & \textbf{Section}
        \tabularnewline
        \hline
        & \textit{Success criteria} \tabularnewline
        S1 & Implement standard flow algorithms & \cref{sec:impl-cycle-cancelling,sec:impl-ssp,sec:impl-relax,sec:impl-cost-scaling} \tabularnewline
        S2 & Design an approximate solver & \cref{sec:impl-approx} \tabularnewline
        S3 & Integrate system with Firmament & \cref{sec:eval-test-integration} \tabularnewline
        S4 & Develop a benchmark suite & \cref{sec:eval-benchmark-strategy} \tabularnewline
        \hline
        & \textit{Optional extensions} \tabularnewline
        E1 & Design an incremental solver & \cref{sec:impl-incremental} \tabularnewline
        E2 & Build cluster simulator & \cref{sec:impl-firmament} \tabularnewline
        E3 & Optimise algorithm implementations & \cref{sec:eval-optimisations} \tabularnewline
        \hline
    \end{tabular}
    % restore section values
    \crefsections
    \caption{Deliverables for the project}
    \label{table:eval-project-requirements}
\end{table}

All success criteria set forth in the project proposal (see \cref{appendix:proposal}) have been met, with many optional extensions implemented. \Cref{table:eval-project-requirements} summarises the work completed.

\todo{Too short?}

\section{Correctness testing}

% For a project of this scope, it is easy for 

% Could link this back to development approaches
% Or just some generic bilge
% Tested individually and when integrated into the system
% Essential for each component to be verified: e.g. standard algorithm implementation being wrong could cause bugs which are hard to track down.

Unit tests were developed for each component. In accordance with the spiral model proposed in \cref{sec:prep-management-model}, these were re-run as a regression test at the end of each development cycle. Integration tests were also performed as soon as the requisite components had been implemented.

\subsection{Unit tests} \label{sec:eval-testing-unit}

Each flow algorithm was tested by comparing the solutions found to those produced by a reference implementation. The test dataset used is outlined in \cref{table:correctness-test-dataset}, and consists of some nnn\todo{Insert exact figure} GB of both natural and synthetic flow networks.

\begin{table}
    \begin{tabular}{c}
        TBC
    \end{tabular}
    % N.B. Include incremental test sets as well!
    \caption{Datasets for correctness testing}
    \label{table:correctness-test-dataset}
\end{table}

Standard unit test frameworks are not designed to work with large, external datasets such as these. Consequently, I developed my own test harness in Python to automate verification of flow algorithms. \Cref{fig:unit-test-harness} shows an extract from a test session.

\begin{figure} 
    TBC
    \caption{Unit test harness}
    \label{fig:unit-test-harness}
\end{figure}

The aforementioned tests verify entire flow algorithms. Unit tests were also written for smaller components, such as data structures, using the GTest framework (see \cref{sec:prep-tools-libraries}).

\subsection{Integration tests} \label{sec:eval-test-integration}

In practice, the algorithms developed in this project would be used as part of a larger cluster scheduling system. To demonstrate this capability, the solvers implemented in this project were integrated into the Firmament system. \Cref{fig:firmament-ui} shows the system in action, on a cluster of 14 machines.

\begin{figure}
    TBC
    \caption{Firmament scheduling tasks using this project's solver}
    \label{fig:firmament-ui}
\end{figure}

\section{Performance testing strategy} \label{sec:eval-benchmark-strategy}

TBC: Intro

\subsection{Simulating a Google cluster}

This project seeks to develop algorithms which will achieve sub-second scheduling latency on warehouse-scale computers, comprising many thousands of compute nodes. Unfortunately, it is not practical to run experiments on such clusters. Performance evaluation must consequently take place using a simulated cluster\footnotemark.
\footnotetext{This is a limitation common to most distributed systems research, not just Part II projects. Even the Quincy system, with all the resources of Microsoft Research behind it, was only ever tested on a small cluster of 243 machines.}

Firmament includes a cluster simulator, however I modified it extensively to support this project (see \cref{sec:impl-firmament}). To ensure the test is realistic, the simulator replays a month-long trace of events from a production Google cluster of 12,000 machines~\cite{clusterdata:Wilkes2011,clusterdata:Reiss2011,Reiss:2012}. This is by far the most detailed trace released by any major cluster operator, enabling an accurate simulation.\todo{Do you agree with this? I've not come across any comparable traces, but I don't feel confident I've not missed anything}

However, the trace is not perfect. To protect commercially sensitive information, much of the data was obfuscated by Google before release. Moreover, some data was simply never recorded: for example, the input file size of tasks. 

Although there is sufficient information to reconstruct the topology of the flow network, some cost models require data that is absent from the trace. The next two sections describe the cost models implemented in simulation, and how the limitations of the dataset were overcome.

\subsection{Octopus cost model}

The Octopus model implements a simple load balancing policy. Tasks and machines are assumed to be homogeneous. The cost of arcs into a machine are proportional to the load on that machine, resulting in tasks being preferentially scheduled on idle machines. 

This policy is too simplistic to be used in production. However, it serves as a useful baseline for comparison. Producing flow networks that are easy to solve compared to more realistic cost models, it provides an upper bound on the real-world scheduling latency that can be achieved.

\subsection{Quincy cost model}

Flow scheduling was pioneered by the Quincy system, developed at Microsoft Research~\cite{Isard:2009}. To enable a direct comparison between the two systems, I implemented the Quincy cost model in Firmament (see \cref{sec:prep-flow-scheduling,sec:impl-firmament}).

In contrast to the Octopus model, Quincy has direct real-world applicability: throughput increased by 40\% and data transfer was reduced by a factor of 3.9~\cite{Isard:2009}. The flow networks produced are considerably more complex, being representative of realistic cost models. Moreover, this complexity can be controlled by specifying the length of the preference lists, supporting scalability testing of the algorithms.

Quincy optimises for data locality, scheduling tasks close to where their input data is stored. To achieve this, the cost of arcs from a task to a machine is made proportional to the network bandwidth consumed were the task to be scheduled on that machine. The costs therefore depend upon location of blocks of input data for a task in the distributed filesystem.

Utilising this fine-grained resource information is a major strength of the Quincy system, and flow scheduling in general. However, it poses a challenge for simulation: such detailed information is not present in the trace, and must instead be estimated. 

Upon starting the simulator, a virtual distributed filesystem is built. Each machine is given 6 TB of simulated storage space, in line with typical specifications for machines in contemporary Google clusters~\cite{GoogleSlideDeck}. Files are replicated across three machines, in accordance with industry standards\footnotemark. Each file consists of 64 MB blocks, as in the Google File System~\cite{Ghemawat}.
\footnotetext{In practice, the replication factor varies with the type of data. For example, it would be higher for critical information such as login credentials, and lower for data such as old e-mail attachments.}

A collection of files is generated to saturate the available storage capacity\footnotemark, with file sizes randomly sampled from a distribution. Unfortunately, Google has not released any information on the file size distribution observed inside the cluster. Instead, I have used data from a similarly sized Facebook cluster to estimate a distribution~\cite{Chen}.
\footnotetext{In practice, a cluster would never run at full disk utilisation, however this distinction is unimportant for this test.}

When a task is submitted, an estimate of its input size $S$ is made. A set of input files is then assigned to the task, by randomly sampling from the distributed filesystem until the cumulative size reaches $S$. Google has not released any information on the input size distribution. For consistency, I have used data from the same Facebook cluster to estimate a distribution~\cite{Chen}.

However, it is possible to do better than simply randomly sampling from this distribution. The Google cluster trace contains information about the runtime of individual tasks, and there is a correlation between runtime and input size. Consequently, I have taken the approach of computing the cumulative probability of the runtime for each submitted task, assigning an input size of the same cumulative probability. So, for example, a task of median runtime will be assigned a median input file size.\todo{Does this explanation make sense to you?}

Given the paucity of the trace data, the simulation cannot be expected to exactly reproduce the behaviour of the original Google cluster. However, I believe that the simulation represents a realistic workload, which could plausibly occur in a production cluster today. 

The exact performance of the algorithms will, of course, depend on the architecture and workload of individual clusters. Most metrics used in this chapter therefore measure \emph{relative} performance, which should be robust to such differences. Absolute runtimes are occasionally reported: while indicative of real-world performance, care should be taken not to generalise beyond the experiments conducted.

\subsection{Test methodology}

Care was taken to minimise experimental error throughout the test process. However, error cannot be completely eliminated: some variation in runtime is inevitable when executing on a time-sharing operating system. Confidence intervals were computed using Student's $t$-distribution to quantify the error, which in most cases is negligible.

Each test was executed multiple times, with the runtimes across test runs aggregated to increase precision. Five iterations were performed by default, with more test runs being performed if the confidence interval was too wide.

Tests typically compared multiple algorithms, operating on the same dataset. Within each test run, algorithms were executed round-robin. This has the desirable property that each algorithm would tend to be equally affected if the performance of the server were to change over time.

However, significant effort was made to minimise variations in performance between runs. Tests took place on machines dedicated to the experiment, ensuring no interference from other workloads. All machines have the same specification (see \cref{appendix:test-machine-spec}), except where explicitly stated, allowing for absolute runtimes to be compared between experiments.

The algorithms were evaluated at multiple scales, ranging from small clusters that might be used within a private company to warehouse-scale computers used to support large cloud applications. \Cref{table:cluster-sizes} summarises the sizes used throughout the rest of the chapter.

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \textbf{Name} & \textbf{Percentage of Google cluster} & \textbf{Number of machines} \tabularnewline
        \hline
        Small & 1\% & 120 \tabularnewline
        Medium & 5\% & 600 \tabularnewline
        Large & 25\% & 3000 \tabularnewline 
        Warehouse-scale & 100\% & 12,000 \tabularnewline
    \end{tabular}
    \caption{Cluster sizes used in testing}
    \label{table:cluster-sizes}
\end{table}

Running the tests manually would be error-prone, as well as extremely time consuming. A test harness was developed in Python to automate the process, described further in \cref{appendix:benchmark-harness}. This ensures the above methodology is followed for all tests.

%* Ensure each algorithm runs at peak performance, before doing comparisons between algorithms. Optimisations, compilers.
%* Say what you're measuring? Perhaps better introduced in each section.

% Specify what properties we're recording? Online vs offline tests?

\section{Optimisations} \label{sec:eval-optimisations}

This project is primarily concerned with algorithmic improvements, however implementation decisions can have a considerable impact on the performance of flow solvers. Comparing an optimised implementation of one algorithm against an unoptimised version of another algorithm could give misleading results. 

Consequently, care has been taken to ensure that all algorithms considered in this project are implemented efficiently. This section evaluates the optimisations applied to each implementation, describes how parameters were set for each algorithm and how a compiler was selected.

%Structure? You probably do want to merge optimisations into a single figure, as Malte suggested. Or at least, a single figure for each algorithm. FIFO is on it's own, but that's about it.
%
%Scaling factor: you'll do this for both your algorithm and Goldie's.
%
%Compilers: everything. But it's boring so you might just want an extract.
%
%So you could have:
%
%* Optimisations: one section for each algorithm, keep it brief.
%* Scaling factor: brief discussion, just for cost scaling.
%* Compilers.
%
%Alternatively:
%
%* Big figure
%* Successive shortest path - ref figure
%* Relax - ref figure
%* Cost scaling - ref figure, and include it's own figure for scaling factor
%* Compilers - everything
%
%Think I favour second approach. You might wanna make this big master figure, though, since it's kinda dependent upon that actually working. Although guess you could break it up, so maybe no dependency... Think it works better if optimisations build on each other. So e.g. small heap + Djikstra with map? Think it wouldn't be too much work to restructure it to do this, the only one where you have interactions is AP. Datasets: only worth having multiple ones when it varies by scale, really.
\subsection{Successive shortest path algorithm}

\begin{figure}
    \centering
    \includegraphics{opt/ap_relative_1col}
    \caption{Speedup relative to naive successive shortest path implementation with standard Djikstra and big heap}
    \label{fig:opt-ap}
\end{figure}

As explained in \cref{sec:impl-ssp-optimisations}, it is possible to terminate Djikstra's algorithm as soon as it finds a shortest path to a deficit node, without needing to compute shortest paths to all nodes in the graph. As \cref{fig:opt-ap} demonstrates, this so-called \emph{partial Djikstra} approach yields a considerable performance improvement.

This inspired a related optimisation, \emph{small heap}. Djikstra's algorithm works by maintaining a priority queue of vertices, implemented using a binary heap in this project (see \cref{sec:impl-ssp-optimisations} for justification of this choice). This results in a $\Theta\left(\lg l\right)$ complexity for priority queue operations, where $l$ is the length of the queue.

In the \emph{big heap} implementation, the heap is initialised to contain all vertices in the graph. \emph{Small heap}, by contrast, is initialised containing just the source vertex. Vertices are inserted into the queue as they are encountered during graph traversal.

Initialising a heap with $n$ elements has cost $\Theta(n)$; by contrast, inserting the elements one at a time has a cost of $\Theta(n\lg n)$. Big heap, therefore, will pay less of an initialisation cost --- at least under standard Djikstra, where each vertex is eventually inserted into the queue. However, the number of elements $l$ in the heap will tend to be larger throughout the lifetime of big heap, making each operation more expensive.

When using standard Djikstra, the performance of the two heap implementations is practically indistinguishable. However, small heap provides a considerable performance gain when used in conjunction with partial Djikstra. This is to be expected: with early termination, the typical length of the queue $l$ is much smaller than the number of vertices $n$ in the graph.

Two versions of small heap were implemented. To support decreasing the key of elements in the priority queue (a common operation in Djikstra's algorithm), it is necessary to maintain a \emph{reverse index} mapping node IDs to elements in the priority queue. In the big heap, this was implemented using an \emph{array} of length $n$. For the small heap, it is possible to reduce memory consumption by using a \emph{hash table}, storing mappings only for nodes $v$ present in the priority queue.

In practice, the algorithm is not memory bound, and runtime is the primary consideration. Both the array and hash table offer $O(1)$ lookup. The array might be expected to be faster: there is no need to compute a hash function, and a result will always be returned after a single memory read (whereas for the hash table, multiple reads may be required in the event of a collision.) However, the hash table may be faster for large heaps: it can be more efficiently cached, since there is less dead space than in the array.

The results clearly show that the array implementation outperforms the hash table version, at all cluster sizes. It may be that the array is never large enough for capacity cache misses to become a problem. Furthermore, hash tables do not necessarily offer a space advantage. The \emph{load factor}, the number of elements divided by the number of buckets, must be kept low to avoid collisions. Consequently, many buckets will be empty, wasting space.

\todo{Should I say which version of the algorithm I ended up using? It seems obvious -- the one which had the best runtime -- but perhaps it's best stated explicitly?}

\subsection{Relaxation algorithm}

\begin{figure}
    \centering
    \includegraphics{opt/relax_arc_cache_relative_2col}
    \caption{Speedup when maintaining set(s) of arcs crossing the cut}
    \label{fig:opt-relax-cache-arcs}
\end{figure}

The relaxation algorithm repeatedly accesses the set of arcs $\left(S,\overline{S}\right)$ crossing the cut. The main relaxation procedure in \cref{algo:relaxation} and \textproc{AugmentFlow} in \cref{algo:relaxation-augment-flow} only consider arcs $(i,j) \in \left(S,\overline{S}\right)$ for which the reduced cost $c_{ij}^{\boldsymbol{\pi}}$ is zero. By contrast, \textproc{UpdatePotentials} in \cref{algo:relaxation-update-potentials} considers only those arcs with positive reduced cost.

In the naive implementation of the algorithm, the entire list of arcs in the graph is searched until an arc satisfying the above conditions is found. It is possible to instead maintain the set of arcs $\left(S,\overline{S}\right)$ crossing the cut. This imposes an additional cost whenever $S$ is updated, but allows for iteration over the set to proceed more efficiently. At no additional cost, we can partition this set into arcs with zero reduced cost and those with positive reduced cost\footnotemark. This is more efficient, as the algorithm never needs to access the entire set $\left(S,\overline{S}\right)$.
\footnotetext{Note, by the reduced cost optimality conditions, no arcs in the residual network ever have negative reduced cost.}

This \emph{dual set} implementation realises considerable performance gains, more than doubling the speed of the algorithm. However, note that the only operation which iterates over the set $\set{(i,j) \in \left(S,\overline{S}\right) | c_{ij}^{\boldsymbol{\pi}} > 0}$ is \textproc{UpdatePotentials}, which occurs comparatively rarely. To investigate whether the speedup of \textproc{UpdatePotentials} justified the overhead of maintaining this second set, a \emph{single set} implementation was developed which maintains only $\set{(i,j) \in \left(S,\overline{S}\right) | c_{ij}^{\boldsymbol{\pi}} = 0}$. This implementation was found to be even faster, with runtimes around a third that of the naive implementation.

\subsection{Cost scaling}

\subsubsection{Wave vs FIFO}

\begin{figure}
    \centering
    \includegraphics{opt/cs_wave_vs_fifo_relative_2col}
    \caption{Speedup when using FIFO rather than wave}
    \label{fig:opt-cs-wave-vs-fifo}
\end{figure}

Two versions of Goldberg's cost scaling algorithm are described in \cref{sec:impl-cost-scaling-implementations}, \emph{wave} and \emph{FIFO}. The wave implementation has time complexity $O\left(n^3 \lg \left(nC\right)\right)$ compared to a $O\left(n^2m\lg\left(nC\right)\right)$ running time for FIFO. However, for flow scheduling networks the asymptotic time complexities are the same, as $m = O(n)$ by \cref{lemma:network-num-arcs}. 

It is clear from \cref{fig:opt-cs-wave-vs-fifo} that FIFO outperforms wave on this problem instance. I suspect this is because the FIFO queue contains only active vertices (see \cref{algo:cost-scaling-first-active-refine}). By contrast, in wave each iteration involves a pass over every vertex in the graph (see \cref{algo:cost-scaling-wave-refine}), imposing a significant overhead.

% Could also test out first-active approach: when relabel happens, add s to front of Q rather than rear

\subsubsection{Scaling factor}

\begin{figure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{opt/cs_scaling_factor_2col}
        \caption{My implementation}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{opt/cs_goldberg_scaling_factor_2col}
        \caption{Goldberg's implementation}
    \end{subfigure}
    \caption{Performance depending on scaling factor}
    \label{fig:opt-cs-scaling-factor}
\end{figure}

Cost scaling computes a series of $\epsilon$-optimal solutions, where $\epsilon$ is reduced by a \emph{scaling factor} $\alpha$ between calls to \textproc{Refine}. A value of $\alpha=2$ was proposed in the original paper by Goldberg and Tarjan~\cite{Goldberg:1987}, and is used in \cref{algo:cost-scaling}. However, this choice is not necessarily optimal: in fact, the best value of $\alpha$ varies depending on the problem instance~\cite[\S7]{Goldberg:1997}. Larger values of $\alpha$ will result in fewer calls to \textproc{Refine}, however each call will take longer to execute.

% This test seems a little bogus without preemption support, at least when running an hour into the trace.
\Cref{fig:opt-cs-scaling-factor} shows the runtime of the cost scaling algorithm by scaling factor, on the warehouse-scale dataset using the Quincy cost model. The experiment was conducted both for my implementation, and Goldberg's reference implementation CS2~\cite{CS2:2009}. TBC\todo{Finish when have updated results}

%\subsection{DIMACS Parser}
%
%\begin{figure}
%    \centering
%    \includegraphics{opt/parser_set_vs_getarc_relative_2col}
%    \caption{Speedup when maintaining cache of arcs present}
%    \label{fig:opt-dimacs-parser}
%\end{figure}
%
%Need to check for duplicate arcs when parsing. Linked list: consult underlying data structure. Set: maintain a separate set of the arcs present to speed lookup.
%
%Set actually works out slower. Conjecture: adjacency lists tend to be short, so overhead of linked list is low. Maintaining separate set harms cache performance, as increases working set?
%
%\todo{Bother including this section or not? It's kind of trivial. Although discussing why the optimisation *didn't* work is maybe interesting.}

\subsection{Compilers}

\begin{figure}
    \includegraphics[width=\textwidth]{com/cs_compiler_1col}
    \caption{Runtime for cost scaling by optimisation setting, for medium (500 machine) cluster}
    \label{fig:compiler-settings}
\end{figure}

%\begin{figure}
%    \begin{subfigure}[c]{0.5\textwidth}
%        \includegraphics[width=\textwidth]{com/cs_compiler_settings_absolute_2col}
%        \caption{GCC}
%    \end{subfigure}
%    \begin{subfigure}[c]{0.5\textwidth}
%        \includegraphics[width=\textwidth]{com/cs_compiler_settings_clang_absolute_2col}
%        \caption{Clang}
%    \end{subfigure}
%    \caption{Runtime for cost scaling by optimisation setting, for medium (500 machine) cluster}
%    \label{fig:compiler-settings}
%\end{figure}

\emph{Optimising compilers} apply transformations which preserve the semantic meaning of the program, while (typically) improving performance. Two compilers were evaluated for this project: the GNU C Compiler (GCC) and Clang. Both are well-established, and have extensive optimisation support. In general, neither is better than the other: some programs run faster when compiled with Clang, others with GCC.

In addition to choosing a compiler, it is necessary to decide upon which optimisation level to use: the choices for GCC and Clang are summarised in \cref{table:optimisation-flags}. Higher levels tend to produce faster code, at the cost of longer compilation times. For this project, runtime performance is the overriding concern. However, optimisation is a heuristic process: some aggressive optimisations can actually harm program performance. For example, inlining functions will increase the size of functions, increasing the risk of instruction cache misses. Consequently, the highest level is not necessarily best.

\Cref{fig:compiler-settings} shows the runtime of my cost scaling implementation compiled with GCC and Clang, under all supported optimisation levels. There is a dramatic improvement from applying the lowest level of optimisations, O1. GCC's O1 setting is fairly aggressive, achieving performance comparable to O2. Clang's is more relaxed, but their performance at O2 and O3 are both comparable.

This test was repeated for all other implementations in the project, the results of which are given in \cref{appendix:compiler-test-results}. Other implementations showed a similar pattern\footnotemark. GCC O3 was always competitive, and sometimes had a slight performance edge over Clang O3, and so is used in all subsequent tests.
\footnotetext{Implementations varied considerably in how much benefit they derived from optimisation, higher optimisation levels never decreased performance. Frangioni's relaxation implementation saw no benefit from an optimising compiler, likely due to it having been extensively hand optimised.}

\section{Approximation algorithm} \label{sec:eval-approx}

Unlike with other algorithms, evaluation of approximate solution methods (see \cref{sec:impl-approx}) must consider not just its speed, but also its accuracy. To complicate matters further, both the speed and accuracy of the approximation algorithm will vary depending on the terminating condition chosen.

I introduce an \emph{oracle} policy to ease interpretation of the results. It is assumed that the oracle knows precisely what the cost of the optimal solution is, and will terminate the algorithm as soon as an intermediate solution is found that reaches he target accuracy. Of course, an oracle model could not actually be implemented: the entire challenge of approximation is that the minimum cost is not known \textit{a priori}. The oracle policy therefore provides an upper bound on the speedup that can be achieved by approximation.

To evaluate the heuristic policies (see \cref{sec:impl-approx-adaptions}), I compute parameter settings which achieve the target accuracy on training data. The algorithm is then run with this policy on (unseen) test data, and the resulting accuracy and speed are measured. This split of test and training data guards against overfitting the policy to the data. The speed of the oracle model is plotted on the same graph, to see how close the heuristic is to maximal performance.

All tests were conducted using my implementation of cost scaling, described in \cref{sec:impl-cost-scaling}. Since the experiments in this section measure runtime \emph{relative} to the same algorithm used as an optimal solver, I believe the results should generalise to other implementations of cost scaling. 

There is one notable exception to this. The oracle policy achieves a speedup even at 100\% accuracy in these experiments, since optimality is often reached when $\epsilon > 1/n$. The \emph{price refinement} heuristic (see \cref{appendix:csheuristics}), present in highly optimised implementations such as Goldberg's CS2~\cite{CS2:2009}, is able to detect such early optimality. Consequently, such implementations would achieve the same performance as the oracle model at 100\% accuracy. Further work is required to determine if any other differences exist.

\todo{This section might be improved by including some example figures. But I want to split up the results into subsections for flow scheduling and other applications, and I'd like to avoid duplicating graphs.}

\subsection{Performance on flow scheduling}

\subsection{Performance in other applications}

\begin{figure}
    \centering
    \includegraphics{app/road_general_oracle_policy_2col}
    \caption{Upper bound on speedup of approximation algorithm for a given accuracy}
    \label{fig:app-oracle-policy}
\end{figure}

\cref{fig:app-oracle-policy} oracle policy: gives an upper bound on performance that can be achieved with an approximation algorithm.

\begin{figure}
    \centering
    \includegraphics{app/road_general_policy_cost_parameters_1col}
    \caption{Accuracy at percentiles for given parameter choices}
    \label{fig:app-policy-parameter-accuracy}
\end{figure}

\cref{fig:app-policy-parameter-accuracy} is computed on training data, to determine a suitable parameter for the heuristic.

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
      \includegraphics[width=\textwidth]{app/road_general_policy_cost_accuracy_narrow_2col}
      \caption{CDF of heuristic error}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
      \includegraphics[width=\textwidth]{app/road_general_policy_cost_speed_2col}
      \caption{CDF of speed under heuristic (compared to optimal performance under oracle)}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of heuristic}
    \label{fig:app-cdf}
\end{figure}

The heuristic is then used with the given parameter, and \cref{fig:app-cdf} evaluates its performance on (unseen) test data.\todo{XXX: In this example they're actually the same data set, but that'll be fixed in real tests}

%\subsubsection{Comparison with optimal algorithms}
%
%Set threshold for approximation algorithm so that it produces 'nearly' optimal results. How does performance compare? (In early tests, it was a big improvement, but need to replicate.)
%
%\subsubsection{Performance-accuracy tradeoff}
%
%How much accuracy do we have to give up to get to a certain performance level? Is there a range of parameters which is clearly best: e.g. can we get a big speedup for a small loss of accuracy, but after a point we have to make big sacrifices in accuracy for small speedups?
%
%\subsubsection{Impact of loss of accuracy}
%
%Test an approximate algorithm on cluster. How much does performance degrade?

\section{Incremental algorithm} \label{sec:eval-incremental}

Incremental solution methods (see \cref{sec:impl-incremental}) operate on sequences of related flow networks. Their performance is evaluated in this section by measuring the \emph{scheduling latency} on the Google cluster trace (see \cref{sec:eval-benchmark-strategy}) under the Quincy cost model. The simulator performs multiple \emph{scheduling rounds}. When an event (such as task submission) is received during an ongoing scheduling round, the event is added to a queue. A new scheduling round is started as soon as the old round completes, with the pending events. If the queue is empty, the simulator waits until a new event is received before starting a scheduling round. The scheduling latency for round $n$ is defined to be the \emph{longest} time any event has spent waiting in the queue while scheduling round $n-1$ runs, plus the runtime of the flow algorithm on scheduling round $n$. This represents the performance that would be observed by users in a real application.
% TODO: Specify the cost model? Should you run tests on both Quincy and Octopus?
% TODO: Maybe you should evaluate the fixed interval one as well? Could put it in the appendix.

\todo{Figure here illustrating possible timeline of events?}

This approach is necessary since, unlike `full' algorithms, the runtime of incremental algorithms depends on the number of changes in the previous scheduling round. Decreasing the runtime of an incremental algorithm will therefore tend to produce an even greater decrease in the scheduling latency, since the number of pending events in the queue will tend to be small. Conversely, small increases in the runtime of incremental solvers will also be amplified when measuring scheduling latency. 

This feedback effect is \emph{not} observed in `full` solvers. Consequently, evaluation earlier in the chapter has used a simpler approach: scheduling rounds were started at fixed time intervals, and raw runtime rather than scheduling latency was reported. Although that method may underestimate the true scheduling latency slightly\footnotemark, it does so equally for all \emph{full} solvers, and has the benefit that the experiments are much less computationally intensive.
\footnotetext{The scheduling latency is very often identical to algorithm runtime, although in the worst case the scheduling latency could be twice as great as the runtime.}

The next section compares the performance of the successive shortest path (see \cref{sec:impl-ssp}) and relaxation (\cref{sec:impl-relax}) algorithms operating incrementally and as a full solver. This overstates the true performance gains somewhat, however, as cost scaling (see \cref{sec:impl-cost-scaling}) outperforms both algorithms on the full problem. The second section therefore compares my implementation of cost scaling to the incremental algorithms. Finally, I compare the performance of my incremental solution methods with those of highly optimised reference implementations of traditional full solvers.

\subsection{Relative performance}

\begin{figure}
    \begin{widepage}
        \begin{subfigure}[c]{\textwidth}
            \includegraphics[width=0.5\textwidth]{inc/same_ap_cdf_2col}
            \includegraphics[width=0.5\textwidth]{inc/same_ap_incremental_only_cdf_2col}
            \caption{Successive shortest path}
        \end{subfigure}
        %XXX: This is currently small dataset. Think this will change given encouraging performance of 'none' caching.
        \begin{subfigure}[c]{\textwidth}
            \includegraphics[width=0.5\textwidth]{inc/same_relax_cdf_2col}                        \includegraphics[width=0.5\textwidth]{inc/same_relax_incremental_only_cdf_2col}
            \caption{Relaxation (my version)}
        \end{subfigure}
        \begin{subfigure}[c]{\textwidth}
            \includegraphics[width=0.5\textwidth]{inc/same_relaxf_cdf_2col}
            \includegraphics[width=0.5\textwidth]{inc/same_relaxf_incremental_only_cdf_2col}
            \caption{Relaxation (my adaptation of RelaxIV)}
        \end{subfigure}
    \end{widepage}
    \caption{Performance in incremental vs full mode, on medium (600 machine) dataset}
    \label{fig:inc-same}
\end{figure}

\Cref{fig:inc-same} shows a dramatic speedup from using the augmenting path class of algorithms (successive shortest path and relaxation) in an incremental mode of operation. For augmenting path, scheduling takes just over \SI{50}{\milli\second} in the incremental case, compared to a best case of just under \SI{2}{\second} in the full case: an improvement of 40x. The difference is even more pronounced when considering the \emph{mean} scheduling latency, which is \SI{3.38}{\second} in the full case but a mere \SI{6.95}{\milli\second} for the incremental solver: a speedup of over 450x. Similar results hold for both versions of relaxation.\todo{Write more once you have results from your new version of relax.}

% SOMEDAY: Could mention performance on other sized datasets?

\subsection{Best-in-class comparison}

\begin{figure}
    \begin{widepage}
        \begin{subfigure}[c]{0.5\textwidth}
            \includegraphics[width=\textwidth]{inc/head_to_head_my_large_cdf_2col}
            \caption{Large cluster (3000 machines)}
        \end{subfigure}
        \begin{subfigure}[c]{0.5\textwidth}
            \includegraphics[width=\textwidth]{inc/head_to_head_my_full_size_cdf_2col}
            \caption{Full Google cluster (12,000 machines)}
        \end{subfigure}
    \end{widepage}
    \caption{Performance of incremental augmenting path vs standard cost scaling}
    \label{fig:inc-head-to-head-my}
\end{figure}

The previous section showed that the incremental mode of operation considerably reduces the work performed by augmenting path algorithms. However, augmenting path algorithms are very slow on the full problem\footnotemark: is the speedup of incremental operation enough to overcome this?
\footnotetext{Relaxation is extremely fast on some networks, but unfortunately not flow scheduling networks.}

Cost scaling is the fastest full algorithm on flow scheduling networks. \todo{Which of my incremental algorithms should I use? Update based on results from new version of relax.} \Cref{fig:inc-head-to-head-my} pits my incremental algorithm against my implementation of cost scaling. The incremental algorithm considerably outperforms cost scaling, with a speedup of 19x on the full (12,000 machine) Google cluster. Note this is, however, significantly less than the 450x speedup observed in the previous section. This is to be expected, as cost scaling is considerably faster on the full problem.

\subsection{Comparative evaluation}

\begin{figure}
    \begin{widepage}
        \begin{subfigure}[c]{0.5\textwidth}
            \includegraphics[width=\textwidth]{inc/head_to_head_optimised_large_cdf_2col}
            \caption{Large cluster (3000 machines)}
            \label{fig:inc-head-to-head-optimised:large}
        \end{subfigure}
        \begin{subfigure}[c]{0.5\textwidth}
            \includegraphics[width=\textwidth]{inc/head_to_head_optimised_full_size_cdf_2col}
            \caption{Full Google cluster (12,000 machines)}
            \label{fig:inc-head-to-head-optimised:full-size}
        \end{subfigure}
    \end{widepage}
    \caption{Performance of optimised versions of incremental relaxation and cost scaling}
    \label{fig:inc-head-to-head-optimised}
\end{figure}

Researchers have spent considerable time developing optimised implementations of flow algorithms. Since the focus of this project is on developing improved algorithmic techniques, I have spent little time optimising my implementations. Although I have attempted to ensure that all implementations in this project are optimised to an equal degree, there is a risk that the results in the previous section do not generalise to state-of-the-art reference implementations. 

To investigate this, I extended the reference implementation of the relaxation algorithm, RelaxIV~\cite{RelaxIV:2011}, to support operating incrementally (see \cref{sec:impl-incremental-impl}). For the full solver, I selected Goldberg's heavily optimised implementation of his cost scaling algorithm, CS2~\cite{CS2:2009}. This has been found to be one of the fastest flow solvers in independent benchmarks~\cite{KiralyKovacs:2012}, corroborated by my own benchmarks. The developers of the Quincy system also opted to use CS2~\cite{Isard:2009}, although my choice was made independently of this.

%\Cref{fig:inc-head-to-head-optimised} shows the incremental solver significantly outperforms the full solver, with a 14.5x speedup. Note that although the absolute performance differs considerably between these solvers and the implementations in the previous section, the relative performance is similar: the corresponding speedup was 19x in the previous section. This is to be expected: implementation optimisations do not change the asymptotic complexity of the algorithm, only its constant factors. However, it is encouraging this is confirmed empirically, and it provides support for the use of relative performance figures elsewhere in this evaluation.
%
%The order of magnitude performance gain shown in \cref{fig:inc-head-to-head-optimised} has considerable practical implications for flow schedulers. With a mean scheduling latency of \SI{197}{\milli\second} for the incremental solver, most tasks will be scheduled without any perceptible delay, even on extremely large clusters such as those operated by Google. The distribution for scheduling latency is observed to have a long tail: the next section will Sub-second scheduling is a key target

\begin{figure}
    \includegraphics[width=\textwidth]{inc/head_to_head_optimised_full_size_incremental_only_target_latency_cdf_1col}
    \caption{Performance of incremental relaxation on full Google cluster (12,000 machines)}
    \label{fig:inc-head-to-head-optimised-inconly}
\end{figure}

\Cref{fig:inc-head-to-head-optimised} shows the incremental solver significantly outperforms the full solver. The incremental solver is faster than the full solver at every percentile: note that the incremental solver is always to the left of the full solver. There is a 14.5x mean speedup on the full (12,000 machine) Google cluster, similar to the 19x speedup observed with the unoptimised implementations\footnotemark. With a mean scheduling latency of \SI{197}{\milli\second} for the incremental solver, most tasks will be scheduled without any perceptible delay. Moreover, \cref{fig:inc-head-to-head-optimised-inconly} shows the incremental solver achieves sub-second scheduling latency in $98.4\%$ of cases.
\footnotetext{This is to be expected: implementation optimisations do not change the asymptotic complexity of the algorithm, only its constant factors. However, it is encouraging this is confirmed empirically, and it provides support for the use of relative performance figures elsewhere in this evaluation.} 

%Both distributions are long tailed, reflecting the added workload when there are large numbers of changes\todo{Add reference to next section when written. Adding preemption support will change this slightly, probably removing the long tail of CS2 but shifting it to the right. If you don't add preemption support, might want to explain why workload varies for standard solver}.

\subsection{Determinants of incremental performance}

TBC. Optional. Show correlation between \# of changes and incremental runtime.

\subsection{Scalability analysis}
TBC. Optional.

\subsubsection{Growing size of network}

Increase number of nodes, but keep structure.

\subsubsection{Increasing complexity of network}

e.g. increase number of preference arcs, or similar. Simulate some more complicated scheduling policy.

\subsubsection{Large incremental changes}

For incremental algorithm only. Simulate large job being added with lots of tasks, or many machines going offline.

\subsubsection{Different cost models}

How does performance vary on new / more complicated cost models?

\section{Other applications}

Hitherto have evaluated on Quincy-style flow graphs. But my algorithms are fully general, will operate on any flow network. How does it fare on other graphs? Make clear this wasn't part of the original goal of the project.

\section{Summary}

TBC: Summary