\chapter{Evaluation} \label{chap:eval}

% Proofread: None

\section{Project requirements} 

\begin{table}
    \centering
    % eliminate section prefix
    \crefformats{chapter,section,subsection}{#2#1#3}
    \crefnames{chapter,section,subsection}{}{}
    \begin{tabular}{clcccc}
        \textbf{\#} & \textbf{Deliverable} & \textbf{Section}
        \tabularnewline
        \hline
        & \textit{Success criteria} \tabularnewline
        S1 & Implement standard flow algorithms & \cref{sec:impl-cycle-cancelling,sec:impl-ssp,sec:impl-relax,sec:impl-cost-scaling} \tabularnewline
        S2 & Design an approximate solver & \cref{sec:impl-approx} \tabularnewline
        S3 & Integrate system with Firmament & \cref{sec:eval-test-integration} \tabularnewline
        S4 & Develop a benchmark suite & \cref{sec:eval-benchmark-strategy} \tabularnewline
        \hline
        & \textit{Optional extensions} \tabularnewline
        E1 & Design an incremental solver & \cref{sec:impl-incremental} \tabularnewline
        E2 & Build cluster simulator & \cref{sec:impl-firmament} \tabularnewline
        E3 & Optimise algorithm implementations & \cref{sec:eval-optimisations} \tabularnewline
        \hline
    \end{tabular}
    % restore section values
    \crefsections
    \caption{Deliverables for the project}
    \label{table:eval-project-requirements}
\end{table}

All success criteria set forth in the project proposal (see \cref{appendix:proposal}) have been met, with many optional extensions implemented. \Cref{table:eval-project-requirements} summarises the work completed.

\todo{Too short?}

\section{Correctness testing}

% For a project of this scope, it is easy for 

% Could link this back to development approaches
% Or just some generic bilge
% Tested individually and when integrated into the system
% Essential for each component to be verified: e.g. standard algorithm implementation being wrong could cause bugs which are hard to track down.

Unit tests were developed for each component. In accordance with the spiral model proposed in \cref{sec:prep-management-model}, these were re-run as a regression test at the end of each development cycle. Integration tests were also performed as soon as the requisite components had been implemented.

\subsection{Unit tests} \label{sec:eval-testing-unit}

Each flow algorithm was tested by comparing the solutions found to those produced by a reference implementation. The test dataset used is outlined in \cref{table:correctness-test-dataset}, and consists of some nnn\todo{Insert exact figure} GB of both natural and synthetic flow networks.

\begin{table}
    \begin{tabular}{c}
        TBC
    \end{tabular}
    % N.B. Include incremental test sets as well!
    \caption{Datasets for correctness testing}
    \label{table:correctness-test-dataset}
\end{table}

Standard unit test frameworks are not designed to work with large, external datasets such as these. Consequently, I developed my own test harness in Python to automate verification of flow algorithms. \Cref{fig:unit-test-harness} shows an extract from a test session.

\begin{figure} 
    TBC
    \caption{Unit test harness}
    \label{fig:unit-test-harness}
\end{figure}

The aforementioned tests verify entire flow algorithms. Unit tests were also written for smaller components, such as data structures, using the GTest framework (see \cref{sec:prep-tools-libraries}).

\subsection{Integration tests} \label{sec:eval-test-integration}

In practice, the algorithms developed in this project would be used as part of a larger cluster scheduling system. To demonstrate this capability, the solvers implemented in this project were integrated into the Firmament system. \Cref{fig:firmament-ui} shows the system in action, on a cluster of 14 machines.

\begin{figure}
    TBC
    \caption{Firmament scheduling tasks using this project's solver}
    \label{fig:firmament-ui}
\end{figure}

\section{Performance testing strategy} \label{sec:eval-benchmark-strategy}

TBC: Intro

%* Introduction
%* Simulating a Google Cluster - include justification of why we're simulating, explain why using Google data (it's big real world example, one of the few publicly released)
%* Quincy Cost Model - note limitations of data set: can't just use existing Firmament cost models. Justify Quincy. But note there isn't the data. Explain proxies used. Cite appendix with exact choice of distributions.
%* Test Methodology - attention to detail. Repeating tests round-robin, etc. Could also give higher-level discussion, introduce next chapter. 
%* Benchmark Suite -- tempted to move this to appendix, reference it from previous section. Partly for wordcount, partly because test methodology leads nicely into the next section.

%Simulator proper described in implementation section. Backward reference, and then describe the dataset we're operating on. Can skim over Google trace, since this isn't your work: cite appropriate work, though. 
%
%Quincy: in more detail, this is some interesting work. Can show some attention to detail in the distributions, you may want to put this in the appendix.
%
%Test methodology: repeating tests, doing round-robin, etc.
%
%Benchmark suite: gotta put it somewhere, may as well get some credit for it.

% Test setup:
% - Machines: Reference to appendix for specification. 
% - Distributed, but all had same specs.
% Methodology:
% - Test machines: reference to appendix for specifications.
% - Distributed, all had same specs. Individual

%Focus on methodology. Accuracy: repeat tests multiple time. Run algorithms round-robin to avoid caching effects. Discount time spent parsing: just measure what's relevant. etc.
%
%Explain different categories of test.

% Specify what properties we're recording? Online vs offline tests?

\subsection{Simulating a Google cluster}

% Why simulate? => Want to see if fast at scale, can't just run test on a WSC computer.
% But, need to ensure test is realistic. => Use data from an actual cluster: hey, Google has given away some.

This project seeks to develop algorithms which will achieve sub-second scheduling latency on warehouse-scale computers, comprising many thousands of compute nodes. Unfortunately, it is not practical to run experiments on such clusters. Performance evaluation must consequently take place using a simulated cluster\footnotemark.
\footnotetext{This is a limitation common to most distributed systems research, not just Part II projects. Even the Quincy system, with all the resources of Microsoft Research behind it, was only ever tested on a small cluster of 243 machines.}

Firmament includes a cluster simulator, however I modified it extensively to support this project (see \cref{sec:impl-firmament}). To ensure the test is realistic, the simulator replays a month-long trace of events from a production Google cluster of 12,000 machines~\cite{clusterdata:Wilkes2011,clusterdata:Reiss2011,Reiss:2012}. This is by far the most detailed trace released by any major cluster operator, enabling an accurate simulation.\todo{Do you agree with this? I've not come across any other comparable traces, but I don't feel confident I've not missed anything}

However, the trace is not perfect. To protect commercially sensitive information, much of the data was obfuscated by Google before release. Moreover, some data was simply never recorded: for example, the input file size of tasks. 

Although there is sufficient information to reconstruct the topology of the flow network, some cost models require data that is absent from the trace. The next two sections describe the cost models implemented in simulation, and how the limitations of the dataset were overcome.

%The data present in the trace may be sufficient for simplistic scheduling policies. For example, the Octopus cost model treats all tasks and machines equally, seeking only to balance load between machines. may not be a problem. For example, the Octopus cost model seeks 

%This posed considerable challenges for simulation. The next section describes how information was inferred from the trace to generate flow networks using the Quincy cost model.

\subsection{Octopus cost model}

The Octopus model implements a simple load balancing policy. Tasks and machines are assumed to be homogeneous. The cost of an arc from a task to a machine is proportional to the load on that machine, resulting in tasks being preferentially scheduled on idle machines. 

This policy is highly simplistic, and would never be used in production. However, it serves as a useful baseline for comparison. Producing flow networks that are easy to solve relative to more realistic cost models, it provides an upper bound on the real-world scheduling latency that can be achieved.

\subsection{Quincy cost model}

% Whereas Octopus will likely never be used, Quincy was developed at Microsoft Research to address a real-world problem, ...

This difference was mostly harmless for traditional `full' solvers, whose runtime does not depend on the number of changes.  But for incremental solvers, the difference is significant. The longer the period between scheduling rounds, the more changes there are, increasing the solver's runtime. The simulator defaulted to a very short fixed time interval, which unfairly skewed the test in favour of incremental solvers. 

To resolve this problem, the time between scheduling rounds was made proportional to the runtime of the solver, as it would be in a real cluster. The statistic reported is no longer the runtime of the algorithm, but the worst-case scheduling latency experienced by an event\footnotemark. This has the added benefit of increasing the realism of the simulation for `full' solvers as well.
\footnotetext{To see why these are different, suppose an event arrives just after a scheduling round begins. It must wait until the solver finishes, before a new scheduling round begins. The event is only processed after the \emph{second} scheduling round finishes, so the event has had to wait for (almost) two executions of the solver.}

%% OLD STUFF

The two preceding sections described the new solution methods implemented by this project. These approaches will only be adopted if a compelling performance improvement can be shown. The experimental setup used to demonstrate this is described in the next chapter, \cref{chap:eval}. This section outlines the implementation work that was necessary to run these tests.

The Firmament flow scheduling system was used in the tests. Some experiments, such as those in \cref{sec:eval-test-integration}, were performed on a real cluster using the standard Firmament code base. However, the goal of this project is to produce algorithms which are fast at \emph{scale}. Testing this would require access to a cluster with tens of thousands of machines, which is clearly not practical.

Instead, a cluster is \emph{simulated} for such tests. Fortunately, the Firmament system includes a simulator based on events recorded in a Google cluster~\cite{clusterdata:Wilkes2011}. The next section, \S\ref{sec:impl-benchmark-simulator}, describes modifications made to the simulator, and other areas of the Firmament code base. An outline of the test harness used to automate the benchmark process is given in the subsequent section, \S\ref{sec:impl-benchmark-harness}.

In a real cluster, Firmament will start a new scheduling round as soon as the previous round has completed, provided any events are pending. By contrast, the simulator started scheduling rounds at fixed time intervals. 

This difference was mostly harmless for traditional `full' solvers, whose runtime does not depend on the number of changes.  But for incremental solvers, the difference is significant. The longer the period between scheduling rounds, the more changes there are, increasing the solver's runtime. The simulator defaulted to a very short fixed time interval, which unfairly skewed the test in favour of incremental solvers. 

To resolve this problem, the time between scheduling rounds was made proportional to the runtime of the solver, as it would be in a real cluster. The statistic reported is no longer the runtime of the algorithm, but the worst-case scheduling latency experienced by an event\footnotemark. This has the added benefit of increasing the realism of the simulation for `full' solvers as well.
\footnotetext{To see why these are different, suppose an event arrives just after a scheduling round begins. It must wait until the solver finishes, before a new scheduling round begins. The event is only processed after the \emph{second} scheduling round finishes, so the event has had to wait for (almost) two executions of the solver.}

\subsection{Test harness} \label{sec:impl-benchmark-harness}

Running the tests manually would be unwieldly and error-prone. A test harness was developed in Python to automate the process. The configuration of the harness includes a list of implementations and datasets. Individual test cases in the configuration specify which implementation to test and what datasets to use, along with other parameters.

In addition to testing the final versions of the solvers, the harness was used to evaluate the impact of optimisations implemented throughout the project, as reported in \cref{sec:eval-optimisations}. To support this, the harness was integrated with the Git version control system~\cite{GitWWW}. Versions of an implementation can be specified by a Git commit ID.

\todo{Add extract from configuration file / example output from the harness to illustrate this?}
It was anticipated that this project would involve running a large number of experiments. The harness was therefore designed to make the process as simple as possible. Test cases can be specified in less than 10 lines. Starting the experiment is simple: the harness will check out appropriate versions of the code and build the implementations with no user intervention, returning once the results of the test are complete.

Efficiency was also a key requirement: with many experiments, it is important that they complete as fast as possible. The harness reuses intermediate computation where possible. Implementations are only checked out and built once. A cache of generated flow networks is maintained\footnotemark, so that the overhead is only incurred on the first run.
\footnotetext{Except where this could affect the results. For example, the test to measure scheduling latency described in the previous section requires the network be regenerated on each run, as it may vary slightly depending on the runtime of the algorithm.}

% Omitted, may want to mention:
% - Supports both offline (graph file) and hybrid/online (simulator) tests
% - For offline: supports both incremental and full, via snapshot solver.
% - For offline: can specify files with globs, etc.

\section{Optimisations} \label{sec:eval-optimisations}

Comparisons between different versions of the *same* algorithm implementation. Subsection for each algorithm implemented.

Optimisations may be algorithmic in nature, or more low-level, e.g. laying out data to make use of caches.

\subsection{Successive shortest path algorithm}

\subsubsection{Large vs small heap}

\begin{figure}
  \centering
  \includegraphics{opt/ap_big_vs_small_relative_2col}
  \caption{Speedup when using small heap}
  \label{fig:opt-ap-big-vs-small}
\end{figure}

Heap used as priority queue in Djikstra's algorithm. Large heap: all nodes in the graph are in the heap. Small heap: only insert nodes into heap once they're visited.

\subsubsection{Terminating Djikstra early}

\begin{figure}
    \centering
    \includegraphics{opt/ap_full_vs_partial_relative_2col}
    \caption{Speedup when terminating Djikstra early}
    \label{fig:opt-ap-terminate-djikstra-early}
\end{figure}

Partial: terminate Djikstra as soon as a node permanently labelled. Full: solve entire SSSP problem.

\subsection{Relaxation algorithm}

\begin{figure}
    \centering
    \includegraphics{opt/relax_arc_cache_relative_2col}
    \caption{Speedup when caching arcs}
    \label{fig:opt-relax-cache-arcs}
\end{figure}

Relaxation algorithm requires knowing the arcs crossing the cut. None: iterate over all arcs and find out which ones cross the cut. Cache zero cost: maintain a list of zero reduced cost arcs crossing the cut. Cache all: maintain two lists, one of zero reduced cost and one of positive reduced cost arcs crossing the cut.

\subsection{Cost scaling}

\subsubsection{Wave vs FIFO}

\begin{figure}
    \centering
    \includegraphics{opt/cs_wave_vs_fifo_relative_2col}
    \caption{Speedup when using FIFO rather than wave}
    \label{fig:opt-cs-wave-vs-fifo}
\end{figure}

% Could also test out first-active approach: when relabel happens, add s to front of Q rather than rear

\subsubsection{Scaling factor}

\begin{figure}
    \centering
    \includegraphics{opt/cs_scaling_factor_2col}
    \caption{Performance depending on scaling factor}
    \label{fig:opt-cs-scaling-factor}
\end{figure}

What factor to reduce $\epsilon$ by on each iteration?

\subsection{DIMACS Parser}

\begin{figure}
    \centering
    \includegraphics{opt/parser_set_vs_getarc_relative_2col}
    \caption{Speedup when maintaining cache of arcs present}
    \label{fig:opt-dimacs-parser}
\end{figure}

Need to check for duplicate arcs when parsing. Linked list: consult underlying data structure. Set: maintain a separate set of the arcs present to speed lookup.

Set actually works out slower. Conjecture: adjacency lists tend to be short, so overhead of linked list is low. Maintaining separate set harms cache performance, as increases working set?

\todo{Bother including this section or not? It's kind of trivial. Although discussing why the optimisation *didn't* work is maybe interesting.}

\section{Compilers}

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/cc_absolute_2col}
        \caption{Cycle cancelling}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/ap_absolute_2col}
        \caption{Augmenting Path}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/relax_absolute_2col}
        \caption{Relaxation}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/cs_absolute_2col}
        \caption{Cost scaling}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/goldberg_absolute_2col}
        \caption{Goldberg's cost scaling}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/frangioni_absolute_2col}
        \caption{Frangioni's relaxation algorithm}
    \end{subfigure}
    \end{widepage}
    \caption{Performance under different compilers and optimisation levels}
    \label{fig:compilers}
\end{figure}

Want to make each implementation run as fast as possible: necessary for later tests to be fair. Different compilers use different optimisation techniques, and so may work better for some applications than others. Evaluated Clang and GCC, two compilers with sophisticated optimisations.

Tested at all optimisation levels. (Table showing the levels? O0 to O3, with brief explanation.) Significant performance improvement when going to O2; limited performance gains in O3, perhaps even worse.

Little difference between Clang and GCC. GCC O3 tended to be the fastest implementation. Given this, GCC O3 was chosen as the standard compiler for subsequent tests.

Probably best not to dwell on this since results aren't that interesting, but worth mentioning it was carried out for thoroughness. Probably want to compact the results significantly. Maybe OK to just give example output from one or two implementations, and mention similar for others.

\section{Performance evaluation}

Tests on final version of algorithms, with all optimisations enabled.

\subsection{Approximation algorithm} \label{sec:eval-approx}

% Reference from Impl:Cost Scaling:Heuristics expects you to justify why comparing against my own cost scaling implementation is legit.

\begin{figure}
    \centering
    \includegraphics{app/road_over_time_2col}
    \caption{Successive approximations to the optimal solution}
    \label{fig:app-cost-over-time}
\end{figure}

\cref{fig:app-cost-over-time} perhaps best in implementation section, to illustrate how algorithm works.

\begin{figure}
    \centering
    \includegraphics{app/road_oracle_policy_2col}
    \caption{Upper bound on speedup of approximation algorithm for a given accuracy}
    \label{fig:app-oracle-policy}
\end{figure}

\cref{fig:app-oracle-policy} oracle policy: gives an upper bound on performance that can be achieved with an approximation algorithm.

\begin{figure}
    \centering
    \includegraphics{app/road_policy_parameter_accuracy_1col}
    \caption{Accuracy at percentiles for given parameter choices}
    \label{fig:app-policy-parameter-accuracy}
\end{figure}

\cref{fig:app-policy-parameter-accuracy} is computed on training data, to determine a suitable parameter for the heuristic.

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
      \includegraphics[width=\textwidth]{app/road_policy_error_cdf_2col}
      \caption{CDF of heuristic error}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
      \includegraphics[width=\textwidth]{app/road_policy_speed_cdf_2col}
      \caption{CDF of speed under heuristic (compared to optimal performance under oracle)}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of heuristic}
    \label{fig:app-cdf}
\end{figure}

The heuristic is then used with the given parameter, and \cref{fig:app-cdf} evaluates its performance on (unseen) test data.\todo{XXX: In this example they're actually the same data set, but that'll be fixed in real tests}

%\subsubsection{Comparison with optimal algorithms}
%
%Set threshold for approximation algorithm so that it produces 'nearly' optimal results. How does performance compare? (In early tests, it was a big improvement, but need to replicate.)
%
%\subsubsection{Performance-accuracy tradeoff}
%
%How much accuracy do we have to give up to get to a certain performance level? Is there a range of parameters which is clearly best: e.g. can we get a big speedup for a small loss of accuracy, but after a point we have to make big sacrifices in accuracy for small speedups?
%
%\subsubsection{Impact of loss of accuracy}
%
%Test an approximate algorithm on cluster. How much does performance degrade?

\subsection{Incremental algorithm} \label{sec:eval-incremental}

\subsubsection{Dataset}

Google cluster trace. Justify usage: representative of actual operations encountered.

\subsubsection{Comparison with non-incremental, same type}

Runtime compared to the {\it same} algorithm, running from scratch. Bit of a strawman: augmenting path is slow from scratch. But would still be interesting to see: what \% of work are we still having to do?

\paragraph{Augmenting path}

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{inc/same_ap_over_time_1col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_ap_cdf_2col}
    \end{subfigure}
   \begin{subfigure}[c]{0.5\textwidth}
       \includegraphics[width=\textwidth]{inc/same_ap_incremental_only_cdf_2col}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of augmenting path in incremental vs standard mode}
    \label{fig:inc-same-ap}
\end{figure}

\paragraph{Relaxation}

\begin{figure}
    \begin{widepage} 
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relax_over_time_1col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relax_cdf_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relax_incremental_only_cdf_2col}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of relaxation in incremental vs standard mode}
    \label{fig:inc-same-relax}
\end{figure}

My version in \cref{fig:inc-same-relax}.

\begin{figure}
    \begin{widepage} 
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relaxf_over_time_1col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relaxf_cdf_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relaxf_incremental_only_cdf_2col}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of relaxation in incremental vs standard mode}
    \label{fig:inc-same-relaxf}
\end{figure}

Frangioni's version in \cref{fig:inc-same-relaxf}. \todo{Tempted to cut this entirely, and just consider it later in comparative evaluation.}

\subsubsection{Comparison with non-incremental, different type}

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_my_over_time_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_my_cdf_2col}
    \end{subfigure}
    \end{widepage}
% These figures just aren't that interesting, similar scale to main one
%    \begin{subfigure}[c]{0.45\textwidth}
%        \includegraphics{inc_head_to_head_my_incremental_only_cdf}
%    \end{subfigure}
    \caption{Performance of incremental augmenting path vs standard cost scaling}
    \label{fig:inc-head-to-head-my}
\end{figure}

Compare to cost scaling, my implementation. (Comparison with reference implementations comes later.) \todo{I've disabled displaying RELAX here; it spikes to 30s occassionally. Try it again when you have a cost model which doesn't trigger the pathological case.}

\section{Scalability analysis}

\subsection{Growing size of network}

Increase number of nodes, but keep structure.

\subsection{Increasing complexity of network}

e.g. increase number of preference arcs, or similar. Simulate some more complicated scheduling policy.

\subsection{Large incremental changes}

For incremental algorithm only. Simulate large job being added with lots of tasks, or many machines going offline.

\subsection{Different cost models}

How does performance vary on new / more complicated cost models?

\section{Comparative evaluation} \label{sec:eval-comparative}

Compare performance with reference implementations, such as Goldberg. Test only those algorithms found to be most competitive in the previous section -- no point testing algorithms which I already know are suboptimal.

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_optimised_over_time_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_optimised_cdf_2col}
    \end{subfigure}
    \end{widepage}
% These figures just aren't that interesting, similar scale to main one
%    \begin{subfigure}[c]{0.45\textwidth}
%        \includegraphics{inc_head_to_head_optimised_incremental_only_cdf}
%    \end{subfigure}
    \caption{Performance of optimised versions of incremental relaxation and cost scaling}
    \label{fig:inc-head-to-head-optimised}
\end{figure}

\section{Other applications}

Hitherto have evaluated on Quincy-style flow graphs. But my algorithms are fully general, will operate on any flow network. How does it fare on other graphs? Make clear this wasn't part of the original goal of the project.

\section{Summary}

TBC: Summary