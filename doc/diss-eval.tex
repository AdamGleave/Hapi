\chapter{Evaluation} \label{chap:eval}

% Proofread: None

\section{Project requirements} 

\begin{table}
    \centering
    % eliminate section prefix
    \crefformats{chapter,section,subsection}{#2#1#3}
    \crefnames{chapter,section,subsection}{}{}
    \begin{tabular}{clcccc}
        \textbf{\#} & \textbf{Deliverable} & \textbf{Section}
        \tabularnewline
        \hline
        & \textit{Success criteria} \tabularnewline
        S1 & Implement standard flow algorithms & \cref{sec:impl-cycle-cancelling,sec:impl-ssp,sec:impl-relax,sec:impl-cost-scaling} \tabularnewline
        S2 & Design an approximate solver & \cref{sec:impl-approx} \tabularnewline
        S3 & Integrate system with Firmament & \cref{sec:eval-test-integration} \tabularnewline
        S4 & Develop a benchmark suite & \cref{sec:eval-benchmark-strategy} \tabularnewline
        \hline
        & \textit{Optional extensions} \tabularnewline
        E1 & Design an incremental solver & \cref{sec:impl-incremental} \tabularnewline
        E2 & Build cluster simulator & \cref{sec:impl-firmament} \tabularnewline
        E3 & Optimise algorithm implementations & \cref{sec:eval-optimisations} \tabularnewline
        \hline
    \end{tabular}
    % restore section values
    \crefsections
    \caption{Deliverables for the project}
    \label{table:eval-project-requirements}
\end{table}

All success criteria set forth in the project proposal (see \cref{appendix:proposal}) have been met, with many optional extensions implemented. \Cref{table:eval-project-requirements} summarises the work completed.

\todo{Too short?}

\section{Correctness testing}

% For a project of this scope, it is easy for 

% Could link this back to development approaches
% Or just some generic bilge
% Tested individually and when integrated into the system
% Essential for each component to be verified: e.g. standard algorithm implementation being wrong could cause bugs which are hard to track down.

Unit tests were developed for each component. In accordance with the spiral model proposed in \cref{sec:prep-management-model}, these were re-run as a regression test at the end of each development cycle. Integration tests were also performed as soon as the requisite components had been implemented.

\subsection{Unit tests} \label{sec:eval-testing-unit}

Each flow algorithm was tested by comparing the solutions found to those produced by a reference implementation. The test dataset used is outlined in \cref{table:correctness-test-dataset}, and consists of some nnn\todo{Insert exact figure} GB of both natural and synthetic flow networks.

\begin{table}
    \begin{tabular}{c}
        TBC
    \end{tabular}
    % N.B. Include incremental test sets as well!
    \caption{Datasets for correctness testing}
    \label{table:correctness-test-dataset}
\end{table}

Standard unit test frameworks are not designed to work with large, external datasets such as these. Consequently, I developed my own test harness in Python to automate verification of flow algorithms. \Cref{fig:unit-test-harness} shows an extract from a test session.

\begin{figure} 
    TBC
    \caption{Unit test harness}
    \label{fig:unit-test-harness}
\end{figure}

The aforementioned tests verify entire flow algorithms. Unit tests were also written for smaller components, such as data structures, using the GTest framework (see \cref{sec:prep-tools-libraries}).

\subsection{Integration tests} \label{sec:eval-test-integration}

In practice, the algorithms developed in this project would be used as part of a larger cluster scheduling system. To demonstrate this capability, the solvers implemented in this project were integrated into the Firmament system. \Cref{fig:firmament-ui} shows the system in action, on a cluster of 14 machines.

\begin{figure}
    TBC
    \caption{Firmament scheduling tasks using this project's solver}
    \label{fig:firmament-ui}
\end{figure}

\section{Performance testing strategy} \label{sec:eval-benchmark-strategy}

TBC: Intro

\subsection{Simulating a Google cluster}

This project seeks to develop algorithms which will achieve sub-second scheduling latency on warehouse-scale computers, comprising many thousands of compute nodes. Unfortunately, it is not practical to run experiments on such clusters. Performance evaluation must consequently take place using a simulated cluster\footnotemark.
\footnotetext{This is a limitation common to most distributed systems research, not just Part II projects. Even the Quincy system, with all the resources of Microsoft Research behind it, was only ever tested on a small cluster of 243 machines.}

Firmament includes a cluster simulator, however I modified it extensively to support this project (see \cref{sec:impl-firmament}). To ensure the test is realistic, the simulator replays a month-long trace of events from a production Google cluster of 12,000 machines~\cite{clusterdata:Wilkes2011,clusterdata:Reiss2011,Reiss:2012}. This is by far the most detailed trace released by any major cluster operator, enabling an accurate simulation.\todo{Do you agree with this? I've not come across any comparable traces, but I don't feel confident I've not missed anything}

However, the trace is not perfect. To protect commercially sensitive information, much of the data was obfuscated by Google before release. Moreover, some data was simply never recorded: for example, the input file size of tasks. 

Although there is sufficient information to reconstruct the topology of the flow network, some cost models require data that is absent from the trace. The next two sections describe the cost models implemented in simulation, and how the limitations of the dataset were overcome.

\subsection{Octopus cost model}

The Octopus model implements a simple load balancing policy. Tasks and machines are assumed to be homogeneous. The cost of arcs into a machine are proportional to the load on that machine, resulting in tasks being preferentially scheduled on idle machines. 

This policy is too simplistic to be used in production. However, it serves as a useful baseline for comparison. Producing flow networks that are easy to solve compared to more realistic cost models, it provides an upper bound on the real-world scheduling latency that can be achieved.

\subsection{Quincy cost model}

Flow scheduling was pioneered by the Quincy system, developed at Microsoft Research~\cite{Isard:2009}. To enable a direct comparison between the two systems, I implemented the Quincy cost model in Firmament (see \cref{sec:prep-flow-scheduling,sec:impl-firmament}).

In contrast to the Octopus model, Quincy has direct real-world applicability: throughput increased by 40\% and data transfer was reduced by a factor of 3.9~\cite{Isard:2009}. The flow networks produced are considerably more complex, being representative of realistic cost models. Moreover, this complexity can be controlled by specifying the length of the preference lists, supporting scalability testing of the algorithms.

Quincy optimises for data locality, scheduling tasks close to where their input data is stored. To achieve this, the cost of arcs from a task to a machine is made proportional to the network bandwidth consumed were the task to be scheduled on that machine. The costs therefore depend upon location of blocks of input data for a task in the distributed filesystem.

Utilising this fine-grained resource information is a major strength of the Quincy system, and flow scheduling in general. However, it poses a challenge for simulation: such detailed information is not present in the trace, and must instead be estimated. 

Upon starting the simulator, a virtual distributed filesystem is built. Each machine is given 6 TB of simulated storage space, in line with typical specifications for machines in contemporary Google clusters~\cite{GoogleSlideDeck}. Files are replicated across three machines, in accordance with industry standards\footnotemark. Each file consists of 64 MB blocks, as in the Google File System~\cite{Ghemawat}.
\footnotetext{In practice, the replication factor varies with the type of data. For example, it would be higher for critical information such as login credentials, and lower for data such as old e-mail attachments.}

A collection of files is generated to saturate the available storage capacity\footnotemark, with file sizes randomly sampled from a distribution. Unfortunately, Google has not released any information on the file size distribution observed inside the cluster. Instead, I have used data from a similarly sized Facebook cluster to estimate a distribution~\cite{Chen}.
\footnotetext{In practice, a cluster would never run at full disk utilisation, however this distinction is unimportant for this test.}

When a task is submitted, an estimate of its input size $S$ is made. A set of input files is then assigned to the task, by randomly sampling from the distributed filesystem until the cumulative size reaches $S$. Google has not released any information on the input size distribution. For consistency, I have used data from the same Facebook cluster to estimate a distribution~\cite{Chen}.

However, it is possible to do better than simply randomly sampling from this distribution. The Google cluster trace contains information about the runtime of individual tasks, and there is a correlation between runtime and input size. Consequently, I have taken the approach of computing the cumulative probability of the runtime for each submitted task, assigning an input size of the same cumulative probability. So, for example, a task of median runtime will be assigned a median input file size.\todo{Does this explanation make sense to you?}

Given the paucity of the trace data, the simulation cannot be expected to exactly reproduce the behaviour of the original Google cluster. However, I believe that the simulation represents a realistic workload, which could plausibly occur in a production cluster today. 

The exact performance of the algorithms will, of course, depend on the architecture and workload of individual clusters. Most metrics used in this chapter therefore measure \emph{relative} performance, which should be robust to such differences. Absolute runtimes are occasionally reported: while indicative of real-world performance, care should be taken not to generalise beyond the experiments conducted.

\subsection{Test methodology}

Care was taken to minimise experimental error throughout the test process. However, error cannot be completely eliminated: some variation in runtime is inevitable when executing on a time-sharing operating system. Confidence intervals were computed using Student's $t$-distribution to quantify the error, which in most cases is negligible.

Each test was executed multiple times, with the runtimes across test runs aggregated to increase precision. Five iterations were performed by default, with more test runs being performed if the confidence interval was too wide.

Tests typically compared multiple algorithms, operating on the same dataset. Within each test run, algorithms were executed round-robin. This has the desirable property that each algorithm would tend to be equally affected if the performance of the server were to change over time.

However, significant effort was made to minimise variations in performance between runs. Tests took place on machines dedicated to the experiment, ensuring no interference from other workloads. All machines have the same specification (see \cref{appendix:test-machine-spec}), except where explicitly stated, allowing for absolute runtimes to be compared between experiments.

The algorithms were evaluated at multiple scales, ranging from small clusters that might be used within a private company to warehouse-scale computers used to support large cloud applications. \Cref{table:cluster-sizes} summarises the sizes used throughout the rest of the chapter.

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \textbf{Name} & \textbf{Percentage of Google cluster} & \textbf{Number of machines} \tabularnewline
        \hline
        Small & 1\% & 120 \tabularnewline
        Medium & 5\% & 600 \tabularnewline
        Large & 25\% & 3000 \tabularnewline 
        Warehouse-scale & 100\% & 12,000 \tabularnewline
    \end{tabular}
    \caption{Cluster sizes used in testing}
    \label{table:cluster-sizes}
\end{table}

Running the tests manually would be error-prone, as well as extremely time consuming. A test harness was developed in Python to automate the process, described further in \cref{appendix:benchmark-harness}. This ensures the above methodology is followed for all tests.

%* Ensure each algorithm runs at peak performance, before doing comparisons between algorithms. Optimisations, compilers.
%* Say what you're measuring? Perhaps better introduced in each section.

% Specify what properties we're recording? Online vs offline tests?

% OLD STUFF

%In a real cluster, Firmament will start a new scheduling round as soon as the previous round has completed, provided any events are pending. By contrast, the simulator started scheduling rounds at fixed time intervals. 
%
%This difference was mostly harmless for traditional `full' solvers, whose runtime does not depend on the number of changes.  But for incremental solvers, the difference is significant. The longer the period between scheduling rounds, the more changes there are, increasing the solver's runtime. The simulator defaulted to a very short fixed time interval, which unfairly skewed the test in favour of incremental solvers. 
%
%To resolve this problem, the time between scheduling rounds was made proportional to the runtime of the solver, as it would be in a real cluster. The statistic reported is no longer the runtime of the algorithm, but the worst-case scheduling latency experienced by an event\footnotemark. This has the added benefit of increasing the realism of the simulation for `full' solvers as well.
%\footnotetext{To see why these are different, suppose an event arrives just after a scheduling round begins. It must wait until the solver finishes, before a new scheduling round begins. The event is only processed after the \emph{second} scheduling round finishes, so the event has had to wait for (almost) two executions of the solver.}

% Alternative -- evaluation of standard algorithms? Then have comparative eval section at the end? But that result will look bad... is it worth it? Perhaps ask Malte.
\section{Optimisations} \label{sec:eval-optimisations}

This project is primarily concerned with algorithmic improvements, however implementation decisions can have a considerable impact on the performance of flow solvers. Comparing an optimised implementation of one algorithm against an unoptimised version of another algorithm could give misleading results. Consequently, care has been taken to ensure that all algorithms considered in this project are implemented efficiently. This section evaluates the optimisations applied to each implementation, describes how parameters were set for each algorithm and how a compiler was selected.

%Structure? You probably do want to merge optimisations into a single figure, as Malte suggested. Or at least, a single figure for each algorithm. FIFO is on it's own, but that's about it.
%
%Scaling factor: you'll do this for both your algorithm and Goldie's.
%
%Compilers: everything. But it's boring so you might just want an extract.
%
%So you could have:
%
%* Optimisations: one section for each algorithm, keep it brief.
%* Scaling factor: brief discussion, just for cost scaling.
%* Compilers.
%
%Alternatively:
%
%* Big figure
%* Successive shortest path - ref figure
%* Relax - ref figure
%* Cost scaling - ref figure, and include it's own figure for scaling factor
%* Compilers - everything
%
%Think I favour second approach. You might wanna make this big master figure, though, since it's kinda dependent upon that actually working. Although guess you could break it up, so maybe no dependency... Think it works better if optimisations build on each other. So e.g. small heap + Djikstra with map? Think it wouldn't be too much work to restructure it to do this, the only one where you have interactions is AP. Datasets: only worth having multiple ones when it varies by scale, really.
\subsection{Successive shortest path algorithm}

\begin{figure}
    \centering
    %\includegraphics{opt/ap_1col}
    \caption{Speedup relative to naive successive shortest path implementation with standard Djikstra and big heap}
    \label{fig:opt-ap}
\end{figure}

As explained in \cref{sec:impl-ssp-optimisations}, it is possible to terminate Djikstra's algorithm as soon as it finds a shortest path to a deficit node, without needing to compute shortest paths to all nodes in the graph. As \cref{fig:opt-ap} demonstrates, this so-called \emph{partial Djikstra} approach yields a considerable performance improvement.

This inspired a related optimisation, \emph{small heap}. Djikstra's algorithm works by maintaining a priority queue of vertices, implemented using a binary heap in this project (see \cref{sec:impl-ssp-optimisations} for justification of this choice). This results in a $\Theta\left(\lg l\right)$ complexity for priority queue operations, where $l$ is the length of the queue.

In the \emph{big heap} implementation, the heap is initialised to contain all vertices in the graph. \emph{Small heap}, by contrast, is initialised containing just the source vertex. Vertices are inserted into the queue as they are encountered during graph traversal.

Initialising a heap with $n$ elements has cost $\Theta(n)$; by contrast, inserting the elements one at a time has a cost of $\Theta(n\lg n)$. Big heap, therefore, will pay less of an initialisation cost --- at least under standard Djikstra, where each vertex is eventually inserted into the queue. However, the number of elements $l$ in the heap will tend to be larger throughout the lifetime of big heap, making each operation more expensive.

When using standard Djikstra, the performance of the two heap implementations is practically indistinguishable. However, small heap provides a considerable performance gain when used in conjunction with partial Djikstra. This is to be expected: with early termination, the typical length of the queue $l$ is much smaller than the number of vertices $n$ in the graph.

Two versions of small heap were implemented. To support decreasing the key of elements in the priority queue (a common operation in Djikstra's algorithm), it is necessary to maintain a \emph{reverse index} mapping node IDs to elements in the priority queue. In the big heap, this was implemented using an \emph{array} of length $n$. For the small heap, it is possible to reduce memory consumption by using a \emph{hash table}, storing mappings only for nodes $v$ present in the priority queue.

In practice, the algorithm is not memory bound, and runtime is the primary consideration. Both the array and hash table offer $O(1)$ lookup. The array might be expected to be faster: there is no need to compute a hash function, and a result will always be returned after a single memory read (whereas for the hash table, multiple reads may be required in the event of a collision.) However, the hash table may be faster for large heaps: it can be more efficiently cached, since there is less dead space than in the array.

The results clearly show that the array implementation outperforms the hash table version, at all cluster sizes. It may be that the array is never large enough for capacity cache misses to become a problem. Furthermore, hash tables do not necessarily offer a space advantage. The \emph{load factor}, the number of elements divided by the number of buckets, must be kept low to avoid collisions. Consequently, many buckets will be empty, wasting space.

\todo{Should I say which version of the algorithm I ended up using? It seems obvious -- the one which had the best runtime -- but perhaps it's best stated explicitly?}

\subsection{Relaxation algorithm}

\begin{figure}
    \centering
    \includegraphics{opt/relax_arc_cache_relative_2col}
    \caption{Speedup when caching arcs}
    \label{fig:opt-relax-cache-arcs}
\end{figure}

Relaxation algorithm requires knowing the arcs crossing the cut. None: iterate over all arcs and find out which ones cross the cut. Cache zero cost: maintain a list of zero reduced cost arcs crossing the cut. Cache all: maintain two lists, one of zero reduced cost and one of positive reduced cost arcs crossing the cut.

\subsection{Cost scaling}

\subsubsection{Wave vs FIFO}

\begin{figure}
    \centering
    \includegraphics{opt/cs_wave_vs_fifo_relative_2col}
    \caption{Speedup when using FIFO rather than wave}
    \label{fig:opt-cs-wave-vs-fifo}
\end{figure}

% Could also test out first-active approach: when relabel happens, add s to front of Q rather than rear

\subsubsection{Scaling factor}

\begin{figure}
    \centering
    \includegraphics{opt/cs_scaling_factor_2col}
    \caption{Performance depending on scaling factor}
    \label{fig:opt-cs-scaling-factor}
\end{figure}

What factor to reduce $\epsilon$ by on each iteration?

\subsection{DIMACS Parser}

\begin{figure}
    \centering
    \includegraphics{opt/parser_set_vs_getarc_relative_2col}
    \caption{Speedup when maintaining cache of arcs present}
    \label{fig:opt-dimacs-parser}
\end{figure}

Need to check for duplicate arcs when parsing. Linked list: consult underlying data structure. Set: maintain a separate set of the arcs present to speed lookup.

Set actually works out slower. Conjecture: adjacency lists tend to be short, so overhead of linked list is low. Maintaining separate set harms cache performance, as increases working set?

\todo{Bother including this section or not? It's kind of trivial. Although discussing why the optimisation *didn't* work is maybe interesting.}

\subsection{Compilers}

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/cc_absolute_2col}
        \caption{Cycle cancelling}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/ap_absolute_2col}
        \caption{Augmenting Path}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/relax_absolute_2col}
        \caption{Relaxation}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/cs_absolute_2col}
        \caption{Cost scaling}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/goldberg_absolute_2col}
        \caption{Goldberg's cost scaling}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{com/frangioni_absolute_2col}
        \caption{Frangioni's relaxation algorithm}
    \end{subfigure}
    \end{widepage}
    \caption{Performance under different compilers and optimisation levels}
    \label{fig:compilers}
\end{figure}

Want to make each implementation run as fast as possible: necessary for later tests to be fair. Different compilers use different optimisation techniques, and so may work better for some applications than others. Evaluated Clang and GCC, two compilers with sophisticated optimisations.

Tested at all optimisation levels. (Table showing the levels? O0 to O3, with brief explanation.) Significant performance improvement when going to O2; limited performance gains in O3, perhaps even worse.

Little difference between Clang and GCC. GCC O3 tended to be the fastest implementation. Given this, GCC O3 was chosen as the standard compiler for subsequent tests.

Probably best not to dwell on this since results aren't that interesting, but worth mentioning it was carried out for thoroughness. Probably want to compact the results significantly. Maybe OK to just give example output from one or two implementations, and mention similar for others.

\section{Performance evaluation}

Tests on final version of algorithms, with all optimisations enabled.

\subsection{Approximation algorithm} \label{sec:eval-approx}

% Reference from Impl:Cost Scaling:Heuristics expects you to justify why comparing against my own cost scaling implementation is legit.

\begin{figure}
    \centering
    \includegraphics{app/road_over_time_2col}
    \caption{Successive approximations to the optimal solution}
    \label{fig:app-cost-over-time}
\end{figure}

\cref{fig:app-cost-over-time} perhaps best in implementation section, to illustrate how algorithm works.

\begin{figure}
    \centering
    \includegraphics{app/road_oracle_policy_2col}
    \caption{Upper bound on speedup of approximation algorithm for a given accuracy}
    \label{fig:app-oracle-policy}
\end{figure}

\cref{fig:app-oracle-policy} oracle policy: gives an upper bound on performance that can be achieved with an approximation algorithm.

\begin{figure}
    \centering
    \includegraphics{app/road_policy_parameter_accuracy_1col}
    \caption{Accuracy at percentiles for given parameter choices}
    \label{fig:app-policy-parameter-accuracy}
\end{figure}

\cref{fig:app-policy-parameter-accuracy} is computed on training data, to determine a suitable parameter for the heuristic.

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
      \includegraphics[width=\textwidth]{app/road_policy_error_cdf_2col}
      \caption{CDF of heuristic error}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
      \includegraphics[width=\textwidth]{app/road_policy_speed_cdf_2col}
      \caption{CDF of speed under heuristic (compared to optimal performance under oracle)}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of heuristic}
    \label{fig:app-cdf}
\end{figure}

The heuristic is then used with the given parameter, and \cref{fig:app-cdf} evaluates its performance on (unseen) test data.\todo{XXX: In this example they're actually the same data set, but that'll be fixed in real tests}

%\subsubsection{Comparison with optimal algorithms}
%
%Set threshold for approximation algorithm so that it produces 'nearly' optimal results. How does performance compare? (In early tests, it was a big improvement, but need to replicate.)
%
%\subsubsection{Performance-accuracy tradeoff}
%
%How much accuracy do we have to give up to get to a certain performance level? Is there a range of parameters which is clearly best: e.g. can we get a big speedup for a small loss of accuracy, but after a point we have to make big sacrifices in accuracy for small speedups?
%
%\subsubsection{Impact of loss of accuracy}
%
%Test an approximate algorithm on cluster. How much does performance degrade?

\subsection{Incremental algorithm} \label{sec:eval-incremental}

\subsubsection{Dataset}

Google cluster trace. Justify usage: representative of actual operations encountered.

\subsubsection{Comparison with non-incremental, same type}

Runtime compared to the {\it same} algorithm, running from scratch. Bit of a strawman: augmenting path is slow from scratch. But would still be interesting to see: what \% of work are we still having to do?

\paragraph{Augmenting path}

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{inc/same_ap_over_time_1col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_ap_cdf_2col}
    \end{subfigure}
   \begin{subfigure}[c]{0.5\textwidth}
       \includegraphics[width=\textwidth]{inc/same_ap_incremental_only_cdf_2col}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of augmenting path in incremental vs standard mode}
    \label{fig:inc-same-ap}
\end{figure}

\paragraph{Relaxation}

\begin{figure}
    \begin{widepage} 
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relax_over_time_1col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relax_cdf_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relax_incremental_only_cdf_2col}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of relaxation in incremental vs standard mode}
    \label{fig:inc-same-relax}
\end{figure}

My version in \cref{fig:inc-same-relax}.

\begin{figure}
    \begin{widepage} 
    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relaxf_over_time_1col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relaxf_cdf_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/same_relaxf_incremental_only_cdf_2col}
    \end{subfigure}
    \end{widepage}
    \caption{Performance of relaxation in incremental vs standard mode}
    \label{fig:inc-same-relaxf}
\end{figure}

Frangioni's version in \cref{fig:inc-same-relaxf}. \todo{Tempted to cut this entirely, and just consider it later in comparative evaluation.}

\subsubsection{Comparison with non-incremental, different type}

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_my_over_time_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_my_cdf_2col}
    \end{subfigure}
    \end{widepage}
% These figures just aren't that interesting, similar scale to main one
%    \begin{subfigure}[c]{0.45\textwidth}
%        \includegraphics{inc_head_to_head_my_incremental_only_cdf}
%    \end{subfigure}
    \caption{Performance of incremental augmenting path vs standard cost scaling}
    \label{fig:inc-head-to-head-my}
\end{figure}

Compare to cost scaling, my implementation. (Comparison with reference implementations comes later.) \todo{I've disabled displaying RELAX here; it spikes to 30s occassionally. Try it again when you have a cost model which doesn't trigger the pathological case.}

\section{Scalability analysis}

\subsection{Growing size of network}

Increase number of nodes, but keep structure.

\subsection{Increasing complexity of network}

e.g. increase number of preference arcs, or similar. Simulate some more complicated scheduling policy.

\subsection{Large incremental changes}

For incremental algorithm only. Simulate large job being added with lots of tasks, or many machines going offline.

\subsection{Different cost models}

How does performance vary on new / more complicated cost models?

\section{Comparative evaluation} \label{sec:eval-comparative}

Compare performance with reference implementations, such as Goldberg. Test only those algorithms found to be most competitive in the previous section -- no point testing algorithms which I already know are suboptimal.

\begin{figure}
    \begin{widepage}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_optimised_over_time_2col}
    \end{subfigure}
    \begin{subfigure}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{inc/head_to_head_optimised_cdf_2col}
    \end{subfigure}
    \end{widepage}
% These figures just aren't that interesting, similar scale to main one
%    \begin{subfigure}[c]{0.45\textwidth}
%        \includegraphics{inc_head_to_head_optimised_incremental_only_cdf}
%    \end{subfigure}
    \caption{Performance of optimised versions of incremental relaxation and cost scaling}
    \label{fig:inc-head-to-head-optimised}
\end{figure}

\section{Other applications}

Hitherto have evaluated on Quincy-style flow graphs. But my algorithms are fully general, will operate on any flow network. How does it fare on other graphs? Make clear this wasn't part of the original goal of the project.

\section{Summary}

TBC: Summary