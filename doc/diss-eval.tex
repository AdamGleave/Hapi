\chapter{Evaluation} \label{chap:eval}

% Proofread: None

\section{Project requirements} 

Demonstrate you have satisfied the success criteria specified in proposal.

\section{Testing}

Correctness testing: does this work, not how fast is it.

\subsection{Unit tests} \label{sec:eval-testing-unit}
Some portions of the code covered with GTest: e.g. some aspects of the networks, task allocation code, etc. 

Algorithms, etc. NOT covered with GTest. Use a Python script instead. Justification -- blackbox testing: test the interface. Interface presented by algorithms is effectively just take in data, spit out result, so can test just as effectively from Python running a command, and more flexible.

Mention approach for incremental tests: same basic idea.

\subsection{Integration tests} \label{sec:eval-test-integration}

Show it works in Firmament.

\section{Performance testing strategy} \label{sec:eval-benchmark-strategy}

Reference benchmark suite \ref{sec:impl-benchmark}. 

Focus on methodology. Accuracy: repeat tests multiple time. Run algorithms round-robin to avoid caching effects. Discount time spent parsing: just measure what's relevant. etc.

Explain different categories of test.

\section{Optimisations} \label{sec:eval-optimisations}

Comparisons between different versions of the *same* algorithm implementation. Subsection for each algorithm implemented.

Optimisations may be algorithmic in nature, or more low-level, e.g. laying out data to make use of caches.

\subsection{Successive shortest path algorithm}

\subsubsection{Large vs small heap}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{opt_ap_big_vs_small_relative}
  \caption{Speedup when using small heap}
  \label{fig:opt-ap-big-vs-small}
\end{figure}

Heap used as priority queue in Djikstra's algorithm. Large heap: all nodes in the graph are in the heap. Small heap: only insert nodes into heap once they're visited.

\subsubsection{Terminating Djikstra early}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{opt_ap_full_vs_partial_relative}
    \caption{Speedup when terminating Djikstra early}
    \label{fig:opt-ap-terminate-djikstra-early}
\end{figure}

Partial: terminate Djikstra as soon as a node permanently labelled. Full: solve entire SSSP problem.

\subsection{Relaxation algorithm}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{opt_relax_arc_cache_relative}
    \caption{Speedup when caching arcs}
    \label{fig:opt-relax-cache-arcs}
\end{figure}

Relaxation algorithm requires knowing the arcs crossing the cut. None: iterate over all arcs and find out which ones cross the cut. Cache zero cost: maintain a list of zero reduced cost arcs crossing the cut. Cache all: maintain two lists, one of zero reduced cost and one of positive reduced cost arcs crossing the cut.

\subsection{Cost scaling}

\subsubsection{Wave vs FIFO}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{opt_cs_wave_vs_fifo_relative}
    \caption{Speedup when using FIFO rather than wave}
    \label{fig:opt-cs-wave-vs-fifo}
\end{figure}

% Could also test out first-active approach: when relabel happens, add s to front of Q rather than rear

\subsubsection{Scaling factor}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{opt_cs_scaling_factor}
    \caption{Performance depending on scaling factor}
    \label{fig:opt-cs-scaling-factor}
\end{figure}

What factor to reduce $\epsilon$ by on each iteration?

\subsection{DIMACS Parser}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{opt_parser_set_vs_getarc_relative}
    \caption{Speedup when maintaining cache of arcs present}
    \label{fig:opt-dimacs-parser}
\end{figure}

Need to check for duplicate arcs when parsing. Linked list: consult underlying data structure. Set: maintain a separate set of the arcs present to speed lookup.

Set actually works out slower. Conjecture: adjacency lists tend to be short, so overhead of linked list is low. Maintaining separate set harms cache performance, as increases working set?

\todo{Bother including this section or not? It's kind of trivial. Although discussing why the optimisation *didn't* work is maybe interesting.}

\section{Compilers}

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{com_cc_absolute}
        \caption{Cycle cancelling}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{com_ap_absolute}
        \caption{Augmenting Path}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{com_relax_absolute}
        \caption{Relaxation}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{com_cs_absolute}
        \caption{Cost scaling}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{com_goldberg_absolute}
        \caption{Goldberg's cost scaling}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{com_frangioni_absolute}
        \caption{Frangioni's relaxation algorithm}
    \end{subfigure}
    \caption{Performance under different compilers and optimisation levels}
    \label{fig:compilers}
\end{figure}

Want to make each implementation run as fast as possible: necessary for later tests to be fair. Different compilers use different optimisation techniques, and so may work better for some applications than others. Evaluated Clang and GCC, two compilers with sophisticated optimisations.

Tested at all optimisation levels. (Table showing the levels? O0 to O3, with brief explanation.) Significant performance improvement when going to O2; limited performance gains in O3, perhaps even worse.

Little difference between Clang and GCC. GCC O3 tended to be the fastest implementation. Given this, GCC O3 was chosen as the standard compiler for subsequent tests.

Probably best not to dwell on this since results aren't that interesting, but worth mentioning it was carried out for thoroughness. Probably want to compact the results significantly. Maybe OK to just give example output from one or two implementations, and mention similar for others.

\section{Performance evaluation}

Tests on final version of algorithms, with all optimisations enabled.

\subsection{Approximation algorithm} \label{sec:eval-approx}

% Reference from Impl:Cost Scaling:Heuristics expects you to justify why comparing against my own cost scaling implementation is legit.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{app_road_over_time}
    \caption{Successive approximations to the optimal solution}
    \label{fig:app-cost-over-time}
\end{figure}

\cref{fig:app-cost-over-time} perhaps best in implementation section, to illustrate how algorithm works.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{app_road_oracle_policy}
    \caption{Upper bound on speedup of approximation algorithm for a given accuracy}
    \label{fig:app-oracle-policy}
\end{figure}

\cref{fig:app-oracle-policy} oracle policy: gives an upper bound on performance that can be achieved with an approximation algorithm.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{app_road_policy_parameter_accuracy}
    \caption{Accuracy at percentiles for given parameter choices}
    \label{fig:app-policy-parameter-accuracy}
\end{figure}

\cref{fig:app-policy-parameter-accuracy} is computed on training data, to determine a suitable parameter for the heuristic.

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
      \includegraphics[width=\textwidth]{app_road_policy_error_cdf}
      \caption{CDF of heuristic error}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
      \includegraphics[width=\textwidth]{app_road_policy_speed_cdf}
      \caption{CDF of speed under heuristic (compared to optimal performance under oracle)}
    \end{subfigure}
    \caption{Performance of heuristic}
    \label{fig:app-cdf}
\end{figure}

The heuristic is then used with the given parameter, and \cref{fig:app-cdf} evaluates its performance on (unseen) test data.\todo{XXX: In this example they're actually the same data set, but that'll be fixed in real tests}

%\subsubsection{Comparison with optimal algorithms}
%
%Set threshold for approximation algorithm so that it produces 'nearly' optimal results. How does performance compare? (In early tests, it was a big improvement, but need to replicate.)
%
%\subsubsection{Performance-accuracy tradeoff}
%
%How much accuracy do we have to give up to get to a certain performance level? Is there a range of parameters which is clearly best: e.g. can we get a big speedup for a small loss of accuracy, but after a point we have to make big sacrifices in accuracy for small speedups?
%
%\subsubsection{Impact of loss of accuracy}
%
%Test an approximate algorithm on cluster. How much does performance degrade?

\subsection{Incremental algorithm} \label{sec:eval-incremental}

\subsubsection{Dataset}

Google cluster trace. Justify usage: representative of actual operations encountered.

\subsubsection{Comparison with non-incremental, same type}

Runtime compared to the {\it same} algorithm, running from scratch. Bit of a strawman: augmenting path is slow from scratch. But would still be interesting to see: what \% of work are we still having to do?

\paragraph{Augmenting path}

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.9\textwidth}
        \includegraphics[width=\textwidth]{inc_same_ap_over_time}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_same_ap_cdf}
    \end{subfigure}
   \begin{subfigure}[c]{0.45\textwidth}
       \includegraphics[width=\textwidth]{inc_same_ap_incremental_only_cdf}
    \end{subfigure}
    \caption{Performance of augmenting path in incremental vs standard mode}
    \label{fig:inc-same-ap}
\end{figure}

\paragraph{Relaxation}

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.9\textwidth}
        \includegraphics[width=\textwidth]{inc_same_relax_over_time}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_same_relax_cdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_same_relax_incremental_only_cdf}
    \end{subfigure}
    \caption{Performance of relaxation in incremental vs standard mode}
    \label{fig:inc-same-relax}
\end{figure}

My version in \cref{fig:inc-same-relax}.

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.9\textwidth}
        \includegraphics[width=\textwidth]{inc_same_relaxf_over_time}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_same_relaxf_cdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_same_relaxf_incremental_only_cdf}
    \end{subfigure}
    \caption{Performance of relaxation in incremental vs standard mode}
    \label{fig:inc-same-relaxf}
\end{figure}

Frangioni's version in \cref{fig:inc-same-relaxf}. \todo{Tempted to cut this entirely, and just consider it later in comparative evaluation.}

\subsubsection{Comparison with non-incremental, different type}

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_head_to_head_my_over_time}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_head_to_head_my_cdf}
    \end{subfigure}
% These figures just aren't that interesting, similar scale to main one
%    \begin{subfigure}[c]{0.45\textwidth}
%        \includegraphics[width=\textwidth]{inc_head_to_head_my_incremental_only_cdf}
%    \end{subfigure}
    \caption{Performance of incremental augmenting path vs standard cost scaling}
    \label{fig:inc-head-to-head-my}
\end{figure}

Compare to cost scaling, my implementation. (Comparison with reference implementations comes later.) \todo{I've disabled displaying RELAX here; it spikes to 30s occassionally. Try it again when you have a cost model which doesn't trigger the pathological case.}

\section{Scalability analysis}

\subsection{Growing size of network}

Increase number of nodes, but keep structure.

\subsection{Increasing complexity of network}

e.g. increase number of preference arcs, or similar. Simulate some more complicated scheduling policy.

\subsection{Large incremental changes}

For incremental algorithm only. Simulate large job being added with lots of tasks, or many machines going offline.

\subsection{Different cost models}

How does performance vary on new / more complicated cost models?

\section{Comparative evaluation} \label{sec:eval-comparative}

Compare performance with reference implementations, such as Goldberg. Test only those algorithms found to be most competitive in the previous section -- no point testing algorithms which I already know are suboptimal.

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_head_to_head_optimised_over_time}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{inc_head_to_head_optimised_cdf}
    \end{subfigure}
% These figures just aren't that interesting, similar scale to main one
%    \begin{subfigure}[c]{0.45\textwidth}
%        \includegraphics[width=\textwidth]{inc_head_to_head_optimised_incremental_only_cdf}
%    \end{subfigure}
    \caption{Performance of optimised versions of incremental relaxation and cost scaling}
    \label{fig:inc-head-to-head-optimised}
\end{figure}

\section{Other applications}

Hitherto have evaluated on Quincy-style flow graphs. But my algorithms are fully general, will operate on any flow network. How does it fare on other graphs? Make clear this wasn't part of the original goal of the project.

\section{Summary}

TBC: Summary