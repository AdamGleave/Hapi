\chapter{Implementation} \label{chap:impl}

\section{Introduction}

As discussed in \S\ref{sec:intro-challenges} and \S\ref{sec:intro-related-work}, considerable research effort has been expended over the past 60 years to produce efficient flow algorithms. Developing a better general-purpose minimum-cost flow algorithm would be a considerable undertaking, more suited to a PhD than a Part II project. Rather than attempting to supplant prior work, my approach is to instead modify and extend it, in order to achieve improved performance in the special-case of networks produced by Quincy-like systems.

Two strategies seemed particularly promising. We could loosen the constraints on the problem, finding \emph{approximate} solutions, discussed in \S\ref{sec:impl-approx}. All extant minimum-cost flow algorithms are designed to find optimal solutions. But for flow schedulers, we may want to trade optimality for reduced scheduling latency. The other strategy is to solve the problem \emph{incrementally}, discussed in \S\ref{sec:impl-incremental}. The network tends to remain mostly unchanged between successive invocations of the scheduler. Suppose a task is submitted: a node will be added, along with a handful of arcs, but no other changes will take place. Significant performance improvements can be realised in cases such as these by reoptimising from the last optimal solution found.

These strategies are not solution methods in and of themselves. Rather, they suggest modifications that can be made to flow algorithms. Consequently, I will start this chapter by describing the implementation of several minimum-cost flow algorithms. Once I have described the standard algorithms implemented in this project, I will move on to discussing the two strategies in more detail. Note that algorithms vary in their suitability to the two strategies; justification will be provided where appropriate.

% TODO: Asymptotic complexity summary somewhere?

\section{Cycle cancelling algorithm}

% Should try and keep fairly consistent structure between algorithms
% Malte, Ionel seemed to think good to briefly discuss properties of the algorithm.
% You'll also want to mention this in the strategies section, though.
% It's important, so think OK to say it twice. But which will be the more detailed one?
% I think depends: if it's specific to a particular algorithm, place it there. But if it's a more general point (e.g. primal vs dual), place it in strategy

Cycle cancelling is the simplest algorithm we will consider. It has little to recommend it from a performance perspective: the original version due to Klein~\cite{Klein:1967}, the focus of this section, is exponential time in the worst case, although in practice its performance is often better than this. Variants have achieved strongly polynomial bounds~\cite{Goldberg:1989,Sokkalingam:2000}, but are still slower both theoretically and empirically than competing algorithms. However, it is a good starting point: it will help cast light on many of the techniques used in more sophisticated algorithms.

\subsection{Algorithm description}

Cycle cancelling is inspired by the following result: \\

\begin{thm}[Negative cycle optimality conditions] \label{thm:optimality-neg-cycle}
A flow $\mathbf{x}$ is an optimal solution to the minimum-cost flow problem if and only if the residual network $G_\mathbf{x}$ has no negative cost (directed) cycle.
\end{thm}
\begin{proof}
This is a standard result in the literature; see, for example, \cite[p.~307]{Ahuja:1993}.
\end{proof}

\begin{algorithm}
    \caption{Cycle cancelling algorithm}
    \label{algo:cycle-cancelling}
    \begin{algorithmic}[1]
        \State $\mathbf{x}\gets $ result of maximum-flow algorithm \Comment{establishes x feasible}
        \While{$G_\mathbf{x}$ contains a negative cost cycle}
        \State identify negative cost cycle $W$ \Comment{e.g. using Bellman-Ford}
        \State $\delta \gets \min_{(i,j) \in W} r_{ij}$
        \State augment $\delta$ units of flow along cycle $W$
        \EndWhile{}
    \end{algorithmic}
\end{algorithm}

The algorithm is initialised with a feasible flow $\mathbf{x}$, which can be found by any maximum-flow algorithm, such as Ford-Fulkerson~\cite{FordFulkerson:1956}. The feasibility of the solution $\mathbf{x}$ is maintained throughout the algorithm, and its cost is reduced. During each iteration of the algorithm, a directed cycle of negative cost is identified in the residual network $G_\mathbf{x}$ and \emph{cancelled} by pushing the maximum possible amount of flow along it. This will cause the cycle to 'drop out' of the residual network: one or more arcs along the cycle will be saturated, and so no longer present in the residual network. The algorithm terminates when no negative cost directed cycles remain.

Note this generic version of the algorithm does not specify \emph{how} negative cycles are to be identified. I opted to use the well-known Bellman-Ford~\cite[p.~651]{CLRS:2009} algorithm for this purpose. Other, more efficient algorithms could be used. However, cycle cancelling was never going to be fast enough for my needs\footnotemark. I chose to implement it so as to have a known-working algorithm early in the project, simplifying subsequent testing. For this purpose, cycle cancelling using the Bellman-Ford algorithm is ideal.

\footnotetext{The asymptotic complexity is considerably worse than that of competing algorithms, and computational benchmarks~\cite{KiralyKovacs:2012} were also unfavourable.}

\subsection{Analysis}

\subsubsection{Correctness}

We will show that, if the algorithm terminates, it produces the correct result. \\

\begin{thm} \label{thm:cycle-cancelling-invariant}
Immediately before each iteration of the loop, $\mathbf{x}$ is a feasible solution.
\end{thm} 
\begin{proof}
For the base case, $\mathbf{x}$ must be initially feasible, by correctness of the maximum-flow algorithm used.

For the inductive case, suppose $\mathbf{x}$ is feasible immediately prior to an iteration of the loop body. The loop body pushes flow along a cycle. This maintains feasibility: the excess at the nodes along the cycle must remain zero, since any increase in the flow leaving the node is counterbalanced by an equal increase in the flow entering the node.
\end{proof}

\begin{cor}
Upon termination, $\mathbf{x}$ is a solution of the minimum-cost flow problem
\end{cor}
\begin{proof}
By \cref{thm:cycle-cancelling-invariant}, $\mathbf{x}$ is a feasible solution upon termination. The algorithm only terminates when no negative-cost directed cycles exist. It follows by \cref{thm:optimality-neg-cycle} that $\mathbf{x}$ is optimal.
\end{proof}

\subsubsection{Asymptotic complexity}

We will now show that the algorithm always terminates, and provide a bound on its running time.\\

\begin{thm} \label{thm:cycle-cancelling-termination}
The algorithm terminates within $O(mCU)$ iterations.
\end{thm}
\begin{proof}
Clearly $mCU$ is an upper bound on the cost of the initial flow. Each iteration of the algorithm identifies a negative cost cycle $w$. The cost of sending one unit of flow along this cycle is $c = \sum_{(i,j) \in w} c_{ij}$, which is strictly negative by definition. We send $\delta = \min_{(i,j) \in w} r_{ij}$ units of flow along the cycle. $\delta$ is strictly positive, otherwise the cycle would not exist in the \emph{residual} network. 

The objective function value changes by $c\delta$. By \cref{assumption:integrality}, $c$ and $\delta$ must both be integral. So as $c\delta < 0$, we have $c\delta \leq 1$.

Hence the cost decreases by at least one each iteration, and so the number of iterations is bounded by $O(mCU)$.
\end{proof}

\begin{cor} \label{corollary:cycle-cancelling-complexity}
The asymptotic complexity is $O(nm^2CU)$.
\end{cor}
\begin{proof}
Note that Bellman-Ford runs in $O(nm)$ time. Augmenting flow along the cycle is of cost linear in the length of the cycle, and so is certainly $O(m)$. Thus each iteration runs in $O(nm)$. By \cref{thm:cycle-cancelling-termination}, it follows the complexity of the algorithm is $O(nm^2CU)$.
\end{proof}

\section{Successive shortest path algorithm}

% Note this algorithm offers polynomial performance for Quincy-like graphs because it is independent of cost, whereas cycle cancelling is not
This algorithm runs in weakly polynomial time. We will show, however, that it has a strongly polynomial time bound of $O(n^2)$ for the class of flow networks produced by Quincy-like systems. It lends itself readily to an incremental implementation (see \S\ref{sec:impl-incremental}), but is inappropriate for an approximate solver.

\subsection{Algorithm description}

\begin{algorithm}
    \caption{Successive shortest path algorithm}
    \label{algo:successive-shortest-path}
    \begin{algorithmic}[1]
        \State $\mathbf{x} \gets \mathbf{0}$ and $\boldsymbol{\pi} \gets \mathbf{0}$
        \While{mass balance constraints not satisfied}
          \State choose node $s$ with $e(s) > 0$ and node $t$ with $e(t) < 0$\footnotemark
          \State solve SSSP problem from $s$ to all other nodes, in the residual network $G_{\mathbf{x}}$ with respect to the reduced costs $c^{\boldsymbol{\pi}}_{ij}$
          \State let $\mathbf{d}$ denote the vector of shortest path distances, s.t. $d_i$ is the shortest path from $s$ to $i\in V$
          \State $\boldsymbol{\pi} \gets \boldsymbol{\pi} - \mathbf{d}$
          \State let $P$ denote a shortest path from $s$ to $t$
          \State $\delta \gets \min\left(e(s), e(t), \min\left\{ r_{ij} \::\: (i,j) \in P\right\}\right)$
          \State augment $\delta$ units of flow along path $P$
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\footnotetext{Note whilst mass balance constraints are unsatisfied there must exist both a source node $s$ and sink node $t$. This is since the total sum of excesses must equal the total sum of deficits, otherwise $\sum_v b(v) \neq 0$ and the problem is infeasible.}

The successive shortest path algorithm maintains a pseudoflow (see \S\ref{sec:prep-flow-pseudo}) $\mathbf{x}$ and potentials $\boldsymbol{\pi}$ which satisfies reduced cost optimality (see \cref{thm:optimality-reduced-cost}), and attempts to attain feasibility. This is in contrast to the cycle cancelling algorithm, which maintains the feasibility strives to achieve optimality.

Each iteration of the algorithm identifies a source node $s$ and sink node $t$. The single-source shortest-path (SSSP) problem~\cite{ch.~24,CLRS:2009} is then solved from $s$. The pseudoflow $\mathbf{x}$ is updated by augmenting along a shortest path to $t$, $P$. We are limited by the minimum residual capacity of arcs along the path $P$. Moreover, we opt to maintain a non-negative supply at $s$ and demand at $t$. This restriction ensures the magnitude of the excess at each node is monotonically decreasing. We also update the potentials $\boldsymbol{\pi}$, to maintain reduced cost optimality. The algorithm terminates when no source nodes or sink nodes exist.

\subsection{Analysis}

\subsubsection{Correctness}

First, we will show that the algorithm maintains reduced cost optimality, for which we will need a number of lemmas. We will then conclude, using the invariant and the terminating condition, that the algorithm will return a solution to the minimum-cost flow problem.\\

\begin{lemma} \label{lemma:ssp-reduced-costs}
Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions \cref{eq:optimality-reduced-cost} with respect to potentials $\boldsymbol{\pi}$. Let $\mathbf{d}$ represent the shortest path distances from a node $s \in V$ to all other nodes in the residual network $G_{\mathbf{x}}$ with respect to the reduced costs $c^{\boldsymbol{\pi}}_{ij}$. Then:
    
\begin{enumerate}[label=(\alph*)]
  \item $\mathbf{x}$ also satisfies reduced cost optimality conditions with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
  \item The reduced costs, $c^{\boldsymbol{\pi}'}_{ij}$, with respect to this new potential $\boldsymbol{\pi}'$, are zero for all arcs $(i,j)$ in the shortest-path tree rooted at $s \in V$.
\end{enumerate}
\end{lemma}
\begin{proof}
See~\cite[p.~320]{Ahuja:1993}.
\end{proof}

\begin{cor} \label{cor:ssp-reduced-costs}
Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions, with respect to some potentials $\boldsymbol{\pi}$. Let $\mathbf{x}'$ denote the pseudoflow obtained from $x$ by sending flow along a shortest path from node $s$ to some other node $k \in V$. Then $x'$ also satisfies the reduced cost optimality conditions, with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
\end{cor}
\begin{proof}
By \cref{lemma:ssp-reduced-costs}(a), $(\mathbf{x},\boldsymbol{\pi'}$ satisfies the reduced cost optimality conditions.

Pushing flow along an arc $(i,j) \in G_{\mathbf{x}}$ might add its reversal $(j,i)$ in the residual network. Let $P$ be a shortest path from $s$ to $k$. By \cref{lemma:ssp-reduced-costs}(b), it follows that any arc $(i,j) \in P$ has $c^{\boldsymbol{\pi}'}_{ij} = 0$. So $c^{\boldsymbol{\pi}'}_{ji} = 0$. Thus any arcs that are added to the residual network by augmenting flow along $P$ have a zero reduced cost, and so still satisfy the reduced cost optimality conditions \cref{eq:optimality-reduced-cost}.
\end{proof}

\begin{thm} \label{thm:ssp-invariant}
Immediately before each iteration of the loop, $(\mathbf{x},\boldsymbol{\pi})$ satisfies reduced cost optimality.
\end{thm}
\begin{proof} (Induction)
    
For the base case, note $(\mathbf{0},\mathbf{0})$ satisfy reduced cost optimality. We have $G_{\boldsymbol{0}} = G$, i.e. the residual and original network are the same. Moreover, all arc costs $c_{ij}$ are non-negative (by \cref{assumption:non-negative-arc-costs}) and so the reduced costs $c^{\boldsymbol{0}}_{ij}$ are also non-negative. Thus \cref{eq:optimality-reduced-cost} holds.

Now, suppose the inductive hypothesis holds: reduced cost optimality holds immediately prior to execution of the loop body. The loop body computes a shortest path distances $s$ from a node $d$, updates $\boldsymbol{\pi}$ to become $\boldsymbol{\pi'}$ as defined in \cref{lemma:ssp-reduced-costs} and pushes flow along a shortest path from $s$ to another node, yielding a new flow $\mathbf{x}$ of the same form as $\mathbf{x'}$ in \cref{cor:ssp-reduced-costs}. It follows by \cref{cor:ssp-reduced-costs} that $(\mathbf{x},\boldsymbol{\pi})$ satisfy reduced cost optimality at the end of the loop body. Hence, the inductive hypothesis continues to hold.
\end{proof}

\begin{thm} \label{thm:ssp-correctness}
Upon termination, $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{thm}
\begin{proof}
The algorithm terminates when the mass balance constraints \cref{eq:mass-balance} are satisfied. At this point, the solution $\mathbf{x}$ is feasible (see \S\ref{sec:prep-flow-pseudo}). 

By \cref{thm:ssp-invariant}, we know the algorithm maintains the invariant that $\mathbf{x}$ satisfies reduced cost optimality. 

It follows that $\mathbf{x}$ is both a feasible flow and optimal upon termination, so $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{proof}

\subsubsection{Asymptotic complexity}

\begin{thm} \label{thm:ssp-complexity}
Let $S(n,m,C)$ denote the time complexity of solving a single-source shortest path problem with non-negative arc costs, bounded above by $C$, over a network with $n$ nodes and $m$ arcs. Then the time complexity of successive shortest path is $O(nUS(n,m,C))$.
\end{thm}
\begin{proof}
Each iteration of the loop body (lines 3-9) decreases the excess of some node $s$ by $k$ and the deficit of some node $t$ by $k$, whilst leaving the excess/deficit of other nodes unchanged. Consequently, the total excess and total deficit are both decreased by $k$. By \cref{assumption:integrality}, $k \geq 1$. So the number of iterations is bounded by the initial total excess. But this is bounded by $nU/2 = O(nU)$. So there are $O(nU)$ iterations.

Within each iteration, the algorithm solves a single-source shortest-path problem on line 4. Since reduced cost optimality is maintained throughout the algorithm, the arc costs in the shortest-path problem are non-negative\footnotemark. Thus the cost of solving this problem is $S(n,m,C)$.
\footnotetext{Algorithms such as Djikstra which assume non-negative arc lengths are asymptotically faster than more general algorithms such as Bellman-Ford.}

The cost of line 3 is $O(n)$, line 6 $O(n)$ and line 8-9 $O(n)$ as the length of $P$ is bounded by $n-1$ (shortest path is acyclic). Certainly $S(n,m,C) = \Omega(n)$, so the cost of each iteration is $O(S(n,m,C))$.

It follows that the overall time complexity of the algorithm is $O(nUS(n,m,C))$.
\end{proof}

\subsection{Optimisations}

% TBC: ap_big_vs_small_heap: keep all vertices in the priority queue, or just add them as needed? Performance will depend on how many vertices get explored before Djikstra quits.
% Unclear whether best to put it here, or in the evaluation section, or both?

\subsubsection{Choice of shortest-path algorithm}
The fastest known single-source shortest-path algorithms are all variants of Djikstra's algorithm~\cite[ch.~4]{Ahuja:1993}. They differ in the heap data structure used to provide the priority queue needed by Djikstra. The asymptotically fastest are Fibonacci heaps, with $S(m,n,C) = O(m + n\lg n)$. By contrast, the widely used binary heap data structure gives $S(m,n,C) = O(m\lg n)$. 

Fibonacci heap's have considerable implementation complexity, however: whilst asymptotically faster, the constant factor hidden by the asymptotic notation is much greater. Computational benchmarks have found them to be slower than binary heaps in practice, for all but the largest of graphs~\cite[p.~15]{KiralyKovacs:2012}. In any case, for the class of networks produced by Quincy-like systems, $m = O(n)$, and so the two are asymptotically equivalent. Given this, I opted for a binary heap implementation. The resulting the asymptotic complexity of my successive shortest path implementation is $O(nmU \lg n)$.

\subsubsection{Terminating Djikstra's algorithm early}

% TBC: Test this using benchmark suite and include in evaluation section? Doesn't seem like it'd be too difficult.
It is possible to modify the successive shortest path algorithm to terminate Djikstra as soon as it permanently labels a deficit node $l$. Whilst not affecting asymptotic complexity, this may considerably improve performance in practice.

\begin{lemma}
Recall \cref{lemma:ssp-reduced-costs}. Let us redefine:
\[\boldsymbol{\pi}'_{i}=\begin{cases}
\boldsymbol{\pi}_{i}-d_{i} & ,\: i\:\mbox{permanently labelled}\\
\boldsymbol{\pi}_{i}-d_{l} & ,\: \:\mbox{otherwise}
\end{cases}\]

The original result for (a) still holds. The result for (b) holds along the shortest path from $s$ to $l$\footnotemark.
\footnotetext{Note that this is all that is needed for the correctness of the algorithm, as this is the only path along which we augment flow.}
\end{lemma}
\begin{proof}
The original proof for the lemma (given in~\cite[p.~320]{Ahuja:1993}) uses the triangle inequality:
\begin{equation} \label{eq:ssp-reduced-costs-triangle}
d_j \leq d_i + c^{\boldsymbol{\pi}}_{ij}\:\forall(i,j)\in E_{\mathbf{x}}
\end{equation}
Terminating Djikstra's algorithm early, we only know the shortest-path distance to nodes which have been permanently labelled. But for any node $i$ not permanently labelled and node $l$ permanently labelled, the shortest path distances satisfy:
\begin{equation} \label{eq:ssp-djikstra-labelled}
d_i \geq d_l
\end{equation}
This is because were $d_i < d_l$ then $i$ must have been permanently labelled before $l$, which is a contradiction.

Now, let us define: 
\begin{equation} \label{eq:ssp-djikstra-distances}
d'_{i}=\begin{cases}
d_{i} & ,\: i\:\mbox{permanently labelled}\\
d_{l} & ,\:\mbox{otherwise}
\end{cases}
\end{equation}

Given \cref{eq:ssp-djikstra-labelled}, we know $\mathbf{d}'$ satisfies \cref{eq:ssp-reduced-costs-triangle}. The original proof for (a) thus still holds, as it makes no further assumptions on $\mathbf{d}'$.

As for (b), every node $i$ along the shortest path from $s$ to $l$ has been permanently labelled, and so $d'_i = d_i$. Hence the original proof still holds along this path.
\end{proof}

Any constant shift in the potential for every node will leave reduced costs unchanged. So we may equivalently redefine:

\[\boldsymbol{\pi}'_{i}=\begin{cases}
\boldsymbol{\pi}_{i}-d_i+d_l & ,\: i\:\mbox{permanently labelled}\\
\boldsymbol{\pi}_{i} & ,\: \:\mbox{otherwise}
\end{cases}\]

This is computationally more efficient, as we will not have to update the potential at any node which has not been permanently labelled (typically the majority of nodes).

\section{Relaxation algorithm}

\section{Cost scaling algorithm}

Describe the algorithm. Can be brief: it's not work you've done, after all. But make sure to communicate how complicated the algorithm is.

\subsection{Algorithm description}

Brief summary.

\subsubsection{Data structures}

\subsubsection{Push operation}

\subsubsection{Relabel operation}

\subsubsection{Complete algorithm}

\subsection{Analysis}

Cite paper giving proof of correctness. State, with citation, time and space complexity achieved.

\subsection{Heuristics}

Summarise what they are, even if you don't have to implement them.

\subsection{Optimisations}

e.g. vertex vs wave. Perhaps this should be merged with heuristics anyway?

\section{Approximation algorithms} \label{sec:impl-approx}

\subsection{Choice of algorithm} \label{sec:impl-approx-choice}

Why Goldberg's cost scaling. Iterative, so readily amenable to approximate solutions. So are others: e.g. cycle cancelling. But Goldberg is fastest: forward reference to benchmarks you've run, or just Kiraly \& Kovacs paper.

\subsection{An iterative approximation}

Approximate algorithm yielded by ending iteration before optimality reached. We have a choice of termination condition.

No way to go from $\epsilon$-optimality to measure of accuracy. So must adopt heuristic approaches.

\subsubsection{Convergence based on cost change}

\subsubsection{Convergence based on task allocations}

\subsubsection{Hybrid schemes}

My code allows the two to be readily combined to produce different policies.

\section{Incremental algorithms} \label{sec:impl-incremental}

Motivation. Most of the time the graph changes only a small amount. Existing approaches recompute from scratch. Can we re-use information?

\subsection{Choice of algorithms}

Explain why primal methods such as Goldberg's work poorly: require solution is feasible at all times. Dual methods better. Note these are slower when run on graph from scratch.

\subsection{Dual algorithms}

Explain how an incremental solver can be built from (any) dual algorithm. Show how reduced cost optimality conditions can be preserved, whatever change occurs.

\section{Input and output}

% Drop this section if short of space, or punt to appendix. Optimisations probably the most useful/interesting ones to talk about.

Loading networks, exporting results. Neither interesting nor impressive, but it is a necessary part of the project. Keep it brief. (Or cut it entirely?)

\subsection{DIMACS}

Representation for network, and network solution. Justify why: standard, widely used, readily available test data, integration with Firmament. 

\subsection{Incremental extension}

Why DIMACS isn't suitable for incremental problem: reloading the graph and computing diff prohibitive cost. 

\subsection{Parser optimisations}

Ignore zero-capacity arcs. Entirely trivial, but did produce noticeable performance improvement. Maybe worth mentioning.

Ignoring duplicates. Necessary for correctness. Slightly more interesting: checking whether an arc is present is slow in some data structures (where the corresponding algorithm doesn't require manual lookup). Parser maintains a bitmap instead to check this quickly irrespective of data structure. Considerable performance improvement. 

\subsection{Scheduling tasks}

Show how solver can achieve original task, by integrating into a cluster scheduler. Choice of Firmament: can be briefly justified, but need not be at all (no real alternatives).

\section{Benchmark suite} \label{sec:impl-benchmark}

% TODO: Maybe include modifications to Firmament in this or a separate section

Motivation. Automate common task. Improve quality of data collection: can run experiment many times with different parameters, no possibility of human error. 

\subsubsection{Architecture}

Specify different implementations by Git 'treeish', and path. Will checkout and build automatically. 

Test cases specify implementations required, I/O parameters, number of iterations.

Output CSV file with statistics.

\subsubsection{Testing on full networks}

Non-incremental case.

\subsubsection{Testing on incremental changes}

Can test two incremental versions head-to-head. Alternately, can test an incremental version compared to applying the diff and running a full solver from scratch.