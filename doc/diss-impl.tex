\chapter{Implementation} \label{chap:impl}

% Proofread: None

\section{Introduction}

Considerable research effort has been expended over the past 60 years to produce efficient flow algorithms, as discussed in \S\ref{sec:intro-challenges} and \S\ref{sec:intro-related-work}. Developing an improved general-purpose minimum-cost flow algorithm would be a substantial undertaking, better suited to a PhD than a Part II project. My approach has been to embrace prior work rather than attempting to supplant it, adapting existing algorithms to improve performance in the special case of flow scheduling.

Two strategies seemed particularly promising. \emph{Approximate} solutions to the problem can be found, described in \S\ref{sec:impl-approx}. Existing algorithms always seek to find optimal solutions: for flow scheduling, we may be happy to trade optimality for reduced scheduling latency. 

Alternatively, the problem can be solved \emph{incrementally}, outlined in \S\ref{sec:impl-incremental}. The network remains mostly the same between runs of the scheduler. Significant performance improvements can be realised by reoptimising from the last optimal solution found.

These strategies are not solution methods in and of themselves. Rather, they suggest modifications that can be made to minimum-cost flow algorithms. Consequently, this chapter starts with an outline of the standard algorithms implemented in this project, before going on to describe the two strategies.

% TODO: Asymptotic complexity summary somewhere?

\section{Cycle cancelling algorithm} \label{sec:impl-cycle-cancelling}

% Should try and keep fairly consistent structure between algorithms
% Malte, Ionel seemed to think good to briefly discuss properties of the algorithm.
% You'll also want to mention this in the strategies section, though.
% It's important, so think OK to say it twice. But which will be the more detailed one?
% I think depends: if it's specific to a particular algorithm, place it there. But if it's a more general point (e.g. primal vs dual), place it in strategy

Cycle cancelling is the simplest algorithm we will consider. It has little to recommend it from a performance perspective: the original version due to Klein~\cite{Klein:1967} is exponential time in the worst case, although in practice its performance is often better than this. Variants have achieved strongly polynomial bounds~\cite{Goldberg:1989,Sokkalingam:2000}, but are still slower both theoretically and empirically than competing algorithms. However, it will help cast light on many of the techniques used in more sophisticated algorithms, so forms a good starting point for this chapter.

\subsection{Algorithm description}

Cycle cancelling is inspired by the negative cycle optimality conditions, given in \cref{thm:optimality-neg-cycle}.

\begin{algorithm}
    \caption{Cycle cancelling algorithm}
    \label{algo:cycle-cancelling}
    \begin{algorithmic}[1]
        \State $\mathbf{x}\gets $ result of maximum-flow algorithm \Comment{establishes x feasible}
        \While{$G_\mathbf{x}$ contains a negative cost cycle}
        \State identify negative cost cycle $W$ \Comment{e.g. using Bellman-Ford}
        \Let{$\delta$}{$\min_{(i,j) \in W} r_{ij}$}
        \State augment $\delta$ units of flow along cycle $W$
        \EndWhile{}
    \end{algorithmic}
\end{algorithm}

The algorithm is initialised with a feasible flow $\mathbf{x}$, which can be found by any maximum-flow algorithm, such as Ford-Fulkerson~\cite{FordFulkerson:1956}. The feasibility of the solution $\mathbf{x}$ is maintained throughout the algorithm, and its cost is reduced. During each iteration of the algorithm, a directed cycle of negative cost is identified in the residual network $G_\mathbf{x}$ and \emph{cancelled} by pushing the maximum possible amount of flow along it. This will cause the cycle to 'drop out' of the residual network: one or more arcs along the cycle will be saturated, and so no longer present in the residual network. The algorithm terminates when no negative cost directed cycles remain.

Note this generic version of the algorithm does not specify \emph{how} negative cycles are to be identified. I opted to use the well-known Bellman-Ford~\cite[p.~651]{CLRS:2009} algorithm for this purpose. Other, more efficient algorithms could be used. However, cycle cancelling was never going to be fast enough for my needs\footnotemark. I chose to implement it so as to have a known-working algorithm early in the project, simplifying subsequent testing. For this purpose, cycle cancelling using the Bellman-Ford algorithm is ideal.

\footnotetext{The asymptotic complexity is considerably worse than that of competing algorithms, and computational benchmarks~\cite{KiralyKovacs:2012} were also unfavourable.}

\subsection{Analysis}

\subsubsection{Correctness}

We will show that, if the algorithm terminates, it produces the correct result. \\

\begin{thm} \label{thm:cycle-cancelling-invariant}
Immediately before each iteration of the loop, $\mathbf{x}$ is a feasible solution.
\end{thm} 
\begin{proof}
For the base case, $\mathbf{x}$ must be initially feasible, by correctness of the maximum-flow algorithm used.

For the inductive case, suppose $\mathbf{x}$ is feasible immediately prior to an iteration of the loop body. The loop body pushes flow along a cycle. This maintains feasibility: the excess at the nodes along the cycle must remain zero, since any increase in the flow leaving the node is counterbalanced by an equal increase in the flow entering the node.
\end{proof}

\begin{cor}
Upon termination, $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{cor}
\begin{proof}
By \cref{thm:cycle-cancelling-invariant}, $\mathbf{x}$ is a feasible solution upon termination. The algorithm only terminates when no negative-cost directed cycles exist. It follows by \cref{thm:optimality-neg-cycle} that $\mathbf{x}$ is optimal.
\end{proof}

\subsubsection{Termination and asymptotic complexity}

We will now show that the algorithm always terminates, and provide a bound on its running time.\\

\begin{thm} \label{thm:cycle-cancelling-termination}
The algorithm terminates within $O(mCU)$ iterations.
\end{thm}
\begin{proof}
Clearly $mCU$ is an upper bound on the cost of the initial flow. Each iteration of the algorithm identifies a negative cost cycle $w$. The cost of sending one unit of flow along this cycle is $c = \sum_{(i,j) \in w} c_{ij}$, which is strictly negative by definition. We send $\delta = \min_{(i,j) \in w} r_{ij}$ units of flow along the cycle. $\delta$ is strictly positive, otherwise the cycle would not exist in the \emph{residual} network. 

The objective function value changes by $c\delta$. By \cref{assumption:integrality}, $c$ and $\delta$ must both be integral. So as $c\delta < 0$, we have $c\delta \leq 1$.

Hence the cost decreases by at least one each iteration, and so the number of iterations is bounded by $O(mCU)$.
\end{proof}

\begin{cor} \label{corollary:cycle-cancelling-complexity}
The asymptotic complexity is $O(nm^2CU)$.
\end{cor}
\begin{proof}
Note that Bellman-Ford runs in $O(nm)$ time. Augmenting flow along the cycle is of cost linear in the length of the cycle, and so is certainly $O(m)$. Thus each iteration runs in $O(nm)$. By \cref{thm:cycle-cancelling-termination}, it follows the complexity of the algorithm is $O(nm^2CU)$.
\end{proof}

\section{Successive shortest path algorithm} \label{sec:impl-ssp}

% Note this algorithm offers polynomial performance for Quincy-like graphs because it is independent of cost, whereas cycle cancelling is not
This algorithm runs in weakly polynomial time. We will show, however, that it has a strongly polynomial time bound of $O(n^2)$ for the class of flow networks produced by Quincy-like systems. It lends itself readily to an incremental implementation (see \S\ref{sec:impl-incremental}), but is inappropriate for an approximate solver.

\subsection{Algorithm description}

\begin{algorithm}
    \caption{Successive shortest path algorithm}
    \label{algo:successive-shortest-path}
    \begin{algorithmic}[1]
        \State $\mathbf{x} \gets \mathbf{0}$ and $\boldsymbol{\pi} \gets \mathbf{0}$
        \While{mass balance constraints not satisfied}
          \State choose node $s$ with $e(s) > 0$ and node $t$ with $e(t) < 0$\footnotemark
          \State solve SSSP problem from $s$ to all other nodes, in the residual network $G_{\mathbf{x}}$ with respect to the reduced costs $c^{\boldsymbol{\pi}}_{ij}$
          \State let $\mathbf{d}$ denote the vector of shortest path distances, s.t. $d_i$ is the shortest path from $s$ to $i\in V$
          \Let{$\boldsymbol{\pi}$}{$\boldsymbol{\pi} - \mathbf{d}$}
          \State let $P$ denote a shortest path from $s$ to $t$
          \Let{$\delta$}{$\min\left(e(s), -e(t), \min\left\{ r_{ij} \::\: (i,j) \in P\right\}\right)$}
          \State augment $\delta$ units of flow along path $P$
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\footnotetext{Note whilst mass balance constraints are unsatisfied there must exist both a source node $s$ and sink node $t$. This is since the total sum of excesses must equal the total sum of deficits, otherwise $\sum_v b(v) \neq 0$ and the problem is infeasible.}

The successive shortest path algorithm maintains a pseudoflow (see \S\ref{sec:prep-flow-pseudo}) $\mathbf{x}$ and potentials $\boldsymbol{\pi}$ which satisfies reduced cost optimality (see \cref{thm:optimality-reduced-cost}), and attempts to attain feasibility. This is in contrast to the cycle cancelling algorithm, which maintains the feasibility strives to achieve optimality.

Each iteration of the algorithm identifies a source node $s$ and sink node $t$. The single-source shortest-path (SSSP) problem~\cite[ch.~24]{CLRS:2009} is then solved from $s$. The pseudoflow $\mathbf{x}$ is updated by augmenting along a shortest path to $t$, $P$. We are limited by the minimum residual capacity of arcs along the path $P$. Moreover, we opt to maintain a non-negative supply at $s$ and demand at $t$. This restriction ensures the magnitude of the excess at each node is monotonically decreasing. We also update the potentials $\boldsymbol{\pi}$, to maintain reduced cost optimality. The algorithm terminates when no source nodes or sink nodes exist.

\subsection{Analysis} \label{sec:impl-ssp-analysis}

\subsubsection{Correctness}

First, we will show that the algorithm maintains reduced cost optimality, for which we will need a number of lemmas. We will then conclude, using the invariant and the terminating condition, that the algorithm will return a solution to the minimum-cost flow problem.\\

\begin{lemma} \label{lemma:ssp-reduced-costs}
Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions \cref{eq:optimality-reduced-cost} with respect to potentials $\boldsymbol{\pi}$. Let $\mathbf{d}$ represent the shortest path distances from a node $s \in V$ to all other nodes in the residual network $G_{\mathbf{x}}$ with respect to the reduced costs $c^{\boldsymbol{\pi}}_{ij}$. Then:
    
\begin{enumerate}[label=(\alph*)]
  \item $\mathbf{x}$ also satisfies reduced cost optimality conditions with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
  \item The reduced costs, $c^{\boldsymbol{\pi}'}_{ij}$, with respect to this new potential $\boldsymbol{\pi}'$, are zero for all arcs $(i,j)$ in the shortest-path tree rooted at $s \in V$.
\end{enumerate}
\end{lemma}
\begin{proof}
See~\cite[p.~320]{Ahuja:1993}.
\end{proof}

\begin{cor} \label{cor:ssp-reduced-costs}
Let a pseudoflow $\mathbf{x}$ satisfy the reduced cost optimality conditions, with respect to some potentials $\boldsymbol{\pi}$. Let $\mathbf{x}'$ denote the pseudoflow obtained from $x$ by sending flow along a shortest path from node $s$ to some other node $k \in V$. Then $x'$ also satisfies the reduced cost optimality conditions, with respect to potentials $\boldsymbol{\pi}' = \boldsymbol{\pi} - \mathbf{d}$.
\end{cor}
\begin{proof}
By \cref{lemma:ssp-reduced-costs}(a), $(\mathbf{x},\boldsymbol{\pi'}$ satisfies the reduced cost optimality conditions.

Pushing flow along an arc $(i,j) \in G_{\mathbf{x}}$ might add its reversal $(j,i)$ in the residual network. Let $P$ be a shortest path from $s$ to $k$. By \cref{lemma:ssp-reduced-costs}(b), it follows that any arc $(i,j) \in P$ has $c^{\boldsymbol{\pi}'}_{ij} = 0$. So $c^{\boldsymbol{\pi}'}_{ji} = 0$. Thus any arcs that are added to the residual network by augmenting flow along $P$ have a zero reduced cost, and so still satisfy the reduced cost optimality conditions \cref{eq:optimality-reduced-cost}.
\end{proof}

\begin{thm} \label{thm:ssp-invariant}
Immediately before each iteration of the loop, $(\mathbf{x},\boldsymbol{\pi})$ satisfies reduced cost optimality.
\end{thm}
\begin{proof} (Induction)
    
For the base case, note $(\mathbf{0},\mathbf{0})$ satisfy reduced cost optimality. We have $G_{\boldsymbol{0}} = G$, i.e. the residual and original network are the same. Moreover, all arc costs $c_{ij}$ are non-negative (by \cref{assumption:non-negative-arc-costs}) and so the reduced costs $c^{\boldsymbol{0}}_{ij}$ are also non-negative. Thus \cref{eq:optimality-reduced-cost} holds.

Now, suppose the inductive hypothesis holds: reduced cost optimality holds immediately prior to execution of the loop body. The loop body computes a shortest path distances $s$ from a node $d$, updates $\boldsymbol{\pi}$ to become $\boldsymbol{\pi'}$ as defined in \cref{lemma:ssp-reduced-costs} and pushes flow along a shortest path from $s$ to another node, yielding a new flow $\mathbf{x}$ of the same form as $\mathbf{x'}$ in \cref{cor:ssp-reduced-costs}. It follows by \cref{cor:ssp-reduced-costs} that $(\mathbf{x},\boldsymbol{\pi})$ satisfy reduced cost optimality at the end of the loop body. Hence, the inductive hypothesis continues to hold.
\end{proof}

\begin{thm} \label{thm:ssp-correctness}
Upon termination, $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{thm}
\begin{proof}
The algorithm terminates when the mass balance constraints \cref{eq:mass-balance} are satisfied. At this point, the solution $\mathbf{x}$ is feasible (see \S\ref{sec:prep-flow-pseudo}). 

By \cref{thm:ssp-invariant}, we know the algorithm maintains the invariant that $\mathbf{x}$ satisfies reduced cost optimality. 

It follows that $\mathbf{x}$ is both a feasible flow and optimal upon termination, so $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{proof}

\subsubsection{Termination and asymptotic complexity}

\begin{thm} \label{thm:ssp-complexity}
Let $S(n,m,C)$ denote the time complexity of solving a single-source shortest path problem with non-negative arc costs, bounded above by $C$, over a network with $n$ nodes and $m$ arcs. Then the time complexity of successive shortest path is $O(nUS(n,m,C))$.
\end{thm}
\begin{proof}
Each iteration of the loop body (lines 3-9) decreases the excess of some node $s$ by $k$ and the deficit of some node $t$ by $k$, whilst leaving the excess/deficit of other nodes unchanged. Consequently, the total excess and total deficit are both decreased by $k$. By \cref{assumption:integrality}, $k \geq 1$. So the number of iterations is bounded by the initial total excess. But this is bounded by $nU/2 = O(nU)$. So there are $O(nU)$ iterations.

Within each iteration, the algorithm solves a single-source shortest-path problem on line 4. Since reduced cost optimality is maintained throughout the algorithm, the arc costs in the shortest-path problem are non-negative\footnotemark. Thus the cost of solving this problem is $S(n,m,C)$.
\footnotetext{Algorithms such as Djikstra which assume non-negative arc lengths are asymptotically faster than more general algorithms such as Bellman-Ford.}

The cost of line 3 is $O(n)$, line 6 $O(n)$ and line 8-9 $O(n)$ as the length of $P$ is bounded by $n-1$ (shortest path is acyclic). Certainly $S(n,m,C) = \Omega(n)$, so the cost of each iteration is $O(S(n,m,C))$.

It follows that the overall time complexity of the algorithm is $O(nUS(n,m,C))$.
\end{proof}

% TBC: Intro promises we'll derive an asymptotic complexity bound of n^2 for Quincy-style networks. I think this is wrong, and in any case we don't show it.
% TBC: Related to this, explain why capacity scaling is unimportant for our application.

\subsection{Optimisations}

% TBC: ap_big_vs_small_heap: keep all vertices in the priority queue, or just add them as needed? Performance will depend on how many vertices get explored before Djikstra quits.
% Unclear whether best to put it here, or in the evaluation section, or both?

\subsubsection{Choice of shortest-path algorithm}
The fastest known single-source shortest-path algorithms are all variants of Djikstra's algorithm~\cite[ch.~4]{Ahuja:1993}. They differ in the heap data structure used to provide the priority queue needed by Djikstra. The asymptotically fastest are Fibonacci heaps, with $S(m,n,C) = O(m + n\lg n)$. By contrast, the widely used binary heap data structure gives $S(m,n,C) = O(m\lg n)$. 

Fibonacci heap's have considerable implementation complexity, however: whilst asymptotically faster, the constant factor hidden by the asymptotic notation is much greater. Computational benchmarks have found them to be slower than binary heaps in practice, for all but the largest of graphs~\cite[p.~15]{KiralyKovacs:2012}. In any case, for the class of networks produced by Quincy-like systems, $m = O(n)$, and so the two are asymptotically equivalent. Given this, I opted for a binary heap implementation. The resulting the asymptotic complexity of my successive shortest path implementation is $O(nmU \lg n)$.

\subsubsection{Terminating Djikstra's algorithm early}

% TBC: Test this using benchmark suite and include in evaluation section? Doesn't seem like it'd be too difficult.
It is possible to modify the successive shortest path algorithm to terminate Djikstra as soon as it permanently labels a deficit node $l$. Whilst not affecting asymptotic complexity, this may considerably improve performance in practice.\\

\begin{lemma}
Recall \cref{lemma:ssp-reduced-costs}. Let us redefine:
{\normalfont
\[\boldsymbol{\pi}'_{i}=\begin{cases}
\boldsymbol{\pi}_{i}-d_{i} & \textrm{if $i$ permanently labelled}\\
\boldsymbol{\pi}_{i}-d_{l} & \textrm{otherwise}
\end{cases}\]}
The original result for (a) still holds. The result for (b) holds along the shortest path from $s$ to $l$\footnotemark.
\footnotetext{Note that this is all that is needed for the correctness of the algorithm, as this is the only path along which we augment flow.}
\end{lemma}
\begin{proof}
The original proof for the lemma (given in~\cite[p.~320]{Ahuja:1993}) uses the triangle inequality:
\begin{equation} \label{eq:ssp-reduced-costs-triangle}
d_j \leq d_i + c^{\boldsymbol{\pi}}_{ij}\:\forall(i,j)\in E_{\mathbf{x}}
\end{equation}
Terminating Djikstra's algorithm early, we only know the shortest-path distance to nodes which have been permanently labelled. But for any node $i$ not permanently labelled and node $l$ permanently labelled, the shortest path distances satisfy:
\begin{equation} \label{eq:ssp-djikstra-labelled}
d_i \geq d_l
\end{equation}
This is because were $d_i < d_l$ then $i$ must have been permanently labelled before $l$, which is a contradiction.

Now, let us define: 
\begin{equation} \label{eq:ssp-djikstra-distances}
d'_{i}=\begin{cases}
d_{i} & \text{if $i$ permanently labelled}\\
d_{l} & \text{otherwise}
\end{cases}
\end{equation}

Given \cref{eq:ssp-djikstra-labelled}, we know $\mathbf{d}'$ satisfies \cref{eq:ssp-reduced-costs-triangle}. The original proof for (a) thus still holds, as it makes no further assumptions on $\mathbf{d}'$.

As for (b), every node $i$ along the shortest path from $s$ to $l$ has been permanently labelled, and so $d'_i = d_i$. Hence the original proof still holds along this path.
\end{proof}

Any constant shift in the potential for every node will leave reduced costs unchanged. So we may equivalently redefine:

\[\boldsymbol{\pi}'_{i}=\begin{cases}
\boldsymbol{\pi}_{i}-d_i+d_l & \text{if $i$ permanently labelled}\\
\boldsymbol{\pi}_{i} & \text{otherwise}
\end{cases}\]

This is computationally more efficient, as we will not have to update the potential at any node which has not been permanently labelled (typically the majority of nodes).

\section{Relaxation algorithm} \label{sec:impl-relax}

Like the successive shortest path algorithm, the relaxation algorithm works by augmenting along shortest paths in the residual network from sources to sinks. Based upon the method of Lagrangian relaxation from mathematical optimization~\cite[ch.~16]{Ahuja:1993}\cite{Fisher:1981}, unlike the successive shortest path algorithm it updates node potentials using intermediate information as it constructs the shortest-path tree. For this reason, it performs much better empirically than the successive shortest path algorithm, and indeed is one of the fastest algorithms for some classes of flow networks~\cite{KiralyKovacs:2012}. However, its worst-case time complexity is exponential, giving it the slowest theoretical performance of the algorithms we consider.\footnotemark Like successive shortest path, it is well suited to being used in an incremental solver, but is inappropriate for an approximate solver.

\footnotetext{The generic version of cycle cancelling is also exponential time, however there are variants which are strongly polynomial. By contrast, there are no variants of relaxation achieving polynomial time, although in practice exponential runtime is not observed on realistic flow networks.}

\subsection{The relaxed integer programming problem}
%TODO: Attribution for Ahuja, et al?
Before we can describe how the relaxation algorithm works, we must understand the problem it seeks to optimise. Previously we have discussed the primal (see \S\ref{sec:prep-flow-mcf}) and dual (see \S\ref{sec:prep-flow-rc-and-dual}) version of the minimum-cost flow problem. We apply Lagrangian relaxation to the primal version to yield a relaxed problem, which is in some sense a hybrid of the primal and dual statement. We denote this formulation of the optimisation problem as $\mathrm{LR}(\boldsymbol{\pi})$.

\begin{equation} \label{eq:relax-obj-fun-excess}
\mathrm{maximise}\: w\left(\boldsymbol{\pi}\right)=\min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\boldsymbol{\pi}_{i}e_{i}\right]
\end{equation}
where $\mathbf{x}$ is subject to capacity constraints:
\begin{equation} \label{eq:relax-capacity-constraints}
0\leq x_{ij}\leq u_{ij}\:\forall\left(i,j\right)\in E
\end{equation}

\begin{lemma}
An equivalent definition for $w(\boldsymbol{\pi})$ is:
\begin{equation} \label{eq:relax-obj-fun-balance}
w(\boldsymbol{\pi})=\min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}^{\boldsymbol{\pi}}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}\right]
\end{equation}
\end{lemma}
\begin{proof}
Recall the excess at node $i$ is defined as:
\[e_{i}=b_{i}+\sum_{(j,i)\in E}x_{ji}-\sum_{(i,j)\in E}x_{ij}\]
So:
\begin{align*}
w\left(\pi\right)= & \min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\pi_{i}\left(b_{i}+\sum_{(j,i)\in E}x_{ji}-\sum_{(i,j)\in E}x_{ij}\right)\right]\:\mbox{substituting for }e_{i}\\
= & \min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}+\sum_{i\in V}\pi_{i}\sum_{(j,i)\in E}x_{ji}-\sum_{i\in V}\pi_{i}\sum_{(i,j)\in E}x_{ij}\right]\:\mbox{expanding}\\
= & \min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}+\sum_{(j,i)\in E}\pi_{i}x_{ji}-\sum_{(i,j)\in E}\pi_{i}x_{ij}\right]\:\mbox{double to single sum}\\
= & \min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}+\sum_{(i,j)\in E}\pi_{j}x_{ij}-\sum_{(i,j)\in E}\pi_{i}x_{ij}\right]\:\mbox{permuting \ensuremath{i} and \ensuremath{j} in 3rd sum}\\
= & \min_{x}\left[\sum_{\left(i,j\right)\in E}\left(c_{ij}-\pi_{i}+\pi_{j}\right)x_{ij}+\sum_{i\in V}\pi_{i}b_{i}\right]\:\mbox{factoring}\\
= & \min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}^{\boldsymbol{\pi}}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}\right]\:\mbox{substituting reduced cost}
\end{align*}
\end{proof}

We may use either form for $w(\boldsymbol{\pi})$ in the remainder of this section, depending on which is most convenient. Note \cref{eq:relax-obj-fun-excess} is expressed in terms of the arc costs and excesses ($\mathbf{x}$-dependent), whereas \cref{eq:relax-obj-fun-balance} is expressed in terms of the reduced costs ($\boldsymbol{\pi}$-dependent) and balances.

\begin{lemma} \label{lemma:relax-rc-lr-equivalence}
Let $\mathbf{x}$ be a pseudoflow and $\boldsymbol{\pi}$ node potentials. Then $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfies reduced cost optimality conditions if and only if $\mathbf{x}$ is an optimal solution to $\mathrm{LR}(\boldsymbol{\pi})$.
\end{lemma}
\begin{proof}
To simplify the proof, we will use the complementary slackness conditions rather than the reduced cost optimality conditions. Recall the two conditions are equivalent, see \S\ref{prep:flow-optimality}.

Now, consider the form of $w(\boldsymbol{\pi})$ stated in terms of reduced cost and balances, given in \cref{eq:relax-obj-fun-balance}. Thus $\mathbf{x}$ is an optimal solution to $\mathrm{LR}(\boldsymbol{\pi})$ if and only if it minimises:
\[\sum_{\left(i,j\right)\in E}c_{ij}^{\boldsymbol{\pi}}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}\]
subject to the capacity constraints given in \cref{eq:relax-capacity-constraints}.

The second sum, $\sum_{i \in V} \pi_i b_i$, is constant in $\mathbf{x}$. Thus $\mathbf{x}$ is an optimal solution to $\mathrm{LR}(\boldsymbol{\pi})$ if and only if it minimises:
\[\sum_{\left(i,j\right)\in E}c_{ij}^{\boldsymbol{\pi}}x_{ij}\]

Note the coefficients $c_{ij}^{\boldsymbol{\pi}}$ are constant in $\mathbf{x}$. Furthermore, note each term is independent of each other: by varying $x_{ij}$, only the term $c_{ij}^{\boldsymbol{\pi}}x_{ij}$ is affected\footnotemark. Thus the sum is minimised when each of its summands is minimised.
\footnotetext{Contrast this to if we were varying a node quantity, such as $\pi_i$, which would affect many arcs.}

When $c_{ij}^{\boldsymbol{\pi}}>0$, the term $c_{ij}^{\boldsymbol{\pi}}x_{ij}$ is minimised by setting $x_{ij}$ to the smallest value permitted by \cref{eq:relax-capacity-constraints}, $0$. When $c_{ij}^{\boldsymbol{\pi}}<0$, set $x_{ij}$ to the largest possible value, $u_{ij}$. When $c_{ij}^{\boldsymbol{\pi}}=0$, the choice of $x_{ij}=0$. This is the same as the complementary slackness conditions given in \cref{thm:optimality-complementary-slackness}. 

Thus complementary slackness optimality is equivalent to $\mathbf{x}$ being an optimal solution to $\mathrm{LR}(\boldsymbol{\pi})$. By equivalence of reduced cost optimality and complementary slackness optimality, the result follows.
\end{proof}

\begin{defn}
Let $z^*$ denote the optimal objective function value of the minimum-cost flow problem, that is the cost of an optimal flow.
\end{defn}
~ % force line break
\begin{lemma} \label{lemma:relax-dual-optimality}
~ % force line break
\begin{enumerate}[label=(\alph*)]
    \item For any potential vector $\boldsymbol{\pi}$, $w(\boldsymbol{\pi}) \leq z^*$.
    \item There exists node potentials $\boldsymbol{\pi}^*$ for which $w(\boldsymbol{\pi}^*) = z^*$.
\end{enumerate}
\end{lemma}
\begin{proof}
For the first part, let $\mathbf{x}^*$ be a feasible flow with objective function value $s(\mathbf{x}^*) = z^*$, that is $\mathbf{x}^*$ an optimal solution to the minimum-cost flow problem given in \cref{eq:mcf-primal-problem}.

Recall the form of $w(\boldsymbol{\pi})$ with original arc costs and excesses, given in \cref{eq:relax-obj-fun-excess}. Thus we have:
\[w(\boldsymbol{\pi})\leq\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}^{*}+\sum_{i\in V}\pi_{i}e_{i}\]
by dropping the $\min_x$ and replacing $=$ with $\leq$.

Note the first sum is equal to $s(\mathbf{x}^*) = z^*$. Since $\mathbf{x}^*$ is a feasible flow, the mass balance constraints are satisfied, so:
\[e_i = 0\:\forall i \in V\]
and thus the second sum satisfies:
\[\sum_{i \in V} \pi_i e_i = 0\]

It follows that:
\[w(\boldsymbol{\pi}) \leq z^* + 0 = z^*\]

To prove the second part, let $\boldsymbol{\pi}^*$ be node potentials such that $\left(\mathbf{x}^*,\boldsymbol{\pi}^*\right)$ satisfy the reduced cost optimality conditions given in \cref{eq:optimality-reduced-cost}\footnotemark. By \cref{lemma:relax-rc-lr-equivalence}, it follows that $\mathbf{x}^*$ is an optimal solution to $\mathrm{LR}(\boldsymbol{\pi}^*)$. Thus $w(\boldsymbol{\pi}^*) = z^*$.
\footnotetext{Note such a choice of $\boldsymbol{\pi}^*$ is guaranteed to exist since $\mathbf{x}^*$ is an optimal solution to the minimum-cost flow problem.}
\end{proof}

% Highlight similarity to weak dulity theorem?

\subsection{Algorithm description}

The algorithm maintains a pseudoflow $\mathbf{x}$ and node potentials $\boldsymbol{\pi}$ such that $\mathbf{x}$ is an optimal solution to $\mathrm{LR}(\boldsymbol{\pi})$. Equivalently, by \cref{lemma:relax-rc-lr-equivalence}, $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality.

So long as the pseudoflow $\mathbf{x}$ is not feasible, the algorithm selects a source node $s$. It then builds a tree rooted at $s$. The algorithm can perform one of two operations, described below. An operation is run as soon as its precondition is satisfied. After it is executed, the tree is destroyed and the process repeats.

The first operation is to update the potentials, increasing the value of the objective function whilst maintaining optimality. That, is $\boldsymbol{\pi}$ is updated to $\boldsymbol{\pi}'$ such that $w\left(\boldsymbol{\pi}'\right) > w\left(\boldsymbol{\pi}\right)$ and $\mathbf{x}$ is updated to $\mathbf{x}'$ such that $\mathbf{x}'$ is an optimal solution to $\mathrm{LR}(\boldsymbol{\pi}')$.

The second possible operation is to augment the flow $\mathbf{x}$, whilst leaving potentials $\boldsymbol{\pi}$ unchanged. The new flow $\mathbf{x}'$ remains an optimal solution of $\mathrm{LR}(\boldsymbol{\pi})$, and reduces the excess of one node without increasing the excess at any source nodes nor the demand at any sink nodes.

When both operations are eligible to be executed, we favour the former. The primary goal of the algorithm is thus to increase the value of the objective function, and the secondary goal is to increase feasibility (whilst leaving the objective function unchanged).

Before we can specify the preconditions for these operations, we must define two functions.

\begin{defn}[Tree excess] \label{defn:relax-tree-excess}
Let $S$ denote the set of nodes spanned by the tree. Let:
\begin{equation} \label{eq:relax-tree-excess}
e(S) = \sum_{i \in S} e_i
\end{equation}
\end{defn}

\begin{defn}[Cuts] \label{defn:relax-cuts}
A \emph{cut} of a graph $G = (V,E)$ is a partition of $V$ into two sets: $S$ and $\overline{S} = V \setminus S$. We denote the cut by $\left[S,\overline{S}\right]$. Let $\left(S,\overline{S}\right)$ and $\left(\overline{S},S\right)$ denote the set of forward and backward arcs crossing the cut, respectively. Formally:
\[\left(S,\overline{S}\right) = \left\{(i,j) \in E\::\: i \in S \land j \in \overline{S} \right\}\]
\[\left(\overline{S},S\right) = \left\{(i,j) \in E\::\: i \in \overline{S} \land j \in S \right\}\]
\end{defn}

\begin{defn} \label{defn:relax-tree-residual}
Let:
\begin{equation} \label{eq:relax-tree-residual}
r(\boldsymbol{\pi},S) = \sum_{(i,j) \in \left(S,\overline{S}\right) \land c_{ij}^{\boldsymbol{\pi}}} r_{ij}
\end{equation}
\end{defn}

\begin{remark}
The algorithm only adds arcs with zero reduced cost to the tree. Consequently, $r(\boldsymbol{\pi},S)$ represents the total residual capacity of arcs that could be added to the tree.
\end{remark}

\begin{algorithm}
    \caption{Relaxation algorithm}
    \label{algo:relaxation}
    \begin{algorithmic}[1]
        \State $\mathbf{x} \gets \mathbf{0}$ and $\boldsymbol{\pi} \gets \mathbf{0}$
        \While{network contains a source node $s$}
        \Let{$S$}{$\left\{s\right\}$}
        \State initialise $e(S)$ and $r(\boldsymbol{\pi},S)$
        \If{$e(S) > r(\boldsymbol{\pi},S)$}
        \Call{UpdatePotentials}{}
        \EndIf
        \While{$e(S) \leq r(\boldsymbol{\pi},S)$}
        \State select arc $(i,j) \in \left(S,\overline{S}\right)$ in the residual network with $c_{ij}^{\boldsymbol{\pi}}=0$
        \If{$e_j \geq 0$}
        \Let{$\mathrm{Pred}_j$}{$i$}
        \Let{$S$}{$S \cup \left\{j\right\}$}
        \State update $e(S)$ and $r(\boldsymbol{\pi},S)$
        \Else
        \State \Call{AugmentFlow}{j}
        \Break
        \EndIf
        \EndWhile
        \If{$e(S) > r(\boldsymbol{\pi},S)$}
        \Call{UpdatePotentials}{}
        \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Relaxation algorithm: potential update procedure}
    \label{algo:relaxation-update-potentials}
    \begin{algorithmic}[1]
        \Require $e(S) > r(\boldsymbol{\pi},S)$
        \Statex
        \Function{UpdatePotentials}{}
        \For{every arc $(i,j) \in \left(S,\overline{S}\right)$ in the residual network with $c_{ij}^{\boldsymbol{\pi}}=0$}
        \State saturate arc $(i,j)$ by sending $r_{ij}$ units of flow
        \EndFor
        \State compute $\alpha \gets \min\left\{c_{ij}^{\boldsymbol{\pi}}\::\:(i,j)\in\left(S,\overline{S}\right)\:\mbox{and}\:r_{ij}>0\right\}$
        \For{every node $i\in S$}
        \Let{$\pi_i$}{$\pi_i + \alpha$}
        \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Relaxation algorithm: flow augmentation procedure}
    \label{algo:relaxation-augment-flow}
    \begin{algorithmic}[1]
        \Require $e(S) \leq r(\boldsymbol{\pi},S)$ and $e(t) < 0$
        \Statex
        \Function{AugmentFlow}{t}
        \State chase predecessors in $\mathrm{Pred}$ starting from $t$ to find a shortest path $P$ from $s$ to $t$
        \Let{$\delta$}{$\min\left(e(s), -e(t), \min\left\{ r_{ij} \::\: (i,j) \in P\right\}\right)$}
        \State augment $\delta$ units of flow along path $P$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\Cref{algo:relaxation,algo:relaxation-update-potentials,algo:relaxation-augment-flow} formally describe the solution method. We maintain a set $S$ of nodes spanned by the shortest path tree. The arcs in the tree are specified by the predecessor array, $\mathrm{Pred}$. Whilst we could recompute $e(S)$ and $r(\boldsymbol{\pi},S)$ when needed, it is more efficient for us to maintain them as variables, updating them when we add a node to the tree.

The outer loop on lines 2-17 of \ref{algo:relaxation} selects a source node $s$. The inner loop on lines 6-15 adds nodes and arcs to the tree. The tree consists solely of non-deficit nodes, and arcs with zero reduced cost. It is thus a shortest path tree in the residual network: the distance between any nodes in the tree is zero\footnotemark.
\footnotetext{Note there cannot be any path in the residual network with negative distance, by assumption of reduced cost optimality.}

If a deficit node $t$ is encountered, we augment the flow along the shortest path discovered from $s$ to $t$. We may terminate before this, however, if at any point $e(S) > r(\boldsymbol{\pi},S)$; in this case, the potentials are updated instead.

\subsection{Analysis} \label{sec:impl-relax-analysis}

\subsubsection{Correctness}

First, we will show that \textproc{UpdatePotentials} and \textproc{AugmentFlow} preserve reduced cost optimality. Using these lemmas, we can then show that reduced cost optimality is maintained as an invariant throughout the algorithm, and thus if the algorithm terminates then $\mathbf{x}$ is a solution to the minimum-cost flow problem. We will also prove properties of \textproc{UpdatePotentials} and \textproc{AugmentFlow} that will be useful for complexity analysis in the next section.\\

\begin{lemma}[Correctness of \textproc{UpdatePotentials}] \label{lemma:relax-correctness-updatepotentials}
Let $e(S) > r(\boldsymbol{\pi},S)$. Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality conditions. Then, after executing $\textproc{UpdatePotentials}$, we have a new pseudoflow and node potentials $\left(\mathbf{x},\boldsymbol{\pi}\right)$ which continue to satisfy reduced cost optimality conditions, and $w(\boldsymbol{\pi}') > w(\boldsymbol{\pi})$.
\end{lemma}
\begin{proof}
Lines 2-3 saturate all zero reduced cost arcs, so that they drop out of the residual network. This maintains reduced cost optimality, as the flow on arcs with zero reduced cost is arbitrary. Moreover, it leaves $w(\boldsymbol{\pi})$ unchanged. Recall the formulation of the objective function in terms of reduced costs and balances, given in \cref{eq:relax-obj-fun-balance}:
\[w(\boldsymbol{\pi})=\min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}^{\boldsymbol{\pi}}x_{ij}+\sum_{i\in V}\pi_{i}b_{i}\right]\]
The second sum is unchanged, as potentials are unchanged. The first sum is also unchanged, as $x'_{ij}$ differs from $x_{ij}$ only when the reduced costs $c_{ij}^{\boldsymbol{\pi}}=0$.

Note that there is now $r(\boldsymbol{\pi},S)$ more flow leaving nodes in $S$, so the tree excess is now:
\[e'(S) = e(S) - r(\boldsymbol{\pi},S)\]
By the precondition, $e'(S)$ is (strictly) positive.

After lines 2-3, all arcs in the residual network crossing the cut have positive reduced cost\footnotemark. We let $\alpha$ be the minimal such remaining reduced cost; note $\alpha$ is (strictly) positive.
\footnotetext{Since none had negative reduced cost, by assumption of reduced cost optimality on entering the procedure.}

Note by \cref{assumption:integrality}, we have $\alpha \geq 1 $ and $e'(S) \geq 1$.

We now obtain $\boldsymbol{\pi}'$ from $\boldsymbol{\pi}$ by increasing the potential by $\alpha$ for every node in the tree $S$. Recall the formulation of the objective function in terms of costs and excesses, given in \cref{eq:relax-obj-fun-excess}:
\[w\left(\boldsymbol{\pi}\right)=\min_{x}\left[\sum_{\left(i,j\right)\in E}c_{ij}x_{ij}+\sum_{i\in V}\boldsymbol{\pi}_{i}e_{i}\right]\]
The first sum is unchanged: modifying the potentials $\boldsymbol{\pi}$ does not change the original arc cost $c_ij$ or flow $x_{ij}$. For the second sum, we have:
\begin{align*}
\sum_{i\in V}\pi'_{i}e'_{i}= & \sum_{i\in S}\left(\pi_{i}+\alpha\right)e'_{i}+\sum_{i\in\overline{S}}\pi_{i}e'_{i}\\
= &\:\alpha\sum_{i\in S}e'_{i}+\sum_{i\in V}\pi_{i}e'_{i}\\
= &\:\alpha e'(S)+\sum_{i\in V}\pi_{i}e'_{i}
\end{align*}
Since $w\left(\boldsymbol{\pi}\right)$ unchanged after updating $\mathbf{x}$, it follows:
\[w\left(\boldsymbol{\pi}'\right)=w(\boldsymbol{\pi})+\alpha e'(S)\]
We have already shown $\alpha, e'(S) \geq 1$. It follows $\alpha e'(S) \geq 1$, so $w\left(\boldsymbol{\pi}'\right) > w\left(\boldsymbol{\pi}\right)$.

It remains to show that updating potentials maintains reduced cost optimality. Note that increasing the potentials of nodes in $S$ by $\alpha$ decreases the reduced cost of arcs in $\left(S,\overline{S}\right)$ by $\alpha$, increases the reduced cost of arcs in $\left(\overline{S},S\right)$ by $\alpha$ and leaves the reduced cost of other arcs unchanged.

Prior to updating potentials, all arcs in the residual network had (strictly) positive reduced costs. Consequently, increasing the reduced cost cannot violate reduced cost optimality\footnotemark.
\footnotetext{Note increasing the reduced cost from zero to a positive number could violate optimality: an arc with zero reduced cost is permitted to have a positive flow, whereas an arc with positive reduced cost is not.} Decreasing the reduced cost might, however. But before updating the potentials, $c_{ij}^{\boldsymbol{\pi}} \geq \alpha$ for all $(i,j)\in\left(S,\overline{S}\right)\:\mbox{and}\:r_{ij}>0$, hence after the potential update $c_{ij}^{\boldsymbol{\pi}'}\geq 0$ for these arcs.
\end{proof}

\begin{lemma}[Correctness of \textproc{AugmentFlow}] \label{lemma:relax-correctness-augmentflow}
Let $e(S) \leq r(\boldsymbol{\pi},S)$, and $e(t) < 0$. Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality conditions. Then, after executing \textproc{AugmentFlow}, we have a new pseudoflow $\mathbf{x}'$ such that $\left(\mathbf{x}',\boldsymbol{\pi}\right)$. Moreover, under $\mathbf{x}'$ the excess at node $s$ and deficit at node $t$ decreases, without changing the excess/deficit at any other node\footnotemark.
\footnotetext{The \emph{feasibility} of the solution has increased}
\end{lemma}
\begin{proof}
Line 2 finds a shortest path $P$ from the source node $s$ to a deficit node $t$. Line 3-4 then sends as much flow as possible along path $P$, subject to:
\begin{enumerate}
    \item satisfying the capacity constraints at each arc on $P$, and
    \item ensuring $e(s) \geq 0$ and $e(t) \leq 0$\footnotemark.
\end{enumerate}
The restriction on $e(s)$ and $e(t)$ ensures we improve the feasibility of the solution: this operation monotonically decreases the excess at source nodes and the deficit at sink nodes\footnotemark.
\footnotetext{If we were to allow the algorithm to 'overshoot' and turn $s$ into a deficit node or $t$ into an excess, we would not be guaranteed to be improving the feasibility of the solution.}

Since we send an equal amount of flow on each arc in $P$, the excess at nodes other than the start and end of path $P$ - $s$ and $t$ - are unchanged.
\end{proof}

\begin{thm}[Correctness] \label{thm:relax-correctness}
Upon termination, $\mathbf{x}$ is a solution of the minimum-cost flow problem.
\end{thm}
\begin{proof}
For the same reason given in \cref{thm:ssp-invariant}, the initial values of $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced cost optimality conditions. \textproc{AugmentFlow} and \textproc{UpdatePotentials} are only invoked when their preconditions are satisfied, and so by \cref{lemma:relax-correctness-augmentflow,lemma:relax-correctness-updatepotentials} it follows that reduced cost optimality is maintained. Moreover, the main algorithm given in \cref{algo:relaxation} does not update $\mathbf{x}$ or $\boldsymbol{\pi}$ except via calls to $\textproc{AugmentFlows}$ and $\textproc{UpdatePotentials}$. So, reduced cost optimality is maintained as an invariant throughout the algorithm.

The algorithm terminates when the mass balance constraints are satisfied, so $\mathbf{x}$ is feasible. Thus upon termination, $\mathbf{x}$ is feasible and satisfies reduced cost optimality conditions. So $\mathbf{x}$ is a solution to the minimum-cost flow problem. 
\end{proof}

\subsubsection{Termination and asymptotic complexity}

\begin{lemma} \label{lemma:relax-complexity-updatepotentials}
The time complexity of $\textproc{UpdatePotentials}$ is $O(m)$.
\end{lemma}
\begin{proof}
The for loop on lines 2-4 of \cref{algo:relaxation-update-potentials} iterates over $O(m)$ arcs. Saturating an arc is an $O(1)$ cost, so this contributes an $O(m)$ cost.

Computing $\alpha$ on line 5 is an $O(m)$ cost, since it involves iterating over $O(m)$ arcs\footnotemark.

Updating potentials on lines 6-8 is an $O(n)$ cost, as the number of nodes in $S$ is $O(n)$.

The overall time complexity is thus $O(m)$.
\end{proof}

\begin{lemma} \label{lemma:relax-complexity-augmentflow}
The time complexity of $\textproc{AugmentFlow}$ is $O(m)$.
\end{lemma}
\begin{proof}
The shortest path $P$ must be acyclic, and so its length is bounded by $n-1=O(n)$. The operations on line 2, 3 and 4 are all linear in the length of $P$, thus the overall time complexity is $O(n)$.
\end{proof}

\begin{lemma} \label{lemma:relax-iterations} 
The following properties hold:
\begin{enumerate}[label=(\alph*)]
    \item $\textproc{UpdatePotentials}$ is called at most $mCU$ times.
    \item $\textproc{AugmentFlow}$ is called at most $nU$ times in between calls to $\textproc{UpdatePotentials}$.
    \item There are at most $nmCU^2$ iterations of the outer loop on lines 2-18 of \cref{algo:relaxation}.
\end{enumerate}
\end{lemma}
\begin{proof}
By \cref{lemma:relax-dual-optimality}, $w\left(\boldsymbol{\pi}\right) \leq z^*$. A feasible flow can have a cost of at most $mCU$. By \cref{lemma:relax-correctness-updatepotentials} and \cref{assumption:integrality}, $w\left(\boldsymbol{\pi}\right)$ increases by at least one each time $\textproc{UpdatePotentials}$ is called. Moreover, $w\left(\boldsymbol{\pi}\right)$ never decreases: $\boldsymbol{\pi}$ is only modified by $\textproc{UpdatePotentials}$. Thus $\textproc{UpdatePotentials}$ is called at most $mCU$ times.

As for $\textproc{AugmentFlow}$, by \cref{lemma:relax-correctness-augmentflow} and \cref{assumption:integrality}, each call to $\textproc{AugmentFlow}$ decreases the total excess by at least one unit. The total excess in a network can be at most $nU$. The only part of the algorithm which may \emph{increase} the excess at nodes is lines 2-3 of $\textproc{UpdatePotentials}$. Thus, in between calls to $\textproc{UpdatePotentials}$, at most $nU$ calls to $\textproc{AugmentFlow}$ may be made.

For a bound on the total number of iterations, note each iteration ends after calling either $\textproc{AugmentFlow}$ on line 12 or $\textproc{UpdatePotentials}$ on line 16\footnotemark. Thus the number of iterations is bounded by the number of possible times either of these functions are called. By part (a), we know there are at most $mCU$ calls to $\textproc{UpdatePotentials}$. This together with part (b) implies there are at most $nmCU^2$ calls to $\textproc{AugmentFlow}$. This gives a bound on the number of iterations.
\footnotetext{Of course we might also call $\textproc{UpdatePotentials}$ on line 4, but there is no guarantee this will take place and so cannot form part of the complexity analysis.}
\end{proof}

\begin{thm} The algorithm runs in $O(nm^2CU^2)$ time.
\end{thm}
\begin{proof}
We will now analyse the algorithm as a whole. Initialisation on line 1 of \cref{algo:relaxation} has cost $O(m)$. The loop body on lines 3-17, excluding the inner loop on lines 6-15 and function calls, has cost $O(1)$: initialising $S$, $e(S)$ and $r(\boldsymbol{\pi},S)$ are all constant cost.

Now, let us consider the execution of the inner loop body on lines 7-14. In order to update $r(\boldsymbol{\pi},S)$, we must scan over all adjacencies when we add a node $j$ to $S$. This dominates the other costs of lines 8-10, since updating the predecessor on line 9 and inserting an element in a set on line 10 are both constant cost. Note it is natural for us to also maintain a queue of arcs $(i,j) \in \left(S,\overline{S}\right)$ in the residual network with $c_{ij}^{\boldsymbol{\pi}}=0$: this does not increase the asymptotic complexity of lines 8-10, since we are already doing much the same work when updating $r$. Given this queue, line 7 has cost $O(1)$. Thus the inner loop body has complexity, excluding procedure calls, of $O\left(\left|\mathrm{Adj}(j)\right|\right)$\footnotemark.
\footnotetext{Note we must include both \emph{incoming} and \emph{outgoing} arcs. Outgoing arcs need to be added to the sum in $r$ and added to the queue, whereas incoming arcs need to be subtracted from the sum in $r$ and removed from the queue. Summing the adjacency lists is thus bounded by $2m$ and not $m$, but this is still $O(m)$.}

Note each iteration of this loop adds a new node to $S$ on each iteration, and so is executed at most once per node. It follows that the total complexity of the inner loop on lines 6-15, excluding procedure calls, is $O(m)$. This loop thus dominates the other costs on lines 3-17, so the loop body overall has a cost of $O(m)$.

To deduce the overall time complexity, we will need the bounds proved in \cref{lemma:relax-iterations}. $\textproc{UpdatePotentials}$ is called $O(mCU)$ times and has cost $O(m)$, so contributes a total cost of $O(m^2CU)$. $\textproc{AugmentFlow}$ is called $O(nmCU^2)$ times and has cost $O(n)$, so contributes a total cost of $O(n^2mCU^2)$. Finally, the body of the while loop on lines 3-17 of \cref{algo:relaxation} is executed $O(nmCU^2)$ times. Excluding the cost of the procedure calls (which we have considered separately), each iteration costs $O(m)$, so this contributes a cost of $O(nm^2CU^2)$. This dominates the other costs, and is thus the overall time complexity of the algorithm.
\end{proof}

\section{Cost scaling algorithm} \label{sec:impl-cost-scaling}

% SOMEDAY: If you run a benchmark of reference implementations, cite it here.
This method is due to Goldberg and Tarjan~\cite{Goldberg:1987}. It runs in weakly polynomial time, with variants offering strongly polynomial bounds. It is one of the most efficient solution methods, both in theory and practice\footnotemark. Cost scaling works by successive approximation, and so it lends itself readily to an approximate solver (see \S\ref{sec:impl-approx}). It is inappropriate for an incremental solver, however.
\footnotetext{In computational benchmarks such as~\cite{KiralyKovacs:2012}, cost scaling is the fastest algorithm for many classes of network. It is occasionally beaten by other algorithms, however its performance still remains competitive in these cases. This makes it more robust than some alternative solution methods, such as relaxation (see \S\ref{sec:impl-relax}), which sometimes outperform it but can in other cases be slower by orders of magnitude.}

The algorithm maintains a feasible flow. The flow need not be optimal, but the error is bounded using the notion of $\epsilon$-optimality. Each iteration of the algorithm refines the solution, halving the error bound, until optimality is achieved. In the next section, the notion of $\epsilon$-optimality is introduced. Afterwards, the general solution method is described. Specific variants are considered next. Finally, techniques to improve the practical performance of the method are considered.

\subsection{$\epsilon$-optimality}

\begin{defn}[$\epsilon$-optimality] \label{defn:epsilon-optimality}
Let $\epsilon \geq 0$. A pseudoflow $\mathbf{x}$ is $\epsilon$-optimal with respect to node potentials $\boldsymbol{\pi}$ if the reduced cost of each arc in the residual network $G_\mathbf{x}$ is greater than $-\epsilon$:
\begin{equation} \label{eq:epsilon-optimality}
c^{\boldsymbol{\pi}}_{ij} \geq -\epsilon\:\forall (i,j) \in E_{\mathbf{x}}
\end{equation}
\end{defn}

\begin{remark}
Note this is a relaxation of the reduced cost optimality conditions given in \cref{eq:optimality-reduced-cost}. When $\epsilon = 0$, the definition of $\epsilon$-optimality in \cref{eq:epsilon-optimality} is equivalent to that of \cref{eq:optimality-reduced-cost}. We can, however, prove a stronger result.\\
\end{remark}

\begin{thm} \label{thm:epsilon-optimality-optimal}
Let $0 \leq \epsilon < 1/n$, and $\mathbf{x}$ an $\epsilon$-optimal feasible flow. Then $\mathbf{x}$ is optimal.
\end{thm}
\begin{proof}
Let $C$ be a simple cycle in $G_\mathbf{x}$. $C$ has at most $n$ arcs. By \cref{defn:epsilon-optimality}, for each arc $(i,j) \in C$ we have $c^{\boldsymbol{\pi}}_{ij} \geq -\epsilon$. It follows the cost of $C$ is $\geq -n\epsilon$.

By assumption on $\epsilon$, we have $n\epsilon > -1$. By \cref{assumption:integrality}, we can deduce that the cost of $C$ is $\geq 0$. The result follows from the negative cycle optimality conditions given in \cref{thm:optimality-neg-cycle}.
\end{proof}

\subsection{Generic algorithm}

% Active vertex = excess
% Admissible arc = negative reduced cost

\subsubsection{Description}

% Difference in presentation: they consider undirected edges, specify data structure.
% I'd like to keep presentation the same as in rest of chapter, not have to deal with
% forward and backwards edges. But this may not be possible...

% Goldberg starts by discussing main procedure, and analyses its asymptotic complexity. Maybe adopt this?
% Or could just have it be a lemma in the asymptotic complexity section.

\begin{algorithm}
\begin{algorithmic}[1]
    \Let{$\mathbf{x}$}{result of maximum-flow algorithm} \Comment{establishes $\mathbf{x}$ feasible}
    \Let{$\boldsymbol{\pi}$}{$\mathbf{0}$}
    \Let{$\epsilon$}{$C$} \Comment{where $C$ is largest arc cost, see \S\ref{sec:prep-flow-complexity}}
    \While{$\epsilon \geq 1/n$} \Comment{loop until $\mathbf{x}$ is optimal}
        \Let{$\epsilon$}{$\epsilon/2$}
        \State \Call{Refine}{$\mathbf{x}$,$\boldsymbol{\pi}$,$\epsilon$}
    \EndWhile
\end{algorithmic}
\caption{Cost scaling algorithm}
\label{algo:cost-scaling}
\end{algorithm}

The procedure described in \cref{algo:cost-scaling} is inspired by \cref{thm:epsilon-optimality-optimal}. First, the algorithm finds a feasible flow $\mathbf{x}$ and initialises the potentials $\boldsymbol{\pi}$ to zero, so reduced costs are equal to arc costs. $\left(\mathbf{x},\boldsymbol{\pi}\right)$ is then trivially $C$-optimal, and so $\epsilon$ is initialised with this value. Next, the algorithm iteratively improves the approximation using the \textproc{Refine} routine.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Require $\mathbf{x}$ is a pseudoflow
        \Ensure $\mathbf{x}$ is a feasible flow and $\left(\mathbf{x},\boldsymbol{\pi}\right)$ is $\epsilon$-optimal
        \Function{Refine}{$\mathbf{x}$,$\boldsymbol{\pi}$,$\epsilon$}
            \For{every arc $(i,j) \in E$} \Comment{initialisation}
                \If{$c^{\boldsymbol{\pi}}_{ij} > 0$} $x_{ij} \gets 0$ \EndIf
                \If{$c^{\boldsymbol{\pi}}_{ij} < 0$} $x_{ij} \gets u_{ij}$ \EndIf
            \EndFor
            \While{mass balance constraints not satisfied} \Comment{main loop}
                \State select an excess vertex $s$
                \If{$\exists (s,j) \in E_{\mathbf{x}} \cdot c^{\boldsymbol{\pi}}_{sj} < 0$} \Call{Push}{$s$,$j$}
                \Else \enspace\Call{Relabel}{$s$,$\epsilon$}
                \EndIf
            \EndWhile
        \EndFunction
    \end{algorithmic}
    \caption{Cost scaling: Generic \textproc{Refine} routine}
    \label{algo:cost-scaling-generic-refine}
\end{algorithm}

This routine is described in \cref{algo:cost-scaling-generic-refine}. Lines 2-4 ensures the reduced cost optimality conditions given in \cref{eq:optimality-reduced-cost} are satisfied. The resulting pseudoflow is $0$-optimal, but will in general not be feasible. Lines 5-8 bring the flow back to feasibility, by applying sequences of basic operations, \textproc{Push} and \textproc{Relabel}, both of which preserve $\epsilon$-optimality. One might wonder why we do not simply set $\epsilon < 1/n$ during lines 5-8 immediately. This approach is used in an algorithm due to Bertsekas~\cite{Bertsekas:1985}, however it results in exponential worst-case complexity.

Lines 5-8 of \textproc{Refine} are non-deterministic: there may be multiple source vertices $s$, and there may exist multiple arcs $(s,j) \in E_\mathbf{x}$ satisfying the condition given. We will show that the correct final solution will be produced whatever choice is made. Moreover, we can prove some complexity bounds for this generic version of the algorithm. However, both the theoretical and practical performance of the algorithm can be improved by certain rules for choosing between operations. Approaches for this are discussed further in \S\ref{sec:impl-cost-scaling-implementations}.

\begin{algorithm}
\begin{algorithmic}[1]
    \Require $e_i > 0$, $(i,j) \in E_{\mathbf{x}}$ and $c^{\boldsymbol{\pi}}_{ij} < 0$
    \Function{Push}{$i$,$j$}
        \State send $\min\left(e_i, r_{ij}\right)$ units of flow from $i$ to $j$
    \EndFunction    
    \setcounter{ALG@line}{0}
    \Statex
    \Require $e_i > 0$ and $\forall(i,j) \in E_{\mathbf{x}} \cdot c^{\boldsymbol{\pi}}_{ij} \geq 0$
    \Function{Relabel}{$i$,$\epsilon$}
        \Let{$\pi_i$}{$\min \left\{\pi_j + c_{ij} + \epsilon \::\: (i,j) \in E_{\mathbf{x}}\right\}$}
    \EndFunction
\end{algorithmic}
\caption{Cost scaling basic operations: push and relabel}
\label{algo:cost-scaling-operations}
\end{algorithm}

The basic operations \textproc{Push} and \textproc{Relabel} are described in \cref{algo:cost-scaling-operations}. \textproc{Push} sends as much flow as possible along arc $(i,j)$ from excess node $i$ to node $j$, without exceeding the excess at $e_i$ or the capacity constraint on arc $(i,j)$. \textproc{Relabel} increases the potential of $i$, decreasing the reduced cost $c_{ij}^{\boldsymbol{\pi}}$ of arcs leaving $i$, allowing more $\textproc{Push}$ operations to take place.

\subsubsection{Correctness}

First, we will prove that \textproc{Push} and \textproc{Relabel} preserve $\epsilon$-optimality. These results allow us to show \textproc{Refine} is correct, and hence that the cost scaling algorithm given in \cref{algo:cost-scaling} is correct. \\

\begin{lemma}[Correctness of \textproc{Push}] \label{lemma:cost-scaling-push-correctness}
Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ be $\epsilon$-optimal. Let the precondition for $\textproc{PUSH}(i,j)$ hold: $e_i > 0$, $(i,j) \in E_{\mathbf{x}}$ and $c^{\boldsymbol{\pi}}_{ij} < 0$. Then $\left(\mathbf{x},\boldsymbol{\pi}\right)$ continues to be $\epsilon$-optimal after $\textproc{PUSH}$.
\end{lemma}
\begin{proof}
$\textproc{PUSH}(i,j)$ increases the flow on arc $(i,j)$. By assumption, $(i,j)$ satisfied $\epsilon$-optimality prior to \textproc{PUSH}. $(i,j)$ may drop out of the residual network after increasing the flow, but this cannot violate optimality. If $(i,j)$ remains in the residual network, $(i,j)$ continues to satisfy $\epsilon$-optimality, since the reduced cost is unchanged.

However, sending flow along $(i,j)$ could add arc $(j,i)$ to the residual network. By the precondition $c^{\boldsymbol{\pi}}_{ij} < 0$, we have $c^{\boldsymbol{\pi}}_{ji} > 0 \geq -\epsilon$. Thus $(j,i)$ satisfies $\epsilon$-optimality.

No other changes are made which could affect the $\epsilon$-optimality conditions given in \cref{eq:epsilon-optimality}, so \textproc{Push} preserves $\epsilon$-optimality.
\end{proof}

\begin{lemma}[Correctness of \textproc{Relabel}] \label{lemma:cost-scaling-relabel-correctness}
Let $\left(\mathbf{x},\boldsymbol{\pi}\right)$ be $\epsilon$-optimal. Let the precondition for $\textproc{Relabel}(i)$ hold: $e_i > 0$ and $\forall(i,j) \in E_{\mathbf{x}} \cdot c^{\boldsymbol{\pi}}_{ij} \geq 0$. Then $\pi'_i\ \geq pi_i + \epsilon$ but $\boldsymbol{\pi}'$ is unchanged at nodes $j \neq i$, and $\left(\mathbf{x},\boldsymbol{\pi}'\right)$ continues to be $\epsilon$-optimal after $\textproc{Relabel}$.
\end{lemma}
\begin{proof}
By the precondition, $\forall(i,j) \in E_{\mathbf{x}} \cdot c^{\boldsymbol{\pi}}_{ij} \geq 0$. Substituting for the definition \cref{eq:reduced-costs} of reduced costs gives $\forall(i,j) \in E_{\mathbf{x}} \cdot c_{ij} + \pi_j \geq \pi_i$. Thus:
\[\pi'_i = \min \left\{\pi_j + c_{ij} + \epsilon \::\: (i,j) \in E_{\mathbf{x}}\right\} \geq \pi_i + \epsilon\]

Increasing $\pi_i$ has the effect of decreasing the reduced cost of outgoing arcs $(i,j)$, but increasing the reduced cost of incoming arcs $(j,i)$; the reduced cost of other arcs is unchanged. Increasing the reduced cost cannot violate $\epsilon$-optimality. However, decreasing the reduced cost may. But we have for any $(v,w) \in E_{\mathbf{x}}$:
\[c_{vw} + \pi_w - \min \left\{\pi_j + c_{ij} \::\: (i,j) \in E_{\mathbf{x}}\right\} \geq 0\]
Thus by definition of $\boldsymbol{\pi}'$, and using $\min\left\{f(x) + k \::\:x\right\} = \min\left\{f(x) \::\:x\right\} + k$:
\[c_{vw} + \pi_w - \pi'_i \geq -\epsilon\]
And so $c_{vw}^{\boldsymbol{\pi}'} \geq -\epsilon$, as required for $\epsilon$-optimality.
\end{proof}

\begin{thm}[Correctness of \textproc{Refine}] \label{thm:cost-scaling-refine-correctness}
Let the precondition $\textproc{Refine}$ hold: $\mathbf{x}$ is a pseudoflow. Then upon termination of \textproc{Refine}, the postcondition holds: $\left(\mathbf{x},\boldsymbol{\pi}\right)$ is $\epsilon$-optimal.
\end{thm}
\begin{proof}
The initial flow after lines 2-4 is $0$-optimal (and so certainly $\epsilon$-optimal). $\epsilon$-optimality is preserved by subsequent \textproc{Push} and \textproc{Relabel} operations by \cref{lemma:cost-scaling-push-correctness,lemma:cost-scaling-relabel-correctness}. Hence $\epsilon$-optimality is maintained as an invariant.
 
Given that \textproc{Refine} has terminated, the mass balance constraints must be satisfied by the loop condition on line 5 of \cref{algo:cost-scaling-generic-refine}. Thus $\mathbf{x}$ must also be a flow upon termination.
\end{proof}

\begin{cor}[Correctness of cost scaling]
Upon termination of the algorithm described in \cref{algo:cost-scaling}, $\mathbf{x}$ is a solution of the minimum-cost flow problem.  
\end{cor}
\begin{proof}
After each iteration of the algorithm, $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy $\epsilon$-optimality by \cref{thm:cost-scaling-refine-correctness}. Upon termination, $\epsilon < 1/n$. So by \cref{thm:epsilon-optimality-optimal}, $\mathbf{x}$ is an optimal solution.
\end{proof}

\subsubsection{Complexity}

\begin{lemma} \label{lemma:cost-scaling-operations-complexity}
The basic operations given in \cref{algo:cost-scaling-operations} have the following time complexities:
\begin{enumerate}[label=(\alph*)]
    \item $\textproc{Push}(i,j)$ runs in $O(1)$ time.
    \item $\textproc{Relabel}(i)$ runs in $O\left(\left|\mathrm{Adj}(i)\right|\right)$ time, that is linear in the number of adjacent arcs.
\end{enumerate}
\end{lemma}
\begin{proof}
Trivial.
\end{proof}

\begin{defn}
An invocation of $\textproc{Push}(i,j)$ is said to be \emph{saturating} if $r_ij = 0$ after the operation; otherwise, it is nonsaturating.\\
\end{defn}

\begin{remark}
Note a push operation is saturating if and only if $e_i \geq r_ij$ prior to calling \textproc{Push}.\\
\end{remark}

\begin{lemma} \label{lemma:cost-scaling-number-operations}
Within an invocation of \textproc{Refine}, the following upper bounds apply on the number of times each basic operation is performed:
\begin{enumerate}[label=(\alph*)]
    \item $O(n^2)$ \textproc{Relabel} operations.
    \item $O(nm)$ saturating \textproc{Push} operations.
    \item $O(n^2m)$ nonsaturating \textproc{Push} operations.
\end{enumerate}
\end{lemma}
\begin{proof}
See~\cite[lemma~6.3, lemma~6.4, lemma~6.7]{Goldberg:1987}.
\end{proof}

\begin{thm} \label{thm:cost-scaling-refine-complexity}
Within an invocation of \textproc{Refine}, the basic operations contribute a complexity of $O(n^2m)$.
\end{thm}
\begin{proof}
We will use \cref{lemma:cost-scaling-operations-complexity,lemma:cost-scaling-number-operations}. \textproc{Relabel} is executed at most $O(n^2\cdot)$ times, with each operation having a cost of $O\left(\left|\mathrm{Adj}(i)\right|\right) = O(n)$\footnotemark, contributing a total of $O(n^3)$. There are up to $O(nm)$ saturating and $O(n^2m)$ nonsaturating \textproc{Push} operations. Each has a cost of $O(1)$, and so \textproc{Push} contributes a total cost of $O(n^2m)$. Hence the basic operations executed by \textproc{Refine} contribute a complexity of $O(n^3 + n^2m) = O(n^2m)$.
\end{proof}

\begin{remark}
It is unfortunately not possible for us to prove a bound on the complexity of the generic refine routine given in \cref{algo:cost-scaling-generic-refine}, since it is dependent on how excess vertices are selected on line 6 and arcs are chosen on line 7. We will prove bounds on specific implementations of refine in \S\ref{sec:impl-cost-scaling-implementations}.\\
\end{remark}

\begin{thm} [{Adapted from~\cite[theorem~4.1]{Goldberg:1987}}] \label{lemma:cost-scaling-overall-algorithm}
Let $R(n,m)$ be the running time of the \textproc{Refine} routine. Then the minimum-cost flow algorithm described in \cref{algo:cost-scaling} runs in $O\left(R(n,m)\lg(nC)\right)$ time.
\end{thm}
\begin{proof}
The algorithm terminates once $\epsilon < 1/n$, which takes place after $\log_2\left(\frac{C}{1/n}\right) = \log_2 (nC)$ iterations of lines 4-6. The loop condition on line 4 and the assignment on line 5 are both $O(1)$, so the dominant cost of each iteration is the execution of \textproc{Refine}. Lines 4-6 thus contribute a cost of $O\left(R(n,m)\lg(nC)\right)$. This dominates the initialisation on lines 1-3, and thus is the overall cost of the algorithm.
\end{proof}

\subsection{Algorithm implementations} \label{sec:impl-cost-scaling-implementations}

The previous section describes the generic version of the algorithm. It cannot be implemented in the form described so far: lines 5-8 of \cref{algo:cost-scaling-generic-refine} are non-deterministic, in the sense that the order in which basic operations is applied is not defined. This order affects the performance of the algorithm, both in theory and in practice. For this project, I implemented two versions, both described by Goldberg~\cite{Goldberg:1990}: "first-active" and "wave".

\subsubsection{FIFO refine}

\begin{algorithm}
\begin{algorithmic}[1]
    \Function{Refine}{$\mathbf{x}$,$\boldsymbol{\pi}$,$\epsilon$}
        \State initialisation as in lines 2-4 of \cref{algo:cost-scaling-generic-refine}
        \Let{$Q$}{$\left[s \in V \::\: e_s > 0\right]$} \Comment{$Q$ is a queue of excess nodes}
        \While{$Q$ not empty}
            \State pop head of $Q$ into $s$
            \State \Call{Discharge}{$s$,$\epsilon$} \Comment{May add nodes to $Q$}
            \If{\textproc{Relabel} called by \textproc{Discharge}}
                \State add $s$ to rear of $Q$
                \Break
            \EndIf
        \EndWhile
    \EndFunction
\end{algorithmic}
\caption{Cost scaling: FIFO \textproc{Refine} routine}.
\label{algo:cost-scaling-first-active-refine}
\end{algorithm}

\Cref{algo:cost-scaling-first-active-refine} maintains a first-in-first-out (FIFO) queue $Q$ of excess nodes (see \S\ref{sec:prep-flow-pseudo}). The initial order of vertices in the queue is arbitrary. At each iteration, an excess node $s$ is removed from the head of the queue, and \emph{discharged} by applying a sequence of \textproc{Push} and \textproc{Relabel} operations (described below). During execution of the algorithm, new nodes may gain an excess; in this case, they must be added to the rear of $Q$.

\begin{algorithm}
\begin{algorithmic}[1]
    \Require{$e_s > 0$}
    \Function{Discharge}{$s$,$\epsilon$}
        \Repeat
            \State \Call{PushOrRelabel}{$s$,$\epsilon$}
        \Until $e_s \leq 0$
    \EndFunction
    \setcounter{ALG@line}{0}
    \Statex
    \Require{$e_s > 0$}
    \Function{PushOrRelabel}{$s$,$\epsilon$}
        \State let $(i,j)$ be the current arc of $s$
        \If{$(i,j) \in E_\mathbf{x}$ and $c_{ij}^{\boldsymbol{\pi}}$} \Call{Push}{$i$,$j$}
        \Else
            \If{$(i,j)$ last arc on the adjacency list for $s$}
                \State \Call{Relabel}{$s$}
                \Let{current arc}{first arc in adjacency list}
            \Else
                \Let{current arc}{next arc in adjacency list}
            \EndIf
        \EndIf
    \EndFunction
\end{algorithmic}
\caption{Cost scaling: \textproc{Discharge} and helper routine \textproc{PushOrRelabel}}
\label{algo:cost-scaling-discharge}
\end{algorithm}

$\textproc{Discharge}(s)$ is described in \cref{algo:cost-scaling-discharge}. It may be applied to any excess node, and performs a sequence of push operations until $e_s = 0$ or \textproc{Relabel} is called. For each node $i \in V$, an adjacency list is maintained, in a fixed (but arbitrary) order. The helper routine \textproc{PushOrRelabel} walks over this adjacency list. A \textproc{Push} operation is performed where possible; otherwise, we skip to the next arc and do nothing. \textproc{Relabel} is only invoked when all arcs in the adjacency list have been either pushed, or skipped; at this point, no other basic operation can be applied to $s$.

Note $\textproc{Push}(i,j)$ may make $j$ become an excess node. \textproc{Push} must be modified to add $j$ to the rear of $Q$ in this case. \textproc{Refine} pops $s$ from $Q$ and then calls \textproc{Discharge}. But if \textproc{Relabel} is called by \textproc{Discharge}, $s$ will still be an active vertex. In this case, we also add $s$ to the rear of $Q$\footnotemark.\\
\footnotetext{It would also be legal to add $s$ to the front of $Q$, and this would result in an algorithm with the same theoretical performance.}

\begin{thm} \label{thm:cost-scaling-first-active-complexity}
The algorithm with first-active refine has a running time of $O(n^2m \lg (nC))$.
\end{thm}
\begin{proof}
See \cite[theorem~6.2]{Goldberg:1990}.
\end{proof}

\begin{remark}
In fact, this complexity bound holds for any refine routine which repeatedly applies \textproc{PushOrRelabel}. However, it has been conjectured by Goldberg that the first-in first-out ordering of vertices used in this variant results in a tighter bound, of $O(n^3 \lg (nC))$. Proving or disproving this assertion is an open research problem.
\end{remark}

\subsubsection{Wave refine}

\begin{algorithm}
\begin{algorithmic}[1]
    \Function{Refine}{$\mathbf{x}$,$\boldsymbol{\pi}$,$\epsilon$}
    \State initialisation as in lines 2-4 of \cref{algo:cost-scaling-generic-refine}
    \Let{$L$}{list of nodes in $V$} \Comment{order of nodes is arbitrary}
    \Repeat
        \For{each vertex $i$ in list $L$}
            \If{$e_i > 0$}
                \State \Call{Discharge}($i$,$\epsilon$)
                \If{\textproc{Relabel} called by \textproc{Discharge}}
                    \State move $i$ to front of $L$
                \EndIf
            \EndIf
        \EndFor
    \Until{no excess vertex encountered in \textbf{for} loop}
    \EndFunction
\end{algorithmic}
\caption{Cost scaling: Wave \textproc{Refine} routine}
\label{algo:cost-scaling-wave-refine}
\end{algorithm}

\Cref{algo:cost-scaling-wave-refine} describes an approach which can be proved to achieve this tighter bound. The method maintains a list $L$ of all nodes in the network, rather than the queue $Q$ maintained by the first-active method. The algorithm preserves the invariant that $L$ is topologically ordered with respect to the \emph{admissible graph}: the subgraph of the residual network containing only arcs with negative reduced cost.

Initially, $L$ contains vertices in arbitrary order. But the admissible graph is empty after line 2 of \cref{algo:cost-scaling-wave-refine}, since lines 2-4 of \cref{algo:cost-scaling-generic-refine} saturate any negative reduced cost arcs, making them drop out of the residual network. Thus $L$ is trivially topologically ordered.

\textproc{Push} operations cannot create admissible edges, and so preserve the topological ordering. \textproc{Relabel} operations can. However, immediately after $\textproc{Relabel}(s)$, there are no admissible edges entering $s$~\cite[lemma~6.5]{Goldberg:1987}. Thus moving $s$ to the beginning of $L$ maintains topological ordering.\\

\begin{thm} \label{thm:cost-scaling-wave-complexity}
The algorithm with wave refine has a running time of $O(n^3 \lg (nC))$.
\end{thm}
\begin{proof}
With this invariant, we can show an $O(n^2)$ bound on the number of passes over the vertex list~\cite[lemma~7.3]{Goldberg:1987}. This allows an $O(n^3)$ bound on the running time of the wave \textproc{Refine} routine to be proved~\cite[theorem~7.4]{Goldberg:1987}. The result follows by \cref{lemma:cost-scaling-overall-algorithm}.
\end{proof}

% \paragraph{Alternative implementations}
% Mention dynamic trees, blocking flows, why I didn't choose to implement these.
% Reason: they're complicated, and reason to believe a large constant factor makes them slower in practice.
% But quite difficult to justify succintly. Perhaps best not mention at all? Reader will not know about them, and mentioning them may just confuse things.

\subsection{Heuristics}

So far, we have considered means by which the theoretical performance of the algorithm may be improved. There has also been considerable research effort on improving the practical performance by means of \emph{heuristics} guiding the actions of the algorithm~\cite{Goldberg:1997}. Whilst these do not alter the asymptotic worst-case time complexity, empirically they yield considerable performance gains. The heuristics are summarised in \cref{appendix:csheuristics}. 

I decided not to implement any heuristics in this project. My focus is on exploring new avenues for improving performance. These heuristics are already well-studied: implementing them would have been unlikely to uncover anything new, and would have diverted considerable development time from other efforts. I judged that it was better to keep my implementation simple, to allow for rapidly exploring new approaches, such as those discussed in \S\ref{sec:impl-approx}.

% Optimisations: changing scaling factor, maintaining integrality, wave vs sequential
% Optimisations may be better left for evaluation? If you do want to include it here, should merge it into heuristics with a section e.g. Improving performance

\section{Approximation algorithms} \label{sec:impl-approx}

\subsection{Choice of algorithm} \label{sec:impl-approx-choice}

Why Goldberg's cost scaling. Iterative, so readily amenable to approximate solutions. So are others: e.g. cycle cancelling. But Goldberg is fastest: forward reference to benchmarks you've run, or just Kiraly \& Kovacs paper.

\subsection{An iterative approximation}

Approximate algorithm yielded by ending iteration before optimality reached. We have a choice of termination condition.

No way to go from $\epsilon$-optimality to measure of accuracy. So must adopt heuristic approaches.

\subsubsection{Convergence based on cost change}

\subsubsection{Convergence based on task allocations}

\subsubsection{Hybrid schemes}

My code allows the two to be readily combined to produce different policies.

\section{Incremental algorithms} \label{sec:impl-incremental}

Flow scheduling systems produce a sequence of closely related flow networks. Cluster events -- such as task submission or completion -- trigger an update of the flow network. Most of these changes are small, adding or removing a node and a handful of arcs. The optimal flow will therefore tend to remain mostly the same. 

My method of \emph{incremental} solution is designed to exploit this. The algorithm reuses the solution for the old flow network, \emph{reoptimising} to produce an answer on the new network. This approach has proved extremely successful, producing a factor of 100 speed-up in my tests (see \S\ref{sec:eval-incremental}). I believe my implementation is the first of its kind, as there is no mention of this method in the existing literature.

Flow scheduling is ideally suited to an incremental solution method. This may go some way to explaining the lack of prior work: this approach is much less compelling for traditional applications of flow networks. As previously mentioned, most changes to the network in flow scheduling are small. This alone does not make the problem easy: reoptimising after even a single change is, in general, as hard as solving the problem from scratch\footnotemark. Informally, the problem is that since the optimisation problem is global, updates in one region of the network may trigger a cascading sequence of changes in the optimal flow affecting distant regions.
\footnotetext{TODO: Idea is get two networks $A$ and $B$. WLOG assume they have source and sink node. Remove supply/demand at those nodes. Introduce new supply and sink node. Add uncapacitated arcs with zero cost from old supply nodes to new supply. Add uncapaciated arcs one with medium cost, one with high cost. Then dropping high cost to zero cost is as hard as solving for one of the networks.}

Crucially, however, an explicit design goal of schedulers is to produce stable assignments of tasks to machines. Preempting a task or migrating it to a different machine are expensive operations: they must sometimes be performed, but only sparingly. Consequently, cost models will naturally be designed to avoid frequent changes in scheduling assignments. This is precisely the property required for incremental solvers to perform well.

My incremental solution method was inspired by a number of sources. The Quincy paper highlighted an incremental solver as an area for further research~\cite[\S6.5]{Isard:2007}. There has been some prior work on reoptimisation and sensitivity analysis on flow networks, which are closely related to the incremental problem. Unfortunately, there has been little study of this area compared to other flow problems. However, the work which has been conducted has proved helpful in my project, and is surveyed below.

Amini and Barr published a comprehensive empirical evaluation of reoptimisation algorithms in 1993~\cite{Amini:1993}. However, this paper is now primarily only of historical interest. The study considered algorithms such as the out-of-kilter method, which have long since become obsolete. Moreover, the largest network in their tests had only 1,500 nodes, orders of magnitude smaller than the ones tackled in this project. The most recent work in this area was performed almost a decade ago by Frangioni and Manca in 2006~\cite{Frangioni:2006}, who undertook a computational study of algorithms for reoptimisation after arc cost changes. Whilst the approaches considered are more contemporary than in the Amini study, the results may not generalise to flow scheduling, where any parameter -- and not just arc costs -- may change. 

Sensitivity analysis on flow networks is concerned with determining how uncertainty in input parameters causes uncertainty in the optimal solution~\cite[\S9.11]{Ahuja:1993}. In scientific applications, this may be used to test the robustness of a model: small changes in the input should produce only small changes in the output, otherwise the model is too sensitive to measurement error. Whilst the goals of sensitivity analysis are considerably different to those of my incremental solver, many of the underlying techniques can be adapted.

The next section will discuss which of the existing flow algorithms are most suited to being modified to operate in an incremental mode. Having made this decision, the following section describes the general approach to building an incremental solver out of these algorithms. The remaining sections describe details of the implementation using each algorithm.

\subsection{Architecture of an incremental algorithm}

\Cref{sec:impl-cycle-cancelling,sec:impl-ssp,sec:impl-relax,sec:impl-cost-scaling} described the implementation of pre-existing algorithms. These are designed to solve the "full" minimum-cost flow problem: finding an optimal solution without the benefit of any existing state. However, it is possible to adapt them to operate in an incremental fashion. An obvious approach is to simply rerun the algorithm on the updated network, using the old state as a starting point.

However, this will not quite work. Flow algorithms typically maintain an invariant on the state of the solver during intermediate computations. After updating the network, this invariant may well fail to hold on the old state. When the invariant is a precondition for the algorithm, as in relaxation, reoptimisation could fail. In other algorithms, such as cost scaling, correctness is not dependent on the invariant. But the algorithm may run slower than the normal time complexity bounds, as the proofs depend on the invariant. To correct this, we will adopt a three-step approach:

\begin{enumerate}
    \item Receive changes to the flow network, updating the representation of the network.
    \item Manipulate state saved from the previous run of the solver, in order to satisfy any invariants under the updated network.
    \item Run the algorithm, more or less as usual.
\end{enumerate}

% Ultimately, updating the graph isn't that big a problem. Most dynamic graph data structures seek to maintain some other property, e.g. shortest path solutions, or sets of connected components. But I think this is still the best reference to give?
Note that, in practice, implementation is considerably more challenging than suggested by this summary. Step 1 superficially appears trivial, but many graph data structures cannot be efficiently modified. Considerable research effort has gone into devising \emph{dynamic} data structures to resolve this problem, \textit{cf.\@}~\cite{Tarjan:1983,Eppstein:1996}. Furthermore, many algorithms rely on auxiliary data structures\footnotemark: these must be updated, in addition to the network itself.
\footnotetext{For example, many algorithms maintain a set of excess vertices, to improve performance.}

Step 2 varies considerably between algorithms, and so will not be discussed until \cref{sec:impl-incremental-impl}. Ironically, step 3 is generally the simplest: the logic at the core of the algorithm can remain mostly unmodified. Typically, all that is needed is to disable any initialisation code, since we are starting from a legal intermediate state.

%\footnotetext{For example, a representation of adjacencies that can be very efficient is to allocate an array large enough to store the adjacency list for each node. These are then laid out sequentially, and a separate array stores the start and end index in the adjacency array for each node. This achieves excellent spatial locality, giving a high cache hit rate when scanning the adjacency list. Inserting an arc into this data structure is extremely expensive, however, as it may require moving a large number of other elements.}

\subsection{Choice of algorithms}
I will discuss, for each of the algorithms described previously in this chapter, the suitability of that algorithm to being used in an incremental mode.

\subsubsection{Cost scaling} 
Goldberg's cost scaling algorithm offers excellent theoretical and practical performance on the full problem. Unfortunately, it fares comparatively poorly on the incremental problem. 

During the main loop in \cref{algo:cost-scaling}, cost scaling maintains the invariant that $\mathbf{x}$ is a feasible flow and $\left(\mathbf{x},\boldsymbol{\pi}\right)$ is $\epsilon$-optimal. This invariant is not required for correctness -- \textproc{Refine}'s precondition is just that $\mathbf{x}$ satisfies flow conservation -- but is needed to prove the asymptotic complexity bounds.

% SOMEDAY: I think my comments about restoring feasibility are right, but think about it some more?
The starting state is of course $0$-optimal on the old network. However, it need not be feasible or optimal on the new network. To satisfy the invariant, we will need to first restore feasibility, such as by running a max-flow algorithm "incrementally"\footnotemark. This step will likely cause more arcs to violate the reduced cost optimality conditions.
\footnotetext{In fact, it is perfectly legitimate to skip this stage and run \textproc{Refine} directly, which will also restore feasibility. However, the analysis is simpler if we consider these stages to be separate.}

Next, we will need to calculate the value of $\epsilon$ for which $\left(\mathbf{x},\boldsymbol{\pi}\right)$ is $\epsilon$-optimal: this can be done in one pass over the arcs in the residual network\footnotemark. Alternatively, we could apply the potential refinement heuristic described by Goldberg~\cite[\S3.2]{Goldberg:1997}: this might find a smaller value of $\epsilon$, by updating the potentials $\boldsymbol{\pi}$.
\footnotetext{This could conceivably be made more efficient by keeping track of which arcs have changed cost in the network update, or been added to the residual network when restoring feasibility.}

% I'm not sure that restoring feasibility does break $\epsilon$-optimality. We can just invoke Refine on the non-feasible flow, that's fine I think.
The key problem with using cost scaling incrementally is that $\epsilon$ may be very large after even a small update. Recall that $\epsilon$-optimality (see \cref{defn:epsilon-optimality}) is defined to be a property satisfied by every arc. $\epsilon$ is thus a measure of the \emph{maximum} error in the solution, not the average. An update as simple as changing the cost on a single arc could therefore raise $\epsilon$ to a high value, even though the rest of the network would remain $0$-optimal\footnotemark.

Since $\epsilon$ may end up close (in logarithmic terms) to the initial starting value $C$, the algorithm is likely to perform almost as many iterations of \textproc{Refine} as it normally would. We cannot hope \textproc{Refine} to run much faster than usual, either. Whilst \textproc{Refine} maintains $\epsilon$-optimality, it may also make some previously optimised segments of the network \emph{less} optimal (up to the bound of $\epsilon$), effectively creating more work for future iterations.

%SOMEDAY: Would be interesting to try and model the performance of CS2. Simple approach which could work: assume new epsilon is ~U[0,C], what is the mean and variance of log_2 (C/epsilon)?
The computational study by Frangioni and Manca~\cite{Frangioni:2006} provides empirical support for this assertion. A modest speed-up of around a factor of 2 was observed when cost scaling was used for cost reoptimisation. This result is disappointing compared to other algorithms they surveyed, where speed-ups of a factor of 20 or more were observed. Moreover, the performance of cost scaling was unpredictable, varying considerably even within networks of the same class.

Given the theoretical and empirical evidence against cost scaling, I decided to focus my attention on more promising algorithms.

\subsubsection{Successive shortest path and relaxation}

Both algorithms work by sending flow along \emph{augmenting paths} from a source to sink node with zero or negative reduced costs. Whilst their implementation differs considerably, they have similar incremental properties. In particular, note that both algorithms maintain the invariant that $\left(\mathbf{x},\boldsymbol{\pi}\right)$ satisfy reduced-cost optimality conditions, and seek to successively improve the feasibility of $\mathbf{x}$\footnotemark.
\footnotetext{This is most clear in the successive shortest path algorithm, which improves the feasibility of $\mathbf{x}$ on every iteration. It is not strictly speaking the case for the relaxation algorithm, since the the \textproc{UpdatePotentials} operation may make the flow \emph{less} feasible. But, indirectly, \textproc{UpdatePotentials} is still working towards feasibility since it allows for more \textproc{AugmentFlow} operations to take place.}

Whilst changes to the network can violate reduced cost optimality, it is relatively simple to update the pseudoflow to maintain optimality, as described in \S\ref{sec:impl-incremental-restoring-rc}. Provided this invariant is maintained, the algorithm can simply be run as usual to reoptimise, restoring feasibility. The time this takes will tend to be proportional to the degree to which the feasibility of the flow was reduced by updates\footnotemark. This is a very desirable property for flow scheduling: most of the changes are small, and so should only impact the feasibility of a small region of the network.
\footnotetext{Note the feasibility may be reduced directly and indirectly. Direct decreases in feasibility occur as an immediate  consequence of a network update: for example, adding a new source and sink node. The feasibility may also be compromised indirectly: the operations needed to maintain reduced cost optimality might require pushing flows along arcs, resulting in excesses or deficits at nodes.}

The relaxation algorithm was evaluated empirically in the study by Frangioni and Manca. As previously discussed in \S\ref{sec:impl-relax}, the practical performance characteristics of the algorithm are heavily dependent on the class of problem. This was apparent in the study: relaxation was competitive on the NETGEN, GRIDGEN and GOTO instances but had disappointing performance on PDS instances~\cite[tables~1~to~4]{Frangioni:2006}. Since relaxation has been found to perform well on flow scheduling networks in the full problem\S(TODO: reference to evaluation), there are reasons to be optimistic about its performance as an incremental solver.

The literature does not contain any benchmarks for successive shortest path in the reoptimisation context. However, there are reasons to believe it will have a similar relative speed-up to relaxation. Its performance is likely to be more robust across different problem types than relaxation, but never reach the peak performance achieved by relaxation.

There is a considerable theoretical basis to favour this class of algorithm for incremental solution methods. Prior to this project, there has been little in the way of empirical investigation into this problem, so it is not possible to draw any firm conclusion. However, the only benchmark which has considered relaxation in this context produced a broadly positive result. Consequently, I decided to implement incremental verisons of successive shortest path and relaxation, described further in \S\ref{sec:impl-incremental-impl}.

\subsubsection{Cycle cancelling}

Whilst the cycle cancelling algorithm could in principle adapted for use in an incremental mode, there is little to recommend it. The algorithm has the same drawbacks as cost scaling, with none of the benefits. Like cost scaling, it maintains the invariant that $\mathbf{x}$ is a feasible flow, which might be violated by updates to the network. This can be fixed by running a maximum flow algorithm "incrementally", but this may severely degrade the optimality of the flow. 

The only benefit cycle cancelling has to offer over cost scaling is that it will monotonically decrease the cost of the solution during reoptimisation\footnotemark. This may cause cycle cancelling to enjoy a larger \emph{relative} speed-up in incremental mode than cost scaling, although it will still lag that of the augmenting path algorithms.

However, cost scaling was only worth considering because it is one of the fastest algorithms on the full problem, certainly trouncing successive shortest path and often beating relaxation. Cycle cancelling, by contrast, is extremely slow on the full problem. It therefore offers no benefits over the augmenting path algorithms, and so was not implemented in an incremental mode in this project.
\footnotetext{It is quite possible for the cost of the solution to temporarily \emph{increase} under cost scaling. Whilst $\epsilon$-optimality imposes an upper bound on the cost, there is no exact relationship. Cost increases are especially likely to occur during reoptimisation, where the initial cost is already close to the minimum, but $\epsilon$ may be large.}

\subsubsection{Conclusion}

The class of augmenting path algorithms, consisting of relaxation and successive shortest path, seem particularly promising for an incremental solver. Consequently, incremental versions of both these algorithms will be implemented, as described in the next section.

Goldberg's cost scaling algorithm enjoys around a factor of 2 speed-up from running in incremental mode. Ultimately, improvements of an order of magnitude or more are required to make flow scheduling systems viable, so this speed-up is too small. Note that the performance of incremental cost scaling is much less sensitive on the number of updates made to the graph than are the augmenting path algorithms\footnotemark. Whilst incremental cost scaling is not suitable for this application, I would conjecture it might be appropriate for use cases where there are frequently large numbers of changes. %SOMEDAY: reference further work, idea of an adaptive scheduler?
\footnotetext{The augmenting path algorithms take time roughly proportional to the number of changes. By contrast, for cost scaling, the time spent reoptimising depends principally on how large $\epsilon$ is after the update. This in in turn depends on the "worst" change received. As the number of changes grows, the likelihood that a new change will be worse than all existing changes shrinks, so $\epsilon$ grows sub-linearly in the number of changes. However, $\epsilon$ grows very quickly initially, making it unattractive for small numbers of changes.}

Cycle cancelling has similar problems to cost scaling, with the added disadvantage of being one of the slowest algorithms on the full problem. Consequently, neither cycle cancelling nor cost scaling will be extended to support incremental operation.

\subsection{Maintaining reduced cost optimality}

Augmenting path algorithms can only run on an initial state that satisfies reduced cost optimality. Maintaining reduced cost optimality of the flow as the network is updated is therefore a prerequisite for adapting these algorithms to run incrementally. Recall these conditions were first defined in \cref{eq:optimality-reduced-cost}, and for reference are:
\[c_{ij}^{\boldsymbol{\pi}}\geq0\:\forall(i,j)\in E_{\mathbf{x}}\]

Normally, reduced cost optimality is satisfied upon entering the routine by initialising both $\mathbf{x}$ and $\boldsymbol{\pi}$ to $\mathbf{0}$. It is subsequently preserved by the algorithms, see \S\ref{sec:impl-ssp-analysis} and \S\ref{sec:impl-relax-analysis}. When using one of these algorithms to reoptimise a network, the initialisation is skipped\footnotemark. The reduced cost optimality conditions must thus be satisfied prior to the algorithm being invoked.
\footnotetext{Otherwise, we would be discarding the very state that we seek to exploit!}

Fortunately, there is a simple method to restore reduced cost optimality after the network is updated. Note that we cannot, in general, hope to also maintain feasibility\footnotemark: otherwise, this routine would itself solve the incremental problem! I describe below how the flow is to be modified to preserve optimality, in response to each possible type of network change. Note that this method modifies only the flow, and not potentials.
\footnotetext{Indeed, some changes may make the network itself infeasible. The flow scheduling system produces changes in batches. Reoptimisation only takes place at the end of each batch, at which point the network is guaranteed to be feasible.}

\subsubsection{Node changes}

Adding or removing a node with no arcs cannot violate reduced cost optimality conditions, as the set of arcs is unchanged. Of course, a node must be added or removed along with its associated arcs\footnotemark, but this is considered in the next section. When a node $i$ is added, the potential vector $\pi$ must be extended. The value $\pi_i$ is initialised to is arbitrary: zero is used in my implementation.
\footnotetext{It must have some arcs, since the graphs is required to be (directly) connected.}

It is also possible to change the supply or demand $b_i$ of a node $i$\footnotemark. Since this does not affect the reduced cost of any arcs, it also cannot violate the optimality conditions, so no action need be taken.
\footnotetext{Although for flow scheduling, in practice this only happens at the sink node $S$} 

\subsubsection{Adding or removing an arc}

Note that an arc with zero capacity is equivalent to their being no such arc: it is never present in the residual network (nor is its reverse), and always has a flow of zero. Thus, it is possible to model addition of an arc $(i,j)$ with cost $c_ij$ and capacity $u_{ij}$ in terms of increasing the capacity of an arc $(i,j)$ with cost $c_ij$ from $0$ to $u_ij$. Similarly, removing an arc $(i,j)$ corresponds to decreasing the capacity to $0$. Maintaining reduced cost optimality in the presence of these capacity changes is described in the following section.

\subsubsection{Changing the capacity of an arc}

Changing the capacity of an arc $(i,j)$ from $u_ij$ to $u'_{ij}$ does not change $c_ij$ or the potentials $\boldsymbol{\pi}$. The reduced cost $c_{ij}^{\boldsymbol{\pi}}$ therefore remains unchanged.

When decreasing the capacity of an arc, we set $x_{ij} \gets \min\left(x_{ij},u'_{ij}\right)$ to ensure the capacity constraints in \cref{eq:capacity-constraints} continue to hold\footnotemark. Reduced cost optimality is always preserved. Suppose the arc is not present in the residual network prior to the update: that is, $(i,j) \not \in E_\mathbf{x}$. Then the arc was previously saturated with $x_{ij} = u_ij$, and we will now have $x_{ij} = u'_{ij}$. Thus the arc remains saturated, and so $(i,j) \not \in E_\mathbf{x}$ continues to hold. Otherwise, if $(i,j) \in E_\mathbf{x}$, then as reduced cost optimality was satisfied prior to the update it follows that $c_{ij}^{\boldsymbol{\pi}} \geq 0$. It may be the case that $x_{ij} \geq u'_{ij}$, in which case $(i,j) \not \in E_\mathbf{x}$ after the update, which is harmless. Otherwise, $(i,j) \in E_\mathbf{x}$ and $c_{ij}^{\boldsymbol{\pi}} \geq 0$ continues to hold, satisfying the optimality conditions.
\footnotetext{Whilst the primary goal of this method is to preserve reduced cost optimality, it a more primitive requirement is that $\mathbf{x}$ continues to be a pseudoflow!}

There are two cases when increasing the capacity of an arc. If $c_{ij}^{\boldsymbol{\pi}} \geq 0$, then no action need be taken. However, if $c_{ij}^{\boldsymbol{\pi}} < 0$ then we must set $x_{ij} \gets u'_{ij}$, so that $(i,j)$ remains absent from the residual network.

\subsubsection{Changing the cost of an arc}

Changing the cost of an arc $(i,j)$ from $c_{ij}$ to $c'_{ij} = c_{ij} + \delta$ will also change the reduced cost from $c_{ij}^{\boldsymbol{\pi}}$ to $c_{ij}^{\prime\boldsymbol{\pi}} = c_{ij}^{\boldsymbol{\pi}} + \delta$. It is simplest for us to consider the complementary slackness conditions, which are equivalent to reduced cost by \cref{thm:optimality-complementary-slackness}.

\paragraph{Case $c_{ij}^{\prime\boldsymbol{\pi}} > 0$}
In this case, we require $x_{ij} = 0$. This will already be the case if $c_{ij}^{\boldsymbol{\pi}} > 0$, but otherwise it may require a flow update.

\paragraph{Case $c_{ij}^{\prime\boldsymbol{\pi}} = 0$}
This case is a no-op. Complementary slackness is satisfied for all values of $x_{ij}$ satisfying the capacity constraints $0 \leq x_{ij} \leq u_{ij}$.

\paragraph{Case $c_{ij}^{\prime\boldsymbol{\pi}} < 0$}
In this case, we require $x_{ij} = u_{ij}$. This will already be the case if $c_{ij}^{\boldsymbol{\pi}} < 0$, but otherwise it may require a flow update.

\subsection{Implementations} \label{sec:impl-incremental-impl}

Three incremental solvers have been developed as part of this project. My implementations of the successive shortest path and relaxation algorithm, described in \cref{sec:impl-ssp} and \cref{sec:impl-relax}, are adapted to support operating in an incremental mode. In addition, I have extended RelaxIV~\cite{BertsekasCodes:1988,RelaxIV:2011} -- a highly optimised reference implementation of the relaxation algorithm -- to operate as an incremental solver. 

\subsubsection{Modifying implementations developed in this project}
This proved relatively straightforward, in large part reflecting the emphasis during earlier development placed on building a modular system. The algorithms themselves needed minimal modifications: a flag was added to indicate whether the initialisation routine should run (when solving the full problem) or be skipped (in the incremental case). 

A separate class was implemented to preserve reduced cost optimality, as described above. This class was able to wrap any graph object, forwarding changes to the underlying data structure whilst updating flows as appropriate. This design allowed for the logic to be shared between the two algorithms, and since the class presented the same interface as the existing graph classes it did not require any changes elsewhere in the codebase.

\subsubsection{Modifying RelaxIV}
This program was considerably more challenging to modify. Whereas my implementations attempted to achieve both flexibility and a reasonable level of efficiency, performance was the overriding objective for RelaxIV. The original version of the code is due to Bertsekas, the creator of the relaxation algorithm, and dates to 1988~\cite{BertsekasCodes:1988}. Frangioni and Gentile made the heroic effort of porting this original Fortran implementation to C++ in 2003~\cite{RelaxIV:2011}. Whilst this port made integration with the rest of my project somewhat easier, many of the foibles of the original code remain, and some new ones have been introduced.

The implementation already supported cost reoptimisation, presumably added to the original implementation by Frangioni for his computational study on this topic~\cite{Frangioni:2006}. Methods to make other updates to the network were also provided, but do not appear to have ever been completed. The bulk of the development time on RelaxIV was spent fixing bugs such as these. In its entirety, the implementation of RelaxIV is some 5000 lines, with some functions longer than 800 lines. Consequently, even simple bugs were sometimes difficult to track down\footnotemark.
\footnotetext{Valgrind was extremely helpful in detecting the large number of off-by-one errors present in the code. These appear to date to a dubious design decision to make whether to start at index 0 or 1 configurable by means of a preprocessor definition. I am skeptical as to whether saving a few bytes in memory was ever worth this increased complexity, but it is certainly unfortunate that this change was applied inconsistently throughout the code.}

The rest of the changes made were comparatively straightforward. RelaxIV allocated memory for data structures statically, which was problematic for an incremental solver as the size of the network is not known in advance. This was resolved by allowing the arrays to grow dynamically, using the usual strategy of exponential growth in size to achieve memory allocation in amortized linear time\footnotemark.
\footnotetext{Spikes in scheduling latency are undesirable, even if the average latency remains low. Since we are not memory bound, my implementation initially allocates twice as much memory as currently needed, in a bid to avoid needing to ever grow the array.}

In a related vein, RelaxIV did not allow the reuse of node identifiers. This would cause a memory leak over time, as we would have to keep growing the size of the data structures as nodes are added even if the total number of live nodes remained constant. I maintain a set of free nodes, with an identifier allocated from this set as necessary. The size of the data structure is only grown when no free nodes remain.

Each arc $(i,j)$ is identified by an index in RelaxIV. Of course, the flow scheduling system is not privy to this internal index, and so can only identify arcs by their source and destination nodes $i$ and $j$. The only way to find the index is to scan the adjacency lists for $i$ and $j$, an $O(n)$ operation. My implementation makes a time-memory tradeoff, maintaining a hash table mapping pairs $(i,j)$ to indices, providing an $O(1)$ lookup\footnotemark.
\footnotetext{Empirically the algorithm is not memory bound, and in any case this change will leave the asymptotic space complexity unchanged. So trading off increased memory usage for lower computation time is the right decision.}

%\section{Input and output}
%
%% Drop this section if short of space, or punt to appendix. Optimisations probably the most useful/interesting ones to talk about.
%
%Loading networks, exporting results. Neither interesting nor impressive, but it is a necessary part of the project. Keep it brief. (Or cut it entirely?)
%
%\subsection{DIMACS}
%
%Representation for network, and network solution. Justify why: standard, widely used, readily available test data, integration with Firmament. 
%
%\subsection{Incremental extension}
%
%Why DIMACS isn't suitable for incremental problem: reloading the graph and computing diff prohibitive cost. 
%
%\subsection{Parser optimisations}
%
%Ignore zero-capacity arcs. Entirely trivial, but did produce noticeable performance improvement. Maybe worth mentioning.
%
%Ignoring duplicates. Necessary for correctness. Slightly more interesting: checking whether an arc is present is slow in some data structures (where the corresponding algorithm doesn't require manual lookup). Parser maintains a bitmap instead to check this quickly irrespective of data structure. Considerable performance improvement. 
%
%\subsection{Scheduling tasks}
%
%Show how solver can achieve original task, by integrating into a cluster scheduler. Choice of Firmament: can be briefly justified, but need not be at all (no real alternatives).

\section{Benchmark suite} \label{sec:impl-benchmark}

% TODO: Maybe include modifications to Firmament in this or a separate section

Motivation. Automate common task. Improve quality of data collection: can run experiment many times with different parameters, no possibility of human error. 

\subsection{Architecture}

Specify different implementations by Git 'treeish', and path. Will checkout and build automatically. 

Test cases specify implementations required, I/O parameters, number of iterations.

Output CSV file with statistics.

\subsection{Testing on full networks}

Non-incremental case.

\subsection{Testing on incremental changes}

Can test two incremental versions head-to-head. Alternately, can test an incremental version compared to applying the diff and running a full solver from scratch.