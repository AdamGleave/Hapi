\chapter{Introduction} \label{chap:intro}

% TBC: Paragraph summarizing project, success

\section{Motivation} \label{sec:intro-motivation}
Clusters of commodity machines have become the dominant platform for high-throughput computing. With the adoption of cloud computing, increasing number of applications must be designed to run across clusters rather than individual machines. Making efficient use of these \emph{warehouse-scale computers} is a major challenge in distributed systems research, with considerable practical implications~\cite{WarehouseScale:2009}.

A cluster scheduler chooses the tasks to run on each machine, coordinating the activities of the cluster. The choice of scheduler has considerable ramifications on cluster performance and efficiency. Despite their importance, most schedulers leave much to be desired. In particular, they have limited flexibility, being unable to adapt to differing cluster designs and application requirements.

The Quincy system was developed at Microsoft Research to address these problems~\cite{Isard:2009}. In a striking departure from traditional designs, Quincy represents the cluster and its tasks as a flow network. A solution sending flow from task nodes to compute nodes corresponds to a schedule mapping tasks to machines. Solving the minimum-cost flow problem finds a schedule minimising resource usage in the cluster.

By modelling resources in the cluster explicitly, Quincy naturally adapts to the idiosyncrasies of different hardware. Furthermore, the scheme is highly flexible. Whereas most schedulers have a particular policy hard-coded, in Quincy it is defined by a cost model: a procedure assigning a cost to each arc in the flow network. This allows the system to be easily tuned to particular application requirements.

Cluster throughput under the Quincy system increased by  40\%~\cite{Isard:2009} in early experiments, demonstrating the power of this so-called "flow scheduling" approach. Paradoxically, whilst the flow network representation is critical to realising these benefits, it is also the greatest drawback to the system. The minimum-cost flow problem must be solved every time a new schedule is produced, which is extremely computationally expensive.

The resulting scheduling latency is prohibitive for many applications. Even in situations where the latency is tolerable, there are concerns as to the scalability of the technique. Quincy was originally tested on a cluster of a few hundred machines. The warehouse-scale computers of today may contain tens or hundreds of thousands of machines, with the size of clusters continuing to grow. By contrast, the scalar performance of processors is believed to have mostly peaked, and flow algorithms have limited parallelism.

In this dissertation, I explore approaches to improve the performance of flow algorithms on networks produced by Quincy-style systems. My goal is to enable flow scheduling systems to scale to the largest clusters built today, as well as to still larger clusters which may be built in the foreseeable future. Moreover, I intend to reduce the scheduling latency to allow the system to be used with applications that have hard latency requirements. Together, this will enable flow scheduling systems to be adopted in practice, with the consequent performance and efficiency gains in data centres.

\section{Challenges} \label{sec:intro-challenges}
Research into the minimum-cost flow problem has been active for over 60 years. There is consequently considerable prior work on this problem, which I describe further in~\S\ref{sec:intro-related-work}. It will be necessary for me to assimilate this large body of existing material before I can attempt to improve upon it.

Given that many seasoned researchers have spent their careers working on this problem, realising a significant performance improvement will be difficult. Not only has there been considerable work to develop efficient algorithms, the reference implementations for these algorithms have been extensively optimised.

Whilst the task ahead is daunting, the reward is commensurate with the risk. Success will enable a new generation of schedulers, able to address the challenges facing today's major technology companies.

\section{Related work} \label{sec:intro-related-work}

Research into flow networks has been ongoing since the 1940s, driven by their numerous practical applications. The area remains active, with new algorithms and implementation techniques being devised even today.

Study of the area begin with the transportation problem, a special case of the minimum-cost flow problem. Kantorovich was the first to formulate the problem in 1939~\cite{Kantorovich:1960}, although his work did not receive recognition outside Russia until sometime later. Hitchcock was the first to study this problem in the Western world, publishing in 1941~\cite{Hitchcock:1941}. Koopmans followed in 1949, demonstrating the theory applied in an economic context~\cite{Koopmans:1949}. Kantorovich and Koopmans would later receive a Nobel prize for their research\footnotemark.
\footnotetext{Hitchcock missed out on the prize as it was awarded in 1975, after he had passed away.}

This early work motivated the development of linear programming, which flow problems can be shown to reduce to. Indeed, the first statement of the general linear programming problem is due to Kantorovich~\cite{Kantorovich:1960}. Linear programming only became an established field with the publication in 1949 of Dantzig's seminal work on the now well-known simplex algorithm~\cite{Dantzig:1949}. One of the earliest applications of this method was to flow networks, with Dantzig specialising the simplex algorithm to the transportation problem in 1951~\cite{Dantzig:1951}.

Growing interest in linear programming spurred study into flow networks. During the 1950s, researchers explored the minimum-cost flow problem and its specializations, such as the maximum-flow problem. By the end of the decade, there were specialist algorithms to solving these problems. Ford and Fulkerson developed a number of primal-dual combinatorial algorithms, whereas Dantzig continued his focus on simplex methods~\cite{FordFulkerson:1962,Dantzig:1962}.

Given this early excitement, the field might be expected to have peaked soon after. However, if anything the pace of innovation has accelerated in recent years, with modern algorithms offering considerable performance gains. Unfortunately, it is not practical for me to discuss all the worthy work which has taken place over the last 60 years. For the rest of this section, I will focus on the algorithms available today which offer the highest performance.

\subsection{Network simplex}

The network simplex algorithm can claim to be the oldest flow solution methods still in use today. Indeed, the simplex approach of Dantzig~\cite{Dantzig:1949} was one of the first approaches used in the solution of flow problems. Specialised versions, known as \emph{network simplex} algorithms, were soon developed, offering considerable performance gains over the generic simplex approach~\cite{Dantzig:1962}.

Significant improvements have been made to the algorithm over time, although the underlying technique remains the same. The generic network simplex algorithm is not guaranteed to run in polynomial time\footnotemark, however many variants have been devised with a polynomial bound~\cite{Tarjan:1991,Goldfarb:1992}. Apart from algorithmic improvements, there has also been considerable work to develop efficient implementations~\cite{Lobel:1996,Grigoriadis:1986}.
\footnotetext{Although in practice it is typically rather efficient.}

\subsection{Successive shortest path}

This algorithm was invented independently by several authors~\cite{Jewell:1958,Iri:1960,BusackerGowen:1960}. Edmonds and Karp~\cite{Edmonds:1972} and Tomizawa~\cite{Tomizawa:1971} independently suggested a technique to maintain non-negative arc costs during the algorithm; this allows for more efficient shortest path computations, considerably improving its performance. The variant given by Edmonds and Karp is notable for having been the first solution method for the minimum-cost flow problem to run in (weakly) polynomial\footnotemark time.

\footnotetext{In a weakly polynomial algorithm, the maximum cost and capacity of arcs may feature in the polynomial bound. By contrast, for a (strongly) polynomial algorithm the bound is a function only of the dimensions of the problem: the number of vertices and arcs.}

Whilst successive shortest path is of significant historical interest, its performance is inferior to those of more modern approaches. The relaxation algorithm is an approach of more recent vintage, developed in 1987 by Bertsekas and Tseng~\cite{BertsekasMethod:1988,BertsekasCodes:1988}. Its approach is inspired by Lagrangian relaxation, a technique used for solving integer programming problems. Perhaps surprisingly, it can be shown to be a special case of the successive shortest-path algorithm.

The worst-case runtime complexity of the relaxation algorithm, whilst (weakly) polynomial, is considerably slower than many algorithms considered in this dissertation. But in practice, the algorithm is highly efficient on many problem instances, with computational benchmarks having found it to be the fastest solver for some types of network~\cite{KiralyKovacs:2012}.

\subsection{Cycle cancelling}

% Slightly later. Classic approach. Has some nice connections with maximum-flow.

\subsection{Cost scaling}

% More modern approach, first algorithms only developed in 1980. Some of the best theoretical and practical performance.

\subsection{Comparative evaluation}

% Reference Kiraly & Kovacs