\chapter{Introduction} \label{chap:intro}

% TBC: Paragraph summarizing project, success

\section{Motivation} \label{sec:intro-motivation}
Clusters of commodity machines have become the dominant platform for high-throughput computing. With the adoption of cloud computing, increasing number of applications must be designed to run across clusters rather than individual machines. Making efficient use of these \emph{warehouse-scale computers} is a major challenge in distributed systems research, with considerable practical implications~\cite{WarehouseScale:2009}.

A distributed scheduler coordinates the cluster, choosing the tasks to run on each machine. This has considerable ramifications on cluster performance and efficiency. For example, throughput is significantly improved by taking into account \emph{data locality}: scheduling tasks on machines close to where their input data is stored. 

Despite their importance, most schedulers leave much to be desired. I would identify two major drawbacks in widely-used schedulers. The first is failing to adapt to different hardware. Some clusters have considerably higher bandwidth interconnects than others. Data locality will be more important in the latter case than the former. The second issue is a lack of policy flexibility. Every application has a unique set of requirements: it should be possible to specify a weighted set of goals to optimize for.

Hadoop, by far the most widely used distributed systems framework, is a good example of these problems. The Hadoop Fair Scheduler (HFS), as the name suggests, has excellent fairness properties: different users will receive equal shares of the cluster's resources. However, it makes no attempt to achieve data locality. Research systems exist that can provide both~\cite{Zaharia:2010}. However, we would struggle to find a system which could support a third objective, highlighting policy inflexibility. And apart from rudimentary knowledge as to the total number of computational nodes, the HFS runs entirely blind to the underlying hardware.

The Quincy system, developed at Microsoft Research, was designed to address these problems~\cite{Isard:2009}. Quincy builds an explicit model of the cluster, incorporating knowledge about the underlying computational and network hardware. The cluster and the tasks to be scheduled are then represented as a flow network. Solutions to the flow problem correspond to schedules for the tasks.

By modelling the cluster explicitly, Quincy naturally takes into account the idiosyncrasies of different hardware, resolving the first problem. Quincy is also exceedingly flexible, as it does not assume any particular policy. Instead, the policy is defined by a cost model: a procedure which assigns a cost to each arc in the flow network. This allows for a wide range of policies to be expressed.

Whilst this so-called "flow scheduling" approach has proven extremely versatile, there is one key weakness. It is extremely computationally expensive to solve the minimum-cost flow problem, needed to produce a schedule. The resulting scheduling latency is prohibitive for many applications. Even in situations where the latency is currently tolerable, there are concerns as to the scalability of the technique. Clusters look set to continue to grow in size. By contrast, the scalar performance of processors is believed to have mostly peaked, and flow algorithms are difficult to parallelise.

In this dissertation, I explore techniques to improve the performance of flow algorithms on networks produced by Quincy-style systems. Significant speed-up will enable flow scheduling systems to be adopted in practice, with the consequent performance and efficiency gains in computer clusters.

\section{Challenges} \label{sec:intro-challenges}
Research into the minimum-cost flow problem has been active since the 1950s, driven by its numerous practical applications. There is consequently considerable prior work from the past 60 years on this problem, which I describe further in \vref{sec:intro-related-work}. It will be necessary for me to assimilate this large body of existing material before I can attempt to improve upon it.

Given that many seasoned researchers have spent their careers working on this problem, realising a significant performance improvement will be difficult. Not only has there been considerable work to develop efficient algorithms, the reference implementations for these algorithms have been extensively optimised.

Whilst the task ahead is daunting, the reward is commensurate with the risk. Success will enable a new generation of schedulers, able to address the challenges facing today's major technology companies.

\section{Related work} \label{sec:intro-related-work}
% TBC
Brief literature survey.