\chapter{Introduction} \label{chap:intro}

\section{Motivation} \label{sec:intro-motivation}
Clusters of commodity machines have become the dominant approach for high-throughput computing. With the adoption of cloud computing, increasing number of applications must be designed to run across clusters rather than individual machines. Making efficient use of these \emph{warehouse-scale computers} is a major challenge in distributed systems research, with considerable practical implications~\cite{WarehouseScale:2009}.

The scheduler coordinates the cluster, choosing the tasks to run on each machine. This can have considerable ramifications on cluster performance and efficiency. For example, throughput is significantly improved by taking into account \emph{data locality}: scheduling tasks on machines close to where their input data is stored. 

Despite their importance, widely-used schedulers leave much to be desired. I would identify two major drawbacks. The first is failing to adapt to different hardware. Some clusters have considerably higher bandwidth interconnects than others. Data locality will be more important in the latter case than the former. The second issue is a lack of policy flexibility. Every application has a unique set of requirements: it should be possible to specify a weighted set of goals to optimize for.

Hadoop, by far the most widely used distributed systems framework, is an excellent example. The Hadoop Fair Scheduler (HFS), as the name suggests, has excellent fairness properties: different users will receive equal shares of the cluster's resources. However, it makes no attempt to achieve data locality. Research systems exist that can provide both~\cite{Zaharia:2010}. However, we would struggle to find a system which could support a third objective, highlighting policy inflexibility. And apart from rudimentary knowledge as to the total number of computational nodes, the HFS runs entirely blind to the underlying hardware.

The Quincy system, developed at Microsoft Research, was designed to address these problems~\cite{Isard:2009}. Quincy builds an explicit model of the data centre, incorporating knowledge about the underlying computational and network hardware. The data centre and the tasks to be scheduled are then represented as a flow network. Solutions to the flow problem correspond to schedules for the tasks.

By modelling the data centre explicitly, Quincy naturally takes into account the idiosyncrasies of different data centres, addressing the first problem. Quincy is also exceedingly flexible, as it does not assume any particular policy. Instead, a cost model must be specified, a procedure to assign a cost to each arc in the network. This allows the policy to be tweaked to achieve a variety of different objectives.

Whilst this so-called "flow scheduling" approach has proven extremely versatile, it has one key weakness. Solving the minimum-cost flow problem to produce an optimal schedule is computationally extremely expensive. This makes the scheduling latency prohibitive for many applications. Even where the latency is tolerable currently, there are concerns about the scalability of the technique. Clusters look set to continue to grow in size. By contrast, the scalar performance of processors is believed to have mostly peaked, and flow algorithms are difficult to parallelise.

In this dissertation, I explore techniques to improve the performance of flow solving algorithms on networks produced by Quincy-style systems. Significant speed-up will enable flow scheduling systems to be adopted in practice, with the consequent performance and efficiency gains in computer clusters.

\section{Challenges} \label{sec:intro-challenges}
Research into the minimum-cost flow problem has been active since the 1950s, owing to its numerous practical applications. There is therefore a considerable body of existing work on the problem, described further in \vref{sec:intro-related-work}. It will be necessary for me to assimilate this large body of existing material before I can attempt to improve upon it.

Given that many seasoned researchers have spent their careers working on this problem, it will be difficult to achieve significant performance improvements. Not only has there been considerable work in developing efficient algorithms, the reference implementations for these algorithms have been extensively hand-optimised.

Whilst the task ahead is daunting, success will enable a new generation of schedulers, able to address the challenges which face today's major technology companies.

\section{Related work} \label{sec:intro-related-work}
Brief literature survey.