\chapter{Preparation} \label{chap:prep}

% PROOFREAD: None/Once

\section{Cluster scheduling} \label{sec:prep-scheduling}

% PROOFREAD: 1. Major edits. 2. Minor edits.

% TODO: Get some references from Malte/Ionel on this?
Much as an operating system schedulers maps threads to processor cores, a cluster scheduler maps tasks to machines. Better scheduling techniques can considerably improve performance: for example, throughput increased by 40\% under Quincy ~\cite{Isard:2009}. Due to their important, cluster schedulers remain an active area of research, both within academia and in industry.

I will start by outlining desirable properties for cluster schedulers. Following this, I will summarise the prevailing approaches to scheduling. To conclude, I will discuss the principles underlying Firmament, and show how it improves upon competing approaches with its use of flow scheduling.

\subsection{Design goals} \label{sec:prep-scheduling-goals}

% PROOFREAD: 1. Major edits. 2. Minor edits.

The cluster scheduler faces a challenging task. The scheduler policy must be targeted to meet the specific needs of the applications it is scheduling. To effectively implement a policy, the scheduler may need to process a wide variety of factors, such as the load at each machine and the resource requirements of every task. This should all take place as fast as possible, to support interactive applications. Furthermore, the system must be able to scale to the size of modern clusters. These goals are summarised in \cref{table:cluster-scheduler-design-goals}.

\begin{table}
    \begin{tabular}{c|l}
        Goal & Description\tabularnewline
        \hline
        Targeted & Application requirements are reflected in the scheduler policy. \tabularnewline
        Effective & The schedules produced are optimal given the policy objectives. \tabularnewline
        Fast and scalable & Scheduling latency remains low even for large clusters. \tabularnewline
    \end{tabular}
    \caption{Design goals for cluster schedulers}
    \label{table:cluster-scheduler-design-goals}
\end{table}

Scheduler policies may have a variety of objectives. The traditional use case of clusters is batch jobs, where the goal is to maximise throughput. Increasingly, clusters run interactive jobs, where low latency is critical. Modern clusters are often multi-tenant: it is important to ensure \emph{fairness}, an equitable distribution of resources between users. Naturally, objectives often conflict with each other: each application must choose a particular trade-off. It is essential, then, that scheduler policy can be targeted to meet the needs of different applications.

Getting the objectives right can be hard enough. Implementing a scheduler which effectively achieves those objectives is more difficult still. Particularly challenging is the sheer amount of information that must be processed. As a simple example, consider the Whare-Map scheduler. It seeks to minimise \emph{co-location interference}: the degradation of performance occurring when tasks running on the same machine compete for shared resources~\cite{Mars:2013}\footnotemark. To achieve this objective, it must monitor the performance of every task running in the cluster, building up a profile of how each application interacts with others.
\footnotetext{In fact the approach is more general than this: Whare-Map tries to take into account machine heterogeneity due to hardware differences as well.}

% SOMEDAY: Terminology re: latency is a bit confused. Here you're talking about scheduling latency to do with the solver being slow. But in the next section you talk about latency due to policy: a FIFO queue.
Finally, the system should remain fast at scale. Today's warehouse scale computer may contain tens of thousands of machines, running hundreds of thousands of tasks\footnotemark. Furthermore, these platforms now often run interactive applications: for example, a job may be spawned in response to a query in a data analytics platform. These applications may demand sub-second or lower scheduling latency~\cite{Ousterhout:2013}.
\footnotetext{At full utilisation, there will certainly be as many tasks on each machine as there are cores, perhaps more. There may also be tasks in the system which remain unscheduled.}

\subsection{Prevailing approaches}

% PROOFREAD: 1. Minor edits.

Apache Hadoop is by far the most widely used distributed storage and processing framework. Originally developed at Yahoo, it has since been adopted by many other firms, including Amazon, Facebook and LinkedIn~\cite{HadoopPoweredBy}. Representing the state of the art used in industry, it is instructive to consider the scheduling techniques it uses.

Until 2008, Hadoop supported only one scheduler~\cite{HadoopFairSchedulerJIRA}: a FIFO queue, executing whichever job had waited longest. This offers high throughput, and so is attractive for batch jobs, Hadoop's traditional application. However, it fares poorly with other applications. In particular, the latency can be very high: a job (no matter how small) must wait in line before all previously submitted tasks. In addition, their is no notion of fairness: a heavy user of the cluster can starve all other users.

Hadoop's developers introduced a pluggable scheduler framework to resolve these shortcomings~\cite{HadoopSchedulingIBM}. In the terminology of \cref{table:cluster-scheduler-design-goals}, by allowing users to write their own scheduler the policy can be precisely \emph{targeted} to specific application needs. Although this approach is powerful, to some extent it merely shifts the problem elsewhere. It is not feasible for each application to be bundled with its own scheduler\footnotemark. The vast majority of installations use one of the schedulers distributed with Hadoop: the Fair Scheduler or Capacity Scheduler.
\footnotetext{Clusters typically run many applications simultaneously, but Hadoop only supports one scheduler per cluster, so this would not be a viable approach anyway.}

The Fair Scheduler was designed at Facebook, guaranteeing each job an equal share of available resources. It pioneered a technique known as delay-scheduling, which allows the cluster administrator to sacrifice some fairness in order to achieve greater data locality~\cite{Zaharia:2010}. 

The Capacity Scheduler was designed at Yahoo, in order to support distinct application classes on the same cluster. Hard resource guarantees can be provided to each application class, ensuring a minimum service level. Applications are allowed to burst beyond this level when there are cluster resources idle.

Note both schedulers are designed with specific purposes in mind. Although effective, offering good performance for their stated objectives, the schedulers cannot be targeted to particular applications: there is limited policy flexibility.

There are also some questions as to how effective these schedulers truly are. Cluster environments are increasingly heterogeneous~\cite{Reiss:2012}, posing problems for traditional schedulers which run blind to the underlying hardware. The LATE scheduler, designed to be robust to heterogeneity, halved response times compared to Hadoop's default scheduler when running on Amazon's Elastic Compute Cloud~\cite{Zaharia:2008}.

Hitherto, I have considered only the public state of the art. With today's major web companies dependent upon clusters, there is naturally considerable research into this area by industry. Unfortunately, the details of the schedulers developed by major technology firms such as Google are trade secrets.

However, the information which is public suggests that the basic approach is the same. The designs have, of course, been tweaked to meet specific application requirements, and to operate efficiently on the cluster architectures favoured by the firm. For example, Google's scheduler lets tasks specify a kernel version constraint, and seeks to minimise peak power demand by load balancing~\cite[\S2.1]{Sharma:2011}.

\subsection{Firmament and flow scheduling}

% PROOFREAD: 1. Major edits. Need to reread.

\begin{table}
\begin{tabular}{c|c|c|c}
    Scheduler & Targeted & Effective & Fast and scalable \tabularnewline
    \hline
    Hadoop FIFO Scheduler & 1 & 3 & 4 \tabularnewline
    Hadoop Fair Scheduler & 2 & 4 & 3 \tabularnewline 
    Hadoop Capacity Scheduler & 2 & 4 & 3 \tabularnewline 
    LATE & 1 & 5 & 3 \tabularnewline 
    Sparrow & 2 & 3 & 5 \tabularnewline 
    \hline
    Quincy & 2 & 5 & 1 \tabularnewline 
    Firmament & 4 & 5 & 1 \tabularnewline 
\end{tabular}
\caption{Scheduler feature matrix}
\label{table:cluster-scheduler-feature-matrix}
\end{table}
\todo{Graphical representation of the numerical ratings} % TODO

Schedulers have traditionally relied on \textit{ad hoc} heuristics to implement a hard-coded policy. This approach has yielded impressive results, however there are signs it may be reaching its limits.

\Cref{table:cluster-scheduler-feature-matrix} summarises the results of the previous section, and previews this section. It is apparent that the key drawback of the heuristic approach is an inability to \emph{target} the scheduling policy. This has led to a proliferation of different schedulers, but it will never be possible to meet all application needs in this way.

There has been some attempt to make heuristic schedulers more general: both the Hadoop Fair Scheduler and Capacity Scheduler are considerably more configurable than their predecessors. Only a small degree of flexibility is afforded by this, however. 

For example, the Fair Scheduler allows the trade-off between data locality and fairness to be specified. But it is rare that these are the only two objectives of importance. Low scheduling latency is important to many applications. This conflicts with the other two objectives: for example, data locality is boosted by keeping a task unscheduled until a compute node close to the data becomes free. There is no way to adjust this trade-off using the Fair Scheduler.

The Firmament\footnotemark cluster scheduler, by contrast, is specifically designed to be flexible. Rather than using a heuristic approach, Firmament models scheduling as an optimisation problem over \emph{flow networks}. The details of this are described in \S\ref{sec:prep-flow-scheduling}, but the key point is that the scheduling policy is encoded in terms of a \emph{cost model} which assigns costs to arcs in the network. Defining a new cost model is relatively straightforward, making it easy to implement a wide variety of policies.
\footnotetext{Firmament has been developed by the supervisors for this project, Malte Schwarzkopf and Ionel Gog, as part of the Cambridge Systems at Scale initiative.}

It would be an overstatement to say that all policies can be expressed in this way: flow networks are not Turing complete. Indeed, there is a concrete counterexample of this. In gang scheduling, a constraint is added that pairs of tasks must be scheduled to run simultaneously. This is difficult or impossible to express in this model: flows in a network are inherently independent. However, cases such as these are the exception that proves the rule. In practice, the model is very powerful, allowing a range of scheduling policies to encoded in terms of cost models. 

This flow scheduling approach may also be more \emph{effective} than heuristic designs. The Quincy system, which pioneered flow scheduling, sought to achieve \emph{data locality}. Throughput under Quincy was 40\% greater than traditional designs; moreover, data transfer was reduced by up to a factor of 3.9, potentially allowing for cost savings when building cluster networks~\cite{Isard:2009}. This was possible due to the flow network representation, allowing for fine-grained resource usage to be easily encoded. 

A more contemporary problem is that of scheduling in heterogeneous environments. There have been attempts to solve this problem using heuristic approaches. LATE, for example, identifies tasks which are under-performing and speculatively executes them on a faster node~\cite{Zaharia:2008}. This, however, is inefficient: the task has already spent time executing on a node, only to be preempted. Firmament encodes the resources available in the cluster explicitly in the flow network, to avoid placing resource-intensive tasks on machines which are already overloaded.

The key advantage of heuristic designs is that they tend to be computationally inexpensive. This allows them to scale to very large clusters. Moreover, for applications which require very low scheduling latencies, it is possible to speed up the scheduler by adopting simpler heuristics, or distributing the scheduling as in Sparrow~\cite{Ousterhout:2013}. The goal of this project is to improve the scheduling latency and scalability of flow based systems, to allow them to compete with heuristic schedulers. The next section provides background on the problem which must be solved.

\section{Flow networks} \label{prep-flow}

We must first formalise the notion of a flow network and its associated optimisation problems, before discussing the details of scheduling using flow networks. Following this, properties of flow networks useful for the analysis and design of flow algorithms will be summarised.

\subsection{Introduction}

% PROOFREAD: 1 minor edits.

\subsubsection{Definitions and notation}

A \emph{flow network} is a weakly connected\footnotemark directed graph $G=(V,E)$.
\footnotetext{A directed graph is weakly connected if the undirected graph formed by replacing all directed edges with undirected edges is itself connected.}

Each arc $(i,j)\in E$ has an associated \emph{capacity} (or \emph{upper	bound}\footnotemark) $u_{ij}$, and a \emph{cost} $c_{ij}$.
\footnotetext{Some authors include a lower bound $l_{ij}$, but this does not feature in flow scheduling. Moreover, any network featuring lower bounds can be transformed into an equivalent one without~\cite[p.~39]{Ahuja:1993}.}

Each node $i\in V$ has an associated supply/demand $b_{i}$. Node $i$ is said to be a \emph{supply node} if $b_{i}>0$, a \emph{demand	node} if $b_{i}<0$ and a \emph{transshipment node} if $b(i)=0$.

The problems we will consider involve finding a solution vector $\mathbf{x}$,
specifying the flow $x_{ij}$ at each arc $(i,j)\in E$. A solution
$\mathbf{x}$ is \emph{feasible}, and we say it is a \emph{flow}, if it satisfies capacity constraints at every arc $(i,j) \in E$:

\begin{equation} \label{eq:capacity-constraints}
0\leq x_{ij}\leq u_{ij}
\end{equation}

and mass balance constraints at each node $i \in V$:

\begin{equation} \label{eq:mass-balance-constraints}
\sum_{j\::\:(i,j)\in E}x_{ij}-\sum_{j\::\:(j,i)\in E}x_{ji}=b_i
\end{equation}

That is, the net flow out of the node is exactly equal to the supply
at that node (which may be negative, in the case of demand nodes).

\subsubsection{Assumptions}

\begin{assumption}[Integrality] \label{assumption:integrality}
All quantities defining the flow network take on only integer values.\\
\end{assumption}
    
\begin{remark} Note any network with rational quantities can be transformed into an equivalent integral network, by multiplying throughout by the greatest common denominator. Meanwhile, flow algorithms may not terminate on irrational data, so there is little point considering this case. Fortunately, the networks produced by systems such as Quincy are naturally integral, since quantities such as resource consumption in a data centre are inherently discrete. \\
\end{remark}

\begin{assumption}[Non-negative arc costs] \label{assumption:non-negative-arc-costs}
For all $(i,j) \in E$, $c_{ij} \geq 0$. \\
\end{assumption}

\begin{remark} 
This assumption is without loss of generality\footnote{Any network with negative arc costs can be transformed into an equivalent one with non-negative arc costs. For each arc $(i,j) \in E$ with a negative cost, replace the flow variable $x_{ij}$ with the variable $u_{ij} - x_{ji}$. Arc $(j,i)$ then replaces arc $(i,j)$, and $c_{ji} = -c_ij \geq 0$. \cite[p.~48]{Ahuja:1993}}. \\
\end{remark}

\begin{cor}All quantities defining arcs take on only values from the natural numbers.\footnotemark \todo{\quad\quad\quad Cut?}
\footnotetext{To avoid confusion, I will follow the convention $0\in\mathbb{N}$ throughout this document.}
\end{cor}
\begin{proof}
An arc is defined by its capacity and its cost. Capacities are non-negative by definition, and arc costs are non-negative by \cref{assumption:non-negative-arc-costs}. This together with \cref{assumption:integrality} guarantees that capacities and costs are natural numbers.
\end{proof}

\begin{remark}  
We can therefore exclusively use unsigned integer data types when representing arcs in computer code.
\end{remark}

\subsubsection{The minimum-cost flow problem} \label{sec:prep-flow-mcf}

The well-known maximum flow problem involves finding a solution vector
$\mathbf{x}$ subject to constraints \cref{eq:capacity-constraints} and \cref{eq:mass-balance-constraints}, i.e. finding a feasible flow.

Our focus will be on a generalization of this, known as the minimum-cost flow problem. Formally, it is:

\begin{equation} \label{eq:mcf-primal-problem}
\mbox{minimise}\ s(\mathbf{x})=\sum_{(i,j)\in E}c_{ij}x_{ij}
\end{equation}
subject to the constraint that $\mathbf{x}$ must be a feasible flow.

Note, in general, this and the maximum flow problem may be infeasible. For example, the network may be imbalanced: $\sum_{i\in V}b_{i}\neq0$. All networks produced by Firmament are guaranteed to be solvable, however, so I will tend not to discuss this case\footnotemark.
\footnotetext{For robustness, all algorithms implemented as part of this project detect if a network is infeasible.}

\subsection{Scheduling using flows} \label{sec:prep-flow-scheduling}

Firmament and Quincy frame scheduling in terms of the minimum-cost flow problem. \emph{Jobs} are submitted to the scheduler, each consisting of a number of \emph{tasks}. Flow drains from task nodes to a sink node. Along the way, it passes either through a machine node (indicating the task is to be placed there) or a special job unscheduled aggregator node (indicating the task is to be left unscheduled).

Arc capacities are used to \emph{restrict} the possible scheduling assignments. The costs associated with arcs specify \emph{preferences} between possible scheduling assignments. Scheduling policies are therefore encoded in terms of a cost model.

\subsubsection{Network structure}

% PROOFREAD: 1 major edits, need to reread.

\todo{Table summarising nodes}
\todo{Flow graphs}

Each task $i$ in job $j$ is represented by a node $\mathbf{T}_i^j$, with a single unit of supply. Machines are represented by nodes $\mathbf{M}_l$, with arcs to a sink node $\mathbf{S}$. Task nodes have arcs to machine nodes, with the arc cost specifying the task's preference for being scheduled on that machine.

There is a problem with this scheme: if there are more tasks submitted than can be scheduled, the flow problem becomes infeasible. To fix this, we introduce \emph{unscheduled aggregators} $\mathbf{U}^j$ for each job $j$, with an arc to $\mathbf{S}$. Each task node $\mathbf{T}_i^j$ has an arc to its unscheduled aggregator $\mathbf{U}_j$, with the cost specifying the penalty for being unscheduled.

In the model above, every task has an arc to each machine. Whilst this allows for the model to be highly precise, it is very inefficient\footnotemark.
\footnotetext{The number of arcs will scale linearly with the number of machines. But, of course, as the size of a cluster grows the number of tasks being scheduled will also tend to grow proportionally. So the number of arcs actually will tend to grow \emph{quadratically} with increases in the number of machines. Scalability is a key concern, making this is highly undesirable.}
Firmament adds support for \emph{equivalence classes}, which may be over tasks or machines. Each equivalence class has an aggregator node, with arcs to all nodes in the class. In this way, multiple arcs connecting to nodes in the same class can be combined into a single arc.
\footnotetext{In fact, Firmament supports representing resources at a finer grain than machines: for example, an individual core. So machines may themselves be aggregator nodes.}

The Quincy system provides a concrete example of an equivalence class. Each rack has an associated \emph{rack aggregator} node $\mathbf{R}_k$, with arcs to every machine in the rack. Moreover, a single \emph{cluster aggregator} $\mathbf{X}$ is introduced, with arcs to each rack aggregator. This scheme is appropriate for Quincy as its costs depend on inter-rack bandwidth usage, so will tend to be similar within a rack\footnote{Costs also depend on intra-rack bandwidth usage, but less weight is given to this factor as it is rarely a bottleneck.}. Rather than arcs from every task $\mathbf{T}_i^j$ to each machine, there is a single arc to $\mathbf{X}$ and a fixed number of arcs to preferred machines and racks.

\subsubsection{Capacities}

% PROOFREAD: 1 minor edits.
Every arc leaving a task node $\mathbf{T}_i^j$ has unit capacity. Note $\mathbf{T}_i^j$ has unit supply and no incoming arcs, so any (non-zero) capacity would be equivalent to unit capacity.

The arcs $\mathbf{M}_l \to \mathbf{S}$ from machine nodes to the sink node have capacity $K$, where $K$ is a parameter specifying the number of tasks which may concurrently run on a machine\footnotemark.
\footnotetext{In the original Quincy system, $K=1$. However, this is clearly a poor utilisation of modern multi-core machines. In fact, performance may often be improved by setting $K > \text{\# of cores}$, on simultaneous multi-threading (SMT) machines.}

The arcs $\mathbf{R}_k \to \mathbf{M}_l$ also have capacity $K$, since up to $K$ jobs may be scheduled on each machine in the rack via the aggregator. The arcs $\mathbf{X} \to \mathbf{R}_k$ have capacity $mK$, where $m$ is the number of machines in each rack\footnotemark.
\footnotetext{In fact, it would be perfectly legitimate to leave these arcs uncapacitated: the capacity on $\mathbf{M}_l \to \mathbf{S}$ will ensure there are not too many tasks scheduled. However, these capacity constraints may improve the efficiency of the solver.}

The arcs $\mathbf{U}^j \to \mathbf{S}$ can be set to be uncapacitated, allowing any number of tasks in job $j$ to be unscheduled. Firmament and Quincy, however, choose to enforce a lower bound $E_j$ and upper bound $F_j$ on the number of tasks that can be scheduled for each job. This can provide some degree of fairness, and in particular specifying $E_j \geq 1$ ensures starvation freedom.

Let $N_j$ be the number of tasks submitted for job $j$. To enforce the upper bound $F_j$, give $\mathbf{U}^j$ a demand of $N_j - F_j$. At least this many tasks in job $j$ must then be left unscheduled, guaranteeing no more than $F_j$ tasks are scheduled.

To enforce the lower bound, we set the capacity on $\mathbf{U}^j \to \mathbf{S}$ to $F_j - E_j$. The maximum number of unscheduled tasks is thus the $N_j - F_j$ which are absorbed at $\mathbf{U}^j$, plus the $F_j - E_j$ draining via $\mathbf{U}^j \to \mathbf{S}$. This gives an upper bound of $N_j - E_j$ unscheduled tasks, guaranteeing that at least $E_j$ tasks remain scheduled.

\subsubsection{Cost models}  

% PROOFREAD: 1 minor edits.

A key benefit of the flow scheduling approach is its flexibility: support for a policy can be added simply by defining a new cost model. The model used in the original Quincy system attempts to achieve \emph{data locality}: scheduling tasks close to their input data, in order to minimise network traffic. 

Firmament supports three other cost models~\cite[ch.~5]{Schwarzkopf:2015}. The Whare-Map and Coordinate Co-Location models aim to improve utilisation of resources in heterogeneous clusters. The Energy-Aware model seeks to achieve energy efficiency while meeting application performance guarantees, in clusters containing machines of varying performance and efficiency.

% SOMEDAY: Not happy with "having an intuition for how cost models operate is valuable", rephrase
Given such a variety of possible policies, the solver developed in this project must be independent of any particular cost model. However, having an intuition for how cost models operate is valuable. For reasons of brevity, I will only summarize the cost model in the original Quincy system. Further details of this model are available in~\cite{Isard:2009}. A description of the other models mentioned is in~\cite[ch.~5]{Schwarzkopf:2015}.

\begin{table}
    \begin{tabular}{c|c|l}
        \hline 
        Cost & Edge & Meaning\tabularnewline
        \hline 
        $\gamma_{i,m}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{M}_{m}$ & Cost of scheduling on machine $\mathbf{M}_{m}$.\tabularnewline
        $\rho_{i,l}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{R}_{l}$ & Cost of scheduling on worst machine in rack $\mathbf{R}_{l}$.\tabularnewline
        $\alpha_{i}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{X}$ & Cost of scheduling on worst machine in cluster.\tabularnewline
        $v_{i}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{U}^{j}$ & Cost of leaving task $\mathbf{T}_{i}^{j}$ unscheduled.\tabularnewline
        \hline 
    \end{tabular}
    \caption{Costs in the Quincy model}
    \label{table:quincy-costs}
\end{table}

\begin{table}
    \begin{tabular}{c|l}
        \hline 
        Variable & Meaning\tabularnewline
        \hline 
        $\chi^{X}\left(\mathbf{T}_{i}^{j}\right),\chi_{l}^{R}\left(\mathbf{T}_{i}^{j}\right),\chi_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ & Data transfer for task $\mathbf{T}_{i}^{j}$ across the core switch.\tabularnewline % if: scheduled on worst possible computer in cluster, scheduled on worst possible computer in rack $\mathbf{R}_{l}$ or scheduled on computer $\mathbf{M}_{m}$ respectively.
        $\mathcal{R}^{X}\left(\mathbf{T}_{i}^{j}\right),\mathcal{R}_{l}^{R}\left(\mathbf{T}_{i}^{j}\right),\mathcal{R}_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ & Data transfer for task $\mathbf{T}_{i}^{j}$ across the top-of-rack switch.\tabularnewline
        $\theta_{i}^{j}$ & Number of seconds task $\mathbf{T}_{i}^{j}$ has spent scheduled.\tabularnewline
        $\nu_{i}^{j}$ & Number of seconds task $\mathbf{T}_{i}^{j}$ has spent unscheduled.\tabularnewline
        \hline
    \end{tabular}
    \caption{Variables in the Quincy model}
    \label{table:quincy-variables}
\end{table}

\begin{table}
    \begin{tabular}{c|l}
        \hline 
        Parameter & Meaning\tabularnewline
        \hline 
        \hline 
        $\epsilon$ & Cost of transferring 1 GB across core switch\tabularnewline
        \hline 
        $\psi$ & Cost of transferring 1 GB across top of rack switch\tabularnewline
        \hline 
        $\omega$ & Wait-time cost factor for unscheduled aggregators\tabularnewline
        \hline 
    \end{tabular}
    \caption{Parameters for the Quincy model}
    \label{table:quincy-parameters}
\end{table}

\todo{Should I shift the Quincy stuff to appendix, maybe? It's important to understanding flow scheduling, but it's true that I don't assume any model.}
The arc costs specified by the Quincy model are given in \cref{table:quincy-costs}. All arcs not mentioned in this table have a cost of zero\footnotemark.
\footnotetext{In particular, arcs leaving aggregator nodes and arcs entering the sink have zero cost.}

The model assumes tasks process data residing in a distributed file system. Files may be accessed from any machine, with the file being fetched from a remote machines if there is no local copy. Retrieving a file from a machine in another rack is more expensive -- both in terms of request latency and network resources consumed -- than from a neighbouring machine. Reading the data from a directly connected disk is cheaper still. The Quincy scheduler quantifies the data transfer costs for each scheduling assignment. Solving the resulting minimum-cost flow problem yields an assignment which minimises the total data transfer cost, achieving data locality.

\Cref{table:quincy-variables} gives the variables maintained by the Quincy scheduler. All arc costs are functions of these variables. The system breaks down the data transfer of a task $\mathbf{T}_{i}^{j}$ into two components: transfer across the core switch, $\chi\left(\mathbf{T}_{i}^{j}\right)$, and top of rack switches, $\mathcal{R}\left(\mathbf{T}_{i}^{j}\right)$.

For a particular machine $\mathbf{M}_m$, the data transfer $\chi_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ and $\mathcal{R}_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ can be computed exactly. However, when it comes to determining the cost of arcs to aggregator nodes $\mathbf{R}_l$ or $\mathbf{X}$, an exact figure is not possible. Instead, Quincy takes a conservative upper bound: the \emph{greatest} data transfer that results from scheduling $\mathbf{T}_{i}^{j}$ on the \emph{worst} machine in rack $\mathbf{R}_l$ or the cluster as a whole, respectively.

The system also keeps track of the time $\theta_i^j$ and $\nu_i^j$ a task spends scheduled and unscheduled, respectively. If the task is stopped and then restarted, these times continue to accumulate. This property is needed to guarantee the scheduler makes progress.

Quincy is controlled by three simple parameters, given in \cref{table:quincy-parameters}. $\epsilon$ and $\psi$ specify the cost of data transfers across core and top of rack switches respectively. In general, we would expect $\epsilon > \psi$, since the core switch is more likely to become a bottleneck. $\omega$ controls the penalty for leaving a task unscheduled\footnotemark. Increasing $\epsilon$ and $\psi$ will cause the scheduler to more aggressively optimise for data locality. Increasing $\omega$ will 
\footnotetext{It is possible to vary $\omega$ between jobs in order to encode a notion of priority.}

It is now possible to state formulae for the arc costs listed in \cref{table:quincy-costs}. For convenience, we will define the data transfer cost function:
\[d_{b}^{A}\left(\mathbf{T}_{i}^{j}\right) = \epsilon\chi_{b}^{A}\left(\mathbf{T}_{i}^{j}\right)+\psi\mathcal{R}_{b}^{A}\left(\mathbf{T}_{i}^{j}\right)\]

The cost of scheduling on the worst machine in the cluster and the worst machine in rack $\mathbf{R}_l$ immediately follow from this:
\[\alpha_{i}^{j} = d^X\left(\mathbf{T}_{i}^{j}\right)\]
\[\rho_{i}^{j,l} = d^R_l\left(\mathbf{T}_{i}^{j}\right)\]
It is tempting to use the same form for the cost, $\gamma^j_{i,m}$ of scheduling on a particular machine $\mathbf{M}_m$. When $\mathbf{T}_{i}^{j}$ is not running on $\mathbf{M}_m$ (either it is unscheduled, or is running on a different machine), this is valid. 

However, this formula is inappropriate when $\mathbf{T}_{i}^{j}$ is already scheduled on  $\mathbf{M}_m$: $d^M_m\left(\mathbf{T}_{i}^{j}\right)$ will overestimate the cost of the remaining data transfer. To account for the work already invested in $\mathbf{T}_{i}^{j}$, we subtract the time it has been running, giving:
\[\gamma_{i,m}^{j} = d_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)-\begin{cases}
\theta_{i}^{j} & \text{if \ensuremath{\mathbf{T}_{i}^{j}}running on \ensuremath{\mathbf{M}_{m}}}\\
0 & \text{otherwise}
\end{cases}\]

The only remaining cost is $v_i^j$, the penalty associated with leaving a task $\mathbf{T}_{i}^{j}$ unscheduled. We set it proportional to the length of time it has been unscheduled:
\[v_i^j = \omega\nu_{n}^{j}\]

\subsubsection{Properties of these networks}

% PROOFREAD: 1 minor edits.

Although the costs assigned to arcs vary between policies, the structure of the network remains the same. Some simple properties are proved below, which will be useful when analysing the asymptotic complexity of algorithms.

\todo{Shift the first two proofs to appendix (but still state the claims here)?}
\begin{lemma} \label{lemma:network-num-nodes}
Let $n$ denote the number of nodes in the network. Then $n = \Theta\left(\text{\# machines} + \text{\# tasks}\right)$.
\end{lemma}
\begin{proof}
There is one node $\mathbf{T}_i^j$ for every task and one node $\mathbf{M}_m$ for every machine, so trivially $n = \Omega\left(\text{\# machines} + \text{\# tasks}\right)$.

It remains to show $n = O\left(\text{\# machines} + \text{\# tasks}\right)$. Consider each class of node in turn.

As stated above $\text{\# task nodes} = \text{\# tasks}$ and $\text{\# machine nodes} = \text{\# machines}$. There is an unscheduled aggregator $\mathbf{U}^j$ for each job $j$, and $\text{\# jobs} = O\left(\text{\# tasks}\right)$. There is at least one machine in every rack, so $\text{\# rack aggregator nodes} = O\left(\text{\# machines}\right)$. 

In addition, there is also a cluster aggregator node $\mathbf{X}$ and sink node $\mathbf{S}$ which are independent of the number of tasks and machines, contributing $O(1)$ nodes.

Thus:
\[n = O\left(\text{\# tasks} + \text{\# machines} + 1\right) = O\left(\text{\# tasks} + \text{\# machines}\right)\]

Hence:
\[n = \Theta\left(\text{\# machines} + \text{\# tasks}\right)\]
\end{proof}

\begin{lemma} \label{lemma:network-num-arcs}
Let $m$ denote the number of arcs in the network. Then $m = O(n)$. That is, the network is \emph{sparse}.
\end{lemma}
\begin{proof}
Consider the outgoing arcs from each class of node. Since every arc is outgoing from exactly one node, this will count every node exactly once.

Each machine $\mathbf{M}_l$ and unscheduled aggregator $\mathbf{U}^j$ has a single outgoing arc, to sink node $\mathbf{S}$. This contributes $O\left(\text{\# machines}\right)$ arcs. The sink node $\mathbf{S}$ has no outgoing arcs.

Rack aggregators $\mathbf{R}_l$ have outgoing arcs to each machine in their rack. Each machine is present in exactly one rack, so these contribute collectively $O\left(\text{\# machines}\right)$ arcs. The cluster aggregator $\mathbf{X}$ has outgoing arcs to each rack; since $O\left(\text{\# racks}\right) = O\left(\text{\# machines}\right)$, this contributes $O\left(\text{\# machines}\right)$ arcs.

It remains to consider task nodes $\mathbf{T}_i^j$. The number of arcs leaving the task node has a constant upper bound. The system computes a \emph{preference list} of machines and racks, and includes arcs only to those nodes (and the cluster aggregator $\mathbf{X}$). Thus this contributes $O\left(\text{\# tasks}\right)$ arcs.

Hence:
\[m = \left(\text{\# machines} + \text{\# tasks}\right)\]
By \cref{lemma:network-num-nodes}, it follows:
\[m = O(n)\]
\end{proof}

\begin{remark}
In fact we have $m = \Theta(n)$: the network is connected so certainly $m = \Omega(n)$. However, we will only use the bound $m = O(n)$.\\
\end{remark}

\begin{lemma} \label{lemma:network-supply}
The largest supply in the network is a unit supply.
\end{lemma}
\begin{proof}
The only nodes in the network with any supply are the task nodes, $\mathbf{T}_i^j$. These by definition have unit supply.
\end{proof}

\begin{remark}
Most nodes in the network are transshipment nodes, with no demand nor supply. The demand nodes in the network are the sink node $\mathbf{S}$ and (optionally) unscheduled aggregators, $\mathbf{U}^j$. These may have a greater than unit demand.
\end{remark}

\subsection{Further definitions and properties}

% PROOFREAD: 1 minor edits except for Residual Networks which was major

\subsubsection{Pseudoflows} \label{sec:prep-flow-pseudo}

% PROOFREAD: 1 minor edits

A \emph{pseudoflow} is a vector $\mathbf{x}$ which satisfies the capacity constraints
\cref{eq:capacity-constraints}, but which need not satisfy the mass balance constraints \cref{eq:mass-balance-constraints}. While all flow algorithms must return a feasible solution, many operate on pseudoflows in intermediate stages. 

The \emph{excess} at a node $i\in V$ is defined to be:

\begin{equation}
e_i=b_i+\sum_{\set{j | (j,i)\in E}}x_{ji}-\sum_{\set{j | (i,j)\in E}}x_{ij}
\end{equation}

Node i is said to be an \emph{excess node} if $e_{i}>0$, and a \emph{deficit node} if $e_{i}<0$ (with deficit $-e_{i}$). If $e_{i}=0$, $i$ is said to be \emph{balanced}. Note the mass balance constraints \cref{eq:mass-balance-constraints} hold if and only if all nodes are balanced.

\subsubsection{Residual networks}

% PROOFREAD: 1 major edits.

The notion of \emph{residual networks} is used by many flow algorithms. The residual network of $G$ is defined with respect to a (pseudo)flow $\mathbf{x}$, and is denoted by $G_{\mathbf{x}}$. Informally, it represents the actions an algorithm can take to modify the (pseudo)flow $\mathbf{x}$. In the case where $\mathbf{x=0},$ we have $G_{\mathbf{0}}=G$.

Formally, we define $G_{\mathbf{x}}=\left(V,E_{\mathbf{x}}\right)$ to be a directed graph where:
\begin{equation}
E_{\mathbf{x}}=\set{(i,j)\in V^{2} | (i,j)\in E\land x_{ij}<u_{ij}} \cup \set{(j,i)\in V^{2} | (i,j)\in E\land x_{ij}>0} 
\end{equation}

The former set contains \emph{forward arcs}, present in the original flow network. Arcs $(i,j)$ which are \emph{saturated}, i.e. $x_{ij}=u_{ij}$, are omitted from the residual network: they do not allow for any additional flow to be pushed. The second set consists of \emph{backward arcs}, the reverse of arcs in the original network. Only arcs with positive flow in the original network have a corresponding backwards arc. Pushing flow along the backward arc corresponds to cancelling flow on the forward arc.

The \emph{residual capacity} of an arc $(i,j)\in E_{\mathbf{x}}$ is defined to be:

\begin{equation}
r_{ij}=\begin{cases}
u_{ij}-x_{ij} & ,\:(i,j)\:\mbox{is a forward arc}\\
x_{ij} & ,\:(i,j)\:\mbox{is a reverse arc}
\end{cases}
\end{equation}

The cost of forward arcs is the same as in the original network, whereas the cost of a reverse arc $(j,i)$ is defined as $-c_{ij}$.

\subsubsection{Reduced cost and duality} \label{sec:prep-flow-rc-and-dual}

% PROOFREAD: 1 minor edits.

Each node $i\in V$ is associated with a \emph{potential}, $\pi_{i}$.\\

The \emph{reduced cost} of an arc $(i,j)\in E$ with respect to potential vector $\boldsymbol{\pi}$ is defined as:
\begin{equation} \label{eq:reduced-costs}
c_{ij}^{\boldsymbol{\pi}}=c_{ij}-\pi_{i}+\pi_{j}
\end{equation}

Every linear programming problem, referred to as a \emph{primal} problem, may be converted to a \emph{dual} problem. Solutions to the dual problem give an upper bound on the objective value of the primal problem.

The primal version of the minimum-cost flow problem is stated in \S\ref{sec:prep-flow-mcf}. Its corresponding dual problem is:

\begin{equation}
\mathrm{maximise}\; w(\boldsymbol{\pi})=\sum_{i\in V}b_{i}\pi_{i}-\sum_{(i,j)\in E}\max\left(0,-c_{ij}^{\pi}\right)u_{ij}
\end{equation}

with no constraints on $\boldsymbol{\pi}$.

Many flow algorithms seek to solve the dual problem, as this may be computationally more efficient. Others try and get the best of both worlds, operating on both the primal and dual version of the problem.

\subsubsection{Optimality conditions} \label{prep:flow-optimality}

% PROOFREAD: 1 minor edits.

Below, I give conditions on a solution vector $\mathbf{x}$ being optimal. These may suggest algorithms for solving the problem. Moreover, they can be used in testing, to verify that a solution is indeed optimal.\\

\begin{restatable}[Negative cycle optimality conditions]{thm}{optimalitynegcycle}
\label{thm:optimality-neg-cycle}
Let $\mathbf{x}$ be a (feasible) flow. It is an optimal solution to the minimum-cost flow problem if and only if the residual network $G_\mathbf{x}$ has no negative cost (directed) cycle.
\end{restatable}
\begin{proof}
See~\cite[p.~307]{Ahuja:1993}.
\end{proof}

\begin{thm}[Reduced cost optimality conditions] \label{thm:optimality-reduced-cost}
Let $\mathbf{x}$ be a (feasible) flow. It is an optimal solution to the minimum-cost flow problem if and only if there exists a node potential vector $\boldsymbol{\pi}$ such that that the reduced cost of each arc in the residual network $G_{\mathbf{x}}$ is non-negative:

\begin{equation} \label{eq:optimality-reduced-cost}
\forall(i,j)\in E_{\mathbf{x}}\cdot c_{ij}^{\boldsymbol{\pi}}\geq 0
\end{equation}
\end{thm}
\begin{proof}
See~\cite[p.~309]{Ahuja:1993}.
\end{proof}

\begin{thm}[Complementary slackness optimality conditions] \label{thm:optimality-complementary-slackness}
Let $\mathbf{x}$ be a (feasible) flow. It is an optimal solution to the minimum-cost flow problem if and only if there exists a node potential vector $\boldsymbol{\pi}$ such that for every arc $(i,j)\in E$:

\normalfont % don't want text in maths italic
\begin{align}
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}>0}, } & \text{then \ensuremath{x_{ij}=0}} \\
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}<0}, } & \text{then \ensuremath{x_{ij}=u_{ij}}} \\
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}=0}, } & \text{then \ensuremath{0\leq x_{ij}\leq  u_{ij}}}
\end{align}
\end{thm}
\begin{proof}
The result follows immediately from expanding out \cref{eq:optimality-reduced-cost}, applying the definition of a residual network and performing a case analysis. A detailed proof is given in~\cite[p.~310]{Ahuja:1993}.
\end{proof}

\subsubsection{Notation for complexity analysis} \label{sec:prep-flow-complexity}

% PROOFREAD: 1 minor edits.

Some notation will be useful for stating the asymptotic complexity of algorithms. 

Let $n=|V|$ and $m=|E|$ denote the number of nodes and arcs respectively.

The running time of algorithms depends not just on the size of the
network, but also on the magnitudes of input data. Let $U$ denote
the largest node supply/demand, or arc capacity:
\begin{equation}
U=\max\left(\max\left\{ |b_{i}|\::\: i\in V\right\} ,\max\left\{ u_{ij}\::\:\left(i,j\right)\in E\right\} \right)
\end{equation}

and let $C$ denote the largest arc cost:

\begin{equation}
C=\max\left\{ c_{ij}\::\:(i,j)\in E\right\} 
\end{equation}

\section{Software engineering} \label{sec:prep-sweng}

\subsection{Requirements analysis} \label{sec:prep-sweng-requirements}
What the system should do. Include table listing goals and their priorities, etc.

\subsection{Model}
State the model you used (something like Spiral probably closest fit). Elaborate on what it is. Justify choice: can reference constraints imposed by requirements, e.g. some of them risky/speculative so waterfall model inappropriate.

\subsection{Testing}
Summarize approach. Maybe make forwards reference to \ref{sec:prep-tools-testing}.

\section{Choice of tools} \label{sec:prep-tool-choice}
\subsection{Language and libraries}
Justify language choice (C++, Python). Libraries: GLOG, Boost.

Perhaps mention coding style guides?

\subsection{Development environment}
Physical machines used: laptop, MCS, SRG cluster.

Revision control: Git.

Backup strategy: GitHub. Daily snapshots backed up to MCS, SRCF, Copy (cloud).

CMake. IDE: Eclipse with plugins, vim.

\subsection{Testing environment} \label{sec:prep-tools-testing}
GTest for unit tests. Other automated tests: Python. Note that performance evaluation part of testing requires custom development: no pre-existing benchmark suites suitable.

\section{Initial experience}
My starting point. C++: from IB Tripos. Algorithms: IA/IB. Maths: IA. No prior knowledge of flow networks.

\section{Project schedule} \label{sec:prep-project-schedule}
How I planned my time. Split into phases. Core success criteria obtained early. Testing completed. Extensions added and additional experiments performed later on.