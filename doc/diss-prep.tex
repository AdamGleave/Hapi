\chapter{Preparation} \label{chap:prep}

% Proofread: None/Once

\section{Cluster scheduling} \label{sec:prep-scheduling}

Much as an operating system scheduler maps threads to processor cores, a cluster scheduler maps tasks to computational nodes. I will start by giving a brief overview of the requirements cluster schedulers must meet. Following this, I will summarise the prevailing approaches to scheduling. Finally, I will discuss the principles underlying the Quincy system, and how it improves upon competing schedulers.

\subsection{Design goals} \label{sec:prep-scheduling-goals}

The cluster scheduler is critical to performance. However, it faces a challenging task. There may be tens or hundreds of thousands of machines to manage, with perhaps many more tasks. Furthermore, there are a mind-boggling number of parameters which can impact the efficiency of a schedule. To name just a few, the fine-grained resource requirements (in terms of CPU, memory and IO), the location of its data and the characteristics of the underlying network all play a role. With the shift away from batch to more interactive applications on clusters, scheduling latency is increasingly also a concern~\cite{Ousterhout:2013}. The ideal scheduler therefore needs to place thousands of tasks, taking into account information from a variety of sources, within a sub-second time frame.

How can we judge whether a scheduler is effective? First we must define what we mean by performance. In batch applications, we care primarily about throughput. For interactive applications, low latency is essential. As is often the case, these two objectives are in conflict: a scheduler cannot optimise for both. Most applications will fall somewhere between these two extremes, and will perform best under a scheduler which achieves a compromise between the two objectives.

Once we know our objectives, it remains to consider what data is necessary to find an optimal solution. Clearly, we must take into account the task and its resource requirements. Equally important, but often neglected, is accounting for the resources present in the cluster. Different clusters may call for drastically different scheduling policies. For example, throughput can be significantly improved in many clusters by the scheduler taking into account \emph{data locality}: placing tasks on machines close to where their input data is stored. However, performance is only improved when IO is a bottleneck. In a cluster with a high-bandwidth specialised interconnect such as InfiniBand, performance might actually be harmed by scheduling with data locality in mind, as it may make less efficient use of other resources in the cluster which may be more scarce.

To summarize, for optimal performance the scheduler must be able to adapt itself to different hardware platforms and application requirements. Furthermore, ideally the scheduling latency would be kept low, in order to support interactive applications.

\subsection{Prevailing approaches} \label{sec:prep-scheduling-approaches}

Apache Hadoop is by far the most widely used open-source distributed systems framework. It was originally developed by Yahoo, where it continues to see active use, with their largest cluster having 36,000 cores. Hadoop is also used at Amazon, Facebook and LinkedIn~\cite{HadoopPoweredBy}. Representing the state of the art used in industry, it is instructive to consider the scheduling techniques it utilises. 

Until 2008, Hadoop supported only one scheduler~\cite{HadoopFairSchedulerJIRA}: a simple FIFO queue, executing whichever job had waited longest. This offers high throughput, and so is attractive for batch jobs, Hadoop's traditional application. However, it fares poorly in other applications. In particular, the latency can be very high, as a job (no matter how small) must wait in line between all previously submitted tasks. In addition, their is no notion of fairness: one heavy user of the cluster can starve other users of resources.

Hadoop has chosen to resolve these shortcomings by introducing a pluggable scheduler framework~\cite{HadoopSchedulingIBM}. This is a reflection of the wide variety of application requirements: new schedulers can be written to meet particular needs. The distribution includes two scheduling algorithms, in addition to the original FIFO queue: the Fair Scheduler and Capacity Scheduler. 

The Fair Scheduler was designed at Facebook. It ensures that, over time, each job gets an equal share of available resources. Moreover, it pioneered a technique known as delay-scheduling, allowing the cluster administrator to sacrifice some fairness in order to achieve greater data locality~\cite{Zaharia:2010}. 

The Capacity Scheduler was designed at Yahoo, with the goal of supporting distinct application classes on the same cluster. Hard resource guarantees can be provided to each application class to ensure a minimum service level. At the same time, applications are allowed to burst beyond this level when there are cluster resources idle.

Note both schedulers are designed with specific purposes in mind. Whilst offering good performance in the narrow domains they were designed for, there is little policy flexibility, limiting their applicability more generally. 

Another drawback is that the schedulers run mostly blind to the underlying hardware. The Fair Scheduler is slightly better in this regard, as it takes into account data locality. But this is not enough. Cluster environments are increasingly heterogeneous~\cite{Reiss:2012}, posing problems for traditional schedulers. The LATE scheduler, designed to be robust to heterogenity, improved response times by a factor of 2 over Hadoop's default scheduler when running on Amazon's Elastic Compute Cloud (EC2)~\cite{Zaharia:2008}. Even in more homogeneous environments, network topology and capacity can be highly important, as discussed in~\S\ref{sec:prep-scheduling-goals}.

Hitherto, I have considered the state of the art amongst schedulers that are in the public domain. Clusters are indispensable assets to today's major web companies, and consequently there is considerable research into this area in industry as well as academia. Owing to the competitive nature of the industry, unfortunately the details of the schedulers used by major technology firms such as Google are trade secrets. Naturally, this makes it difficult to evaluate the state of the art within industry. 

However, the information which is public suggests that the same basic approaches we have seen above are in use in industry. The designs may, of course, have been tweaked to meet specific application requirements, and to work efficiently on the cluster architectures favoured by the respective technology firm. For example, the Google scheduler allows tasks to specify a kernel version constraint, and attempts to minimise peak power demand by load balancing. Apart from idiosyncrasies such as these, the approach is broadly similar to published research systems~\cite[\S2.1]{Sharma:2011}.

\subsection{Quincy and related systems} \label{sec:prep-scheduling-quincy}

Despite considerable research effort, both in industry and academia, the traditional scheduler designs discussed above suffer from two major deficiencies. The first is a lack of policy flexibility. Hadoop attempts to solve this by allowing for an ecosystem of different schedulers, but this approach is limited. As we try to trade-off more objectives, there is a combinatorial explosion in the number of scheduler designs needed. The second problem is a failure to take the underlying cluster architecture into account when making scheduling decisions.

% Nice example from Malte's PhD thesis of three-way relationship between data locality, fair resource sharing and scheduling delay. Could use this or something similar to illustrate this?

Microsoft Research developed the Quincy system~\cite{Isard:2009} to address these problems. Working with the Dryad distributed execution engine~\cite{Isard:2007}\footnotemark, the Quincy scheduler builds a model of the cluster and the tasks. A flow network is generated from this model, with solving the flow problem dual to finding a schedule.
\footnotetext{Dryad allows tasks ("computational vertices") to be connected together by communication "channels" to form an (acyclic) dataflow graph. Other computational frameworks, such as MapReduce and relational algebra, reduce to special cases of Dryad.}

In the configuration described in the original paper, Quincy optimised for high levels of data locality and fairness. Its performance in this regard was admirable. Quincy's throughput was 40\% greater than that of standard queue-based approaches, such as those used by Hadoop. Moreover, data transfer was reduced by up to a factor of 3.9. This could allow for considerable cost savings when building out clusters, by reducing the interconnect capacity required.

% TODO: Any other papers that are relevant in this regard?
Since the paper introducing Quincy was published in 2009, new scheduling algorithms designed specifically with data locality and fairness in mind have been published~\cite{Guo:2012,Jin:2011,Ibrahim:2010}. Although no up-to-date empirical evaluation is available, my suspicion is Quincy would no longer lead the pack in this regard. The power of Quincy is not, however, in being yet another scheduler optimised for a narrow domain. Rather, it is that the system offers an extremely high level of flexibility.

As we saw above, most schedulers focus on one or two specific goals. There may be some degree of configurability: the Hadoop Fair Scheduler, for instance, allows the administrator to control the trade-off between fairness and data locality. Quincy, by contrast, is almost infinitely configurable. Indeed, the scheduler does not assume any specific policy. Instead, the policy is defined by a cost model: a procedure which assigns costs to arcs in the flow network.

It would be an overstatement to say that all policies can be expressed in this way: flow networks are not Turing complete. Indeed, there is a concrete counterexample of this. In gang scheduling\footnotemark, a constraint is added that pairs of tasks must be scheduled to run simultaneously. This is difficult or impossible to express in this model: flows in a network are inherently independent.
\footnotetext{Note gang scheduling does not benefit systems such as Dryad. It is most useful when there is bidirectional communication between the two tasks. This would correspond to a cycle in the dataflow graph, which is explicitly disallowed.}

However, cases such as these are the exception that proves the rule. In practice, the model is very powerful, allowing a range of scheduling policies to encoded in terms of cost models. Moreover, the policies naturally tend to adapt themselves to different cluster architectures, as the topology of the flow network will also adjust itself.

% TBC: Add reference to Firmament

\section{Flow networks} \label{prep-flow}

Before we discuss how Quincy encodes scheduling in terms of a flow network, we must pause to formalise the notion of a flow network and the associated flow problems, and establish the notation to be used throughout the rest of the dissertation. After this, I will summarise properties of flow networks which will be needed later when analysing and designing flow algorithms.

\subsection{Introduction}

\subsubsection{Definitions and notation}

A \emph{flow network} is a weakly connected\footnotemark directed graph $G=(V,E)$.
\footnotetext{A directed graph is weakly connected if the undirected graph formed by replacing all directed edges with undirected edges is itself connected.}

Each arc $(i,j)\in E$ has an associated \emph{capacity} (or \emph{upper	bound}\footnotemark) $u_{ij}$, and a \emph{cost} $c_{ij}$.
\footnotetext{Some authors also include a lower bound $l_{ij}$. This will not feature in flow scheduling. Moreover, any network featuring lower bounds can be transformed into an equivalent one where all lower bounds are zero. \cite[p.~39]{Ahuja:1993}}

Each node $i\in V$ has an associated supply/demand $b_{i}$. Node $i$ is said to be a \emph{supply node} if $b_{i}>0$, a \emph{demand	node} if $b_{i}<0$ and a \emph{transshipment node} if $b(i)=0$.

The problems we will consider involve finding a solution vector $\mathbf{x}$,
specifying the flow $x_{ij}$ at each arc $(i,j)\in E$. A solution
$\mathbf{x}$ is \emph{feasible}, and we say it is a \emph{flow}, if it satisfies capacity constraints at every arc:

\begin{equation} \label{eq:capacity-constraints}
0\leq x_{ij}\leq u_{ij}\:\forall(i,j)\in E
\end{equation}

and mass balance constraints at each node:

\begin{equation} \label{eq:mass-balance}
\sum_{j\::\:(i,j)\in E}x_{ij}-\sum_{j\::\:(j,i)\in E}x_{ji}=b(i)\:\forall i\in V
\end{equation}

That is, the net flow out of the node is exactly equal to the supply
at that node (which may be negative.)

\subsubsection{Assumptions}

\begin{assumption}[Integrality] \label{assumption:integrality}
All quantities defining the flow network take on only integer values.\end{assumption}
    
Note any network with rational quantities can be transformed into an equivalent integral network, by multiplying throughout by the greatest common denominator. Meanwhile, flow algorithms may not terminate on irrational data, so there is little point considering this case. Fortunately, the networks produced by systems such as Quincy are naturally integral, since quantities such as resource consumption in a data centre are inherently discrete. \\

\begin{assumption}[Non-negative arc costs] \label{assumption:non-negative-arc-costs}
For all $(i,j) \in E$, $c_{ij} \geq 0$.\end{assumption}

~

\begin{remark} 
This assumption is without loss of generality\footnote{Any network with negative arc costs can be transformed into an equivalent one with non-negative arc costs. For each arc $(i,j) \in E$ with a negative cost, replace the flow variable $x_{ij}$ with the variable $u_{ij} - x_{ji}$. Arc $(j,i)$ then replaces arc $(i,j)$, and $c_{ji} = -c_ij \geq 0$. \cite[p.~48]{Ahuja:1993}}.
\end{remark}

\begin{cor}All quantities defining arcs take on only values from the natural numbers.\footnotemark
\footnotetext{To avoid confusion, I will follow the convention $0\in\mathbb{N}$ throughout this document.}
\end{cor}
\begin{proof}
Every arc is defined by its capacity and its cost. Capacities are non-negative by definition, and arc costs are non-negative by \cref{assumption:non-negative-arc-costs}. This together with \cref{assumption:integrality} guarantee capacities and costs are natural numbers.
\end{proof}

We can therefore use exclusively unsigned integer data types when representing arcs in computer code.

\subsubsection{The minimum-cost flow problem} \label{sec:prep-flow-mcf}

The well-known maximum flow problem involves finding a solution vector
$\mathbf{x}$ subject to constraints (1) and (2), i.e. finding a feasible
flow.

Our focus will be on a generalization of this, known as the minimum
cost problem. Formally, it is:

\begin{equation} \label{eq:mcf-primal-problem}
\mbox{minimise}\ s(\mathbf{x})=\sum_{(i,j)\in E}c_{ij}x_{ij}
\end{equation}
where $\mathbf{x}$ is constrained to be a feasible flow. A feasible
flow $\mathbf{x}$ minimizing the above quantity is said to be \emph{optimal}.

In general, there may be no feasible solution to a network. For example,
the problem may be imbalanced: $\sum_{i\in V}b_{i}\neq0$. All networks
produced by the Quincy system are guaranteed to be solvable, however,
so this does not concern us. Consequently, I will assume in subsequent
discussions that networks are feasible, unless stated otherwise%
\footnote{For robustness, however, all algorithms implemented as part of this project detect if a network is infeasible.}.

\subsection{Scheduling using flows}

% Previously have described the advantages of Quincy, and roughly its architecture, but not gone into any technical detail.
% Intro: models scheduling as an optimisation problem, the minimum-cost flow problem.
% Flow from tasks (source nodes) to machines or unscheduled aggregators, draining to a sink
% Capacities specify possible scheduling states of tasks: e.g. enforce minimum number of tasks scheduled per job
% Costs specify preferences

Quincy and Firmament frame scheduling in terms of the minimum-cost flow problem. \emph{Jobs} are submitted to the scheduler, each consisting of a number of \emph{tasks}. Flow drains from task nodes to a sink node. Along the way, it passes either through a machine node (indicating the task is to be placed there) or a special job unscheduled aggregator node (indicating the task is to be left unscheduled).

Arc capacities are used to \emph{restrict} the possible scheduling assignments. The costs associated with arcs, by contrast, specify \emph{preferences} between possible scheduling assignments. Different scheduling policies may be encoded by varying the costs.

\subsubsection{Network structure}
%TBC: Table here would really help
%TBC: Flow graphs really useful here! Illustrate each type of network.

Each task $i$ in job $j$ is represented by a node $\mathbf{T}_i^j$, with a single unit of supply. Machines are represented by nodes $\mathbf{M}_l$, with arcs to a sink node $\mathbf{S}$. Task nodes have arcs to machine nodes, with a cost specifying the preference of being scheduled on that machine.

There is a problem with this scheme: if there are more tasks submitted than can be scheduled, the flow problem would become unfeasible. To fix this, we introduce \emph{unscheduled aggregators} $\mathbf{U}^j$ for each job $j$, with an arc to $\mathbf{S}$. Each task node $\mathbf{T}_i^j$ also has an arc to its unscheduled aggregator $\mathbf{U}_j$, with a cost specifying the penalty for being unscheduled.

In the model above, every task has an arc to every machine. Whilst this allows for the model to be highly precise, it is very inefficient\footnotemark. Quincy makes the modification of introducing \emph{rack aggregator} nodes $\mathbf{R}_k$, each with an arc to every machine in that rack. Moreover, there is a single \emph{cluster aggregator} $\mathbf{X}$ with an arc to every rack aggregator\footnotemark. Each task then has a fixed number of arcs. Some of these may be to individual machines, some to rack aggregators and every task has an arc to the cluster aggregator. 
\footnotetext{The number of arcs will scale linearly with the number of machines. But, of course, as the size of a cluster grows the number of tasks being scheduled will also tend to grow proportionally. So the number of arcs actually will tend to grow \emph{quadratically} with increases in the number of machines. Scalability is a key concern, making this is highly undesirable.}
\footnotetext{Of course, this hierarchical scheme could be extended to any number of levels.}

This scheme of rack aggregators is particularly suited to Quincy. Its cost model is based in large upon the amount of network bandwidth used by a task, which will be similar within a rack. This approach may work less well for other cost models. Firmament generalised this method by introducing \emph{equivalence classes}, both for task and machine\footnotemark nodes, with an aggregator node for entities within an equivalence class.
\footnotetext{In fact, Firmament supports representing resources at a finer grain than machines: for example, an individual core. So machines may themselves be aggregator nodes.}

\subsubsection{Capacities}

Every arc leaving a task node $\mathbf{T}_i^j$ has unit capacity. Note $\mathbf{T}_i^j$ has unit supply and no incoming arcs, so any (non-zero) capacity would be equivalent to unit capacity.

The arcs $\mathbf{M}_l \to \mathbf{S}$ from machine nodes to the sink node have capacity $K$, where $K$ is a parameter specifying the number of tasks which may concurrently run on a machine\footnotemark.
\footnotetext{In the original Quincy system, $K=1$. However, this is clearly a poor utilisation of modern multi-core machines. In fact, performance may often be improved by setting $K > \text{\# of cores}$, on simultaneous multi-threading (SMT) machines.}

The arcs $\mathbf{R}_k \to \mathbf{M}_l$ from rack aggregators to nodes representing their constituent machines also have capacity $K$, since up to $K$ jobs may be scheduled on each machine in the rack via the aggregator. The arcs $\mathbf{X} \to \mathbf{R}_k$ have capacity $mK$, where $m$ is the number of machines in each rack.

The arcs $\mathbf{U}^j \to \mathbf{S}$ can be set to be uncapacitated, allowing any number of tasks in job $j$ to be unscheduled. It is possible, however, to enforce a lower bound $E_j$ and upper bound $F_j$ on the number of tasks that can be scheduled for each job. This is used in Quincy and Firmament. It can provide some degree of fairness, and in particular specifying $E_j \geq 1$ ensures starvation freedom.

Let $N_j$ be the number of tasks submitted for job $j$. To enforce the upper bound $F_j$, give $\mathbf{U}^j$ a demand of $N_j - F_j$. At least this many tasks in job $j$ must then be left unscheduled, guaranteeing no more than $F_j$ tasks are scheduled.

To enforce the lower bound, we set the capacity on $\mathbf{U}^j \to \mathbf{S}$ to $F_j - E_j$. The maximum number of unscheduled tasks is thus the $N_j - F_j$ which are absorbed at $\mathbf{U}^j$, plus the $F_j - E_j$ draining via $\mathbf{U}^j \to \mathbf{S}$. This gives an upper bound of $N_j - E_j$ unscheduled tasks, guaranteeing that at least $E_j$ tasks remain scheduled.

\subsubsection{Cost models}  

% TODO: Firmament supports more cost models, such as Octopus. Will this be in PhD/can I cite it?
A key benefit to the flow scheduling approach is policy flexibility: support for a policy can be added simply by providing a new cost model. The cost model used in the original Quincy system attempts to achieve \emph{data locality}: scheduling tasks close to their input data, in order to minimise network traffic. Firmament includes three other cost models~\cite[ch.~5]{Schwarzkopf:2015}. The Whare-Map and Coordinate Co-Location models aim to improve the utilisation of the cluster by avoiding co-location interference and taking into account the heterogeneity of available resources. The Energy-Aware cost model seeks to achieve energy efficiency whilst meeting application performance guarantees, in a hypothetical heterogeneous cluster containing a mixture of high performance (but low efficiency) and medium performance (but high efficiency) machines.

Given such a variety of possible policies, the solver developed in this project must be independent of any particular cost model. However, having an intuition for how cost models operate is valuable. For reasons of brevity, I will only summarize the cost model in the original Quincy system. Further details of this cost model are available in~\cite{Isard:2009}. A description of the other cost models mentioned above is in~\cite[ch.~5]{Schwarzkopf:2015}.

\paragraph{Quincy cost model}
\begin{table}
    \begin{tabular}{c|c|l}
        \hline 
        Cost & Edge & Meaning\tabularnewline
        \hline 
        \hline 
        $v_{i}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{U}^{j}$ & Cost of leaving task $\mathbf{T}_{i}^{j}$ unscheduled\tabularnewline
        \hline 
        $\alpha_{i}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{X}$ & Cost of scheduling on worst possible machine\tabularnewline
        \hline 
        $\rho_{i,l}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{R}_{l}$ & Cost of scheduling on worst machine in rack $\mathbf{R}_{l}$\tabularnewline
        \hline 
        $\gamma_{i,m}^{j}$ & $\mathbf{T}_{i}^{j}\to\mathbf{M}_{m}$ & Cost of scheduling on machine $\mathbf{M}_{m}$\tabularnewline
        \hline 
    \end{tabular}
    \caption{Costs in the Quincy model}
    \label{table:quincy-costs}
\end{table}

\begin{table}
    \begin{tabular}{c|l}
        \hline 
        Variable & Meaning\tabularnewline
        \hline 
        $\chi^{X}\left(\mathbf{T}_{i}^{j}\right),\chi_{l}^{R}\left(\mathbf{T}_{i}^{j}\right),\chi_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ & Data transfer for task $\mathbf{T}_{i}^{j}$ across the core switch.\tabularnewline % if: scheduled on worst possible computer in cluster, scheduled on worst possible computer in rack $\mathbf{R}_{l}$ or scheduled on computer $\mathbf{M}_{m}$ respectively.
        $\mathcal{R}^{X}\left(\mathbf{T}_{i}^{j}\right),\mathcal{R}_{l}^{R}\left(\mathbf{T}_{i}^{j}\right),\mathcal{R}_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ & Data transfer for task $\mathbf{T}_{i}^{j}$ across the top-of-rack switch.\tabularnewline
        $\theta_{i}^{j}$ & Number of seconds $\mathbf{T}_{i}^{j}$ has spent scheduled.\tabularnewline
        $\nu_{i}^{j}$ & Number of seconds task $\mathbf{T}_{i}^{j}$ has spent unscheduled.\tabularnewline
        \hline
    \end{tabular}
    \caption{Variables in the Quincy model}
    \label{table:quincy-variables}
\end{table}

\begin{table}
    \begin{tabular}{c|l}
        \hline 
        Parameter & Meaning\tabularnewline
        \hline 
        \hline 
        $\epsilon$ & Cost of transferring 1 GB across core switch\tabularnewline
        \hline 
        $\psi$ & Cost of transferring 1 GB across top of rack switch\tabularnewline
        \hline 
        $\omega$ & Wait-time cost factor for unscheduled aggregators\tabularnewline
        \hline 
    \end{tabular}
    \caption{Parameters for the Quincy model}
    \label{table:quincy-parameters}
\end{table}

The arc costs specified by the model are given in \cref{table:quincy-costs}. All arcs not mentioned in this table have a cost of zero\footnotemark.
\footnotetext{In particular, arcs leaving aggregator nodes, and arcs incoming to the sink, both have zero cost.}

The model assumes the data processed by tasks resides in a distributed file system. Each machine in the cluster can access any object in the file system, by fetching the data from a remote machine. However, retrieving an object from a machine in another rack is more expensive -- both in terms of request latency and network resources consumed -- than transferring the data from a neighbouring machine. Reading the data from a directly connected disk is cheaper still. The Quincy scheduler quantifies these data transfer costs for each scheduling assignment. It then finds an assignment minimising the global data transfer cost across the cluster, in order to achieve data locality.

\Cref{table:quincy-variables} gives the variables maintained by the Quincy scheduler. All arc costs are functions of these variables. The system breaks down the data transfer of a task $\mathbf{T}_{i}^{j}$ into two components: transfer across the core switch, $\chi\left(\mathbf{T}_{i}^{j}\right)$, and top of rack switches, $\mathcal{R}\left(\mathbf{T}_{i}^{j}\right)$. For a particular machine $\mathbf{M}_m$, the data transfer $\chi_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ and $\mathcal{R}_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)$ can be computed exactly. However, when it comes to determining the cost of arcs to aggregator nodes $\mathbf{R}_l$ or $\mathbf{X}$, an exact figure is not possible. Instead, we take a conservative upper bound: the \emph{greatest} data transfer that results from scheduling $\mathbf{T}_{i}^{j}$ on the \emph{worst} machine in rack $\mathbf{R}_l$ or the cluster as a whole, respectively.

The system also keeps track of the time $\theta_i^j$ and $\nu_i^j$ a task spends scheduled and unscheduled, respectively. If the task is stopped and then restarted, these times continue to accumulate. This property is needed to guarantee the scheduler makes progress.

Quincy is controlled by three simple parameters, given in \cref{table:quincy-parameters}. $\epsilon$ and $\psi$ specify the cost of data transfers across core and top of rack switches respectively. In general, we would expect $\epsilon > \psi$, since the core switch is more likely to become a bottleneck. $\omega$ controls the penalty for leaving a task unscheduled\footnotemark. Increasing $\epsilon$ and $\psi$ will cause the scheduler to more aggressively optimise for data locality. Increasing $\omega$ will 
\footnotetext{It is possible to vary $\omega$ between jobs in order to encode a notion of priority.}

We are now ready to give formulae for the arc costs given in \cref{table:quincy-costs}. First, let us define the data transfer cost:
\[d_{b}^{A}\left(\mathbf{T}_{i}^{j}\right) = \epsilon\chi_{b}^{A}\left(\mathbf{T}_{i}^{j}\right)+\psi\mathcal{R}_{b}^{A}\left(\mathbf{T}_{i}^{j}\right)\]

The cost of scheduling on the worst machine in the cluster and the worst machine in rack $\mathbf{R}_l$ follow simply:
\[\alpha_{i}^{j} = d^X\left(\mathbf{T}_{i}^{j}\right)\]
\[\rho_{i}^{j,l} = d^R_l\left(\mathbf{T}_{i}^{j}\right)\]
It is tempting to use the same form for the cost, $\gamma^j_{i,m}$ of scheduling on a particular machine $\mathbf{M}_m$. When $\mathbf{T}_{i}^{j}$ is not running on $\mathbf{M}_m$ (either it is unscheduled, or is running on a different machine), this is valid. However, this formula is inappropriate when $\mathbf{T}_{i}^{j}$ is already scheduled on  $\mathbf{M}_m$.

When the task has already been running, $d^M_m\left(\mathbf{T}_{i}^{j}\right)$ will overestimate the cost of the remaining data transfer. To account for the work already invested in $\mathbf{T}_{i}^{j}$, we subtract the time it has been running, giving:
\[\gamma_{i,m}^{j} = d_{m}^{M}\left(\mathbf{T}_{i}^{j}\right)-\begin{cases}
\theta_{i}^{j} & \text{if \ensuremath{\mathbf{T}_{i}^{j}}running on \ensuremath{\mathbf{M}_{m}}}\\
0 & \text{otherwise}
\end{cases}\]

The only remaining cost is $v_i^j$, the penalty associated with leaving a task $\mathbf{T}_{i}^{j}$ unscheduled. We set it proportional to the length of time it has been unscheduled:
\[v_i^j = \omega\nu_{n}^{j}\]

\subsubsection{Properties of these networks}

\begin{lemma} \label{lemma:network-num-nodes}
Let $n$ denote the number of nodes in the network. Then $n = \Theta\left(\text{\# machines} + \text{\# tasks}\right)$.
\end{lemma}
\begin{proof}
There is one node $\mathbf{T}_i^j$ for every task and one node $\mathbf{M}_m$ for every machine, so trivially $n = \Omega\left(\text{\# machines} + \text{\# tasks}\right)$.

It remains to show $n = O\left(\text{\# machines} + \text{\# tasks}\right)$. Consider each class of node in turn.

As stated above $\text{\# task nodes} = \text{\# tasks}$ and $\text{\# machine nodes} = \text{\# machines}$. There is an unscheduled aggregator $\mathbf{U}^j$ for each job $j$. Certainly $\text{\# jobs} = O\left(\text{\# tasks}\right)$. There is at least one machine in every rack, so $\text{\# rack aggregator nodes} = O\left(\text{\# machines}\right)$. 

In addition, there is also a cluster aggregator node $\mathbf{X}$ and sink node $\mathbf{S}$ which are independent of the number of tasks and machines, contributing $O(1)$ nodes.

Thus:
\[n = O\left(\text{\# tasks} + \text{\# machines} + 1\right) = O\left(\text{\# tasks} + \text{\# machines}\right)\]

Hence:
\[n = \Theta\left(\text{\# machines} + \text{\# tasks}\right)\]
\end{proof}

\begin{lemma} \label{lemma:network-num-arcs}
Let $m$ denote the number of arcs in the network. Then $m = O(n)$. That is, the network is \emph{sparse}.
\end{lemma}
\begin{proof}
Consider the outgoing arcs from each class of node. Since every arc is outgoing from exactly one node, this will count every node exactly once.

Each machine $\mathbf{M}_l$ and unscheduled aggregator $\mathbf{U}^j$ has a single outgoing arc, to sink node $\mathbf{S}$. This contributes $O\left(\text{\# machines}\right)$ arcs. The sink node $\mathbf{S}$ has no outgoing arcs.

Rack aggregators $\mathbf{R}_l$ have outgoing arcs to each machine in their rack. Each machine is present in exactly one rack, so these contribute collectively $O\left(\text{\# machines}\right)$ arcs. The cluster aggregator $\mathbf{X}$ has outgoing arcs to each rack; since $O\left(\text{\# racks}\right) = O\left(\text{\# machines}\right)$, this contributes $O\left(\text{\# machines}\right)$ arcs.

It remains to consider task nodes $\mathbf{T}_i^j$. The number of arcs leaving the task node has a constant upper bound. The system computes a \emph{preference list} of machines and racks, and includes arcs only to those nodes (and the cluster aggregator $\mathbf{X}$). Thus this contributes $O\left(\text{\# tasks}\right)$ arcs.

Hence:
\[m = \left(\text{\# machines} + \text{\# tasks}\right)\]
By \cref{lemma:network-num-nodes}, it follows:
\[m = O(n)\]
\end{proof}

\begin{remark}
In fact we have $m = \Theta(n)$: the network is connected so certainly $m = \Omega(n)$. However, we will only use the bound $m = O(n)$.\\
\end{remark}

\begin{lemma} \label{lemma:network-supply}
The largest supply in the network is a unit supply.
\end{lemma}
\begin{proof}
The only nodes in the network with any supply are the task nodes, $\mathbf{T}_i^j$. These by definition have unit supply.
\end{proof}

\begin{remark}
Most nodes in the network are transshipment nodes, with no demand nor supply. The demand nodes in the network are the sink node $\mathbf{S}$ and (optionally) unscheduled aggregators, $\mathbf{U}^j$. These may have a greater than unit demand.
\end{remark}

\subsection{Further definitions and properties}

\subsubsection{Pseudoflows} \label{sec:prep-flow-pseudo}

Whilst all flow algorithms must return a feasible solution, many operate
by manipulating \emph{pseudoflows} in intermediate stages. A pseudoflow
is a vector $\mathbf{x}$ which satisfies the capacity constraints
\cref{eq:capacity-constraints}, but which may not satisfy the mass balance constraints \cref{eq:mass-balance}.

We define the \emph{excess} at a node $i\in V$ to be:

\begin{equation}
e_{i}=b_{i}+\sum_{j\::\:(j,i)\in E}x_{ji}-\sum_{j\::\:(i,j)\in E}x_{ij}
\end{equation}

Node i is said to be an \emph{excess node} if $e_{i}>0$, and a \emph{deficit
	node} if $e_{i}<0$ (with deficit $-e_{i}$). If $e_{i}=0$, $i$
is said to be \emph{balanced}. Note the mass balance constraints hold if and
only if all nodes are balanced.

\subsubsection{Residual networks}

Also important to many flow algorithms is the notion of \emph{residual}
\emph{networks. }The residual network is defined with respect to the
original flow network $G$ and a (pseudo)flow $\mathbf{x}$, and is
denoted by $G_{\mathbf{x}}$. Informally, it represents the actions
an algorithm can take to modify the (pseudo)flow $\mathbf{x}$. In
the case where $\mathbf{x=0},$ we have $G_{\mathbf{0}}=G$.

Formally, we define $G_{\mathbf{x}}=\left(V,E_{\mathbf{x}}\right)$
to be a directed graph where:
\begin{equation}
E_{\mathbf{x}}=\left\{ (i,j)\in V^{2}\::\:(i,j)\in E\land x_{ij}<u_{ij}\right\} \cup\left\{ (j,i)\in V^{2}\::\:(j,i)\in E\land x_{ij}>0\right\} 
\end{equation}


The former set consists of \emph{forward arcs}, which were present
in the original flow network. An arc $(i,j)$ which is saturated,
i.e. $x_{ij}=u_{ij}$, drops out of the residual network. The latter
set consists of \emph{backward arcs}, the reverse of arcs in the original
network. Only arcs with positive flow in the original network have
a corresponding backwards arc.

We define the \emph{residual capacity} of an arc $(i,j)\in E_{\mathbf{x}}$
to be:

\begin{equation}
r_{ij}=\begin{cases}
u_{ij}-x_{ij} & ,\:(i,j)\:\mbox{is a forward arc}\\
x_{ij} & ,\:(i,j)\:\mbox{is a reverse arc}
\end{cases}
\end{equation}

We define the cost of a forward arc $(i,j)$ to be $c_{ij}$, and
a reverse arc $(j,i)$ to be $-c_{ij}$.


\subsubsection{Reduced cost and duality} \label{sec:prep-flow-rc-and-dual}

We may associate with each node $i\in V$ a \emph{potential}, $\pi_{i}$.

\begin{defn}[Reduced costs] \label{defn:reduced-costs}
The \emph{reduced cost} of an arc $(i,j)\in E$ with respect to a
potential $\boldsymbol{\pi}$ is defined as:
\begin{equation}
c_{ij}^{\boldsymbol{\pi}}=c_{ij}-\pi_{i}+\pi_{j}
\end{equation}
\end{defn}

Every linear programming problem, referred to as a \emph{primal} problem,
can be converted to a \emph{dual} problem. Solutions to the dual problem
give an upper bound on the objective value of the primal problem.

The primal version of the minimum-cost flow problem is stated in \S\ref{sec:prep-flow-mcf}. The dual problem is:

\begin{equation}
\mathrm{maximise}\; w(\boldsymbol{\pi})=\sum_{i\in V}b_{i}\pi_{i}-\sum_{(i,j)\in E}\max\left(0,-c_{ij}^{\pi}\right)u_{ij}
\end{equation}

with no constraints on $\boldsymbol{\pi}$.

It can be computationally more efficient to solve the dual problem
rather than the primal problem. Many flow algorithms adopt a dual
approach, or a hybrid of a primal and dual approach.

\subsubsection{Optimality conditions} \label{prep:flow-optimality}

Below, I give conditions on a solution vector $\mathbf{x}$ which
imply it is optimal. These conditions suggest algorithms for solving
the problem. Moreover, they can be used in testing, to verify that
a solution is indeed optimal.\\

\begin{thm}[Negative cycle optimality conditions] \label{thm:optimality-neg-cycle}
A flow $\mathbf{x}$ is an optimal solution to the minimum-cost flow problem if and only if the residual network $G_\mathbf{x}$ has no negative cost (directed) cycle.
\end{thm}
\begin{proof}
See~\cite[p.~307]{Ahuja:1993}.
\end{proof}

\begin{thm}[Reduced cost optimality conditions] \label{thm:optimality-reduced-cost}
Let $\mathbf{x}$ be a feasible solution. It is an optimal solution
to the minimum-cost flow problem if and only if there exists a node
potential vector $\boldsymbol{\pi}$ such that that the reduced cost
of each arc in the residual network $G_{\mathbf{x}}$ is non-negative:

\begin{equation} \label{eq:optimality-reduced-cost}
c_{ij}^{\boldsymbol{\pi}}\geq0\:\forall(i,j)\in E_{\mathbf{x}}
\end{equation}
\end{thm}
\begin{proof}
See~\cite[p.~309]{Ahuja:1993}.
\end{proof}

\begin{thm}[Complementary slackness optimality conditions] \label{thm:optimality-complementary-slackness}
Let $\mathbf{x}$ be a feasible solution. It is an optimal solution
to the minimum-cost flow problem if and only if there exists a node
potential vector $\boldsymbol{\pi}$ such that for every arc $(i,j)\in E$:

\begin{equation}
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}>0}, then \ensuremath{x_{ij}=0}}
\end{equation}

\begin{equation}
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}<0}, then \ensuremath{x_{ij}=u_{ij}}}
\end{equation}

\begin{equation}
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}=0}, then \ensuremath{0\leq x_{ij}\leq u_{ij}}}
\end{equation}
\end{thm}
\begin{proof}
This can easily be seen to be equivalent to the reduced cost optimality conditions, by expanding out \cref{eq:optimality-reduced-cost} using the definition of a residual network and performing a case analysis. A detailed proof is given in~\cite[p.~310]{Ahuja:1993}. 
\end{proof}

\subsubsection{Notation for complexity analysis} \label{sec:prep-flow-complexity}

Some notation will be useful when it comes to analysing the asymptotic complexity
of algorithms. 

Let $n=|V|$ and $m=|E|$ denote the number of nodes and arcs respectively.

The running time of algorithms depends not just on the size of the
network, but also on the magnitudes of input data. Let $U$ denote
the largest node supply/demand, or arc capacity:
\begin{equation}
U=\max\left(\max\left\{ |b_{i}|\::\: i\in V\right\} ,\max\left\{ u_{ij}\::\:\left(i,j\right)\in E\right\} \right)
\end{equation}

and let $C$ denote the largest arc cost:

\begin{equation}
C=\max\left\{ c_{ij}\::\:(i,j)\in E\right\} 
\end{equation}

\section{Software engineering} \label{sec:prep-sweng}

\subsection{Requirements analysis} \label{sec:prep-sweng-requirements}
What the system should do. Include table listing goals and their priorities, etc.

\subsection{Model}
State the model you used (something like Spiral probably closest fit). Elaborate on what it is. Justify choice: can reference constraints imposed by requirements, e.g. some of them risky/speculative so waterfall model inappropriate.

\subsection{Testing}
Summarize approach. Maybe make forwards reference to \ref{sec:prep-tools-testing}.

\section{Choice of tools} \label{sec:prep-tool-choice}
\subsection{Language and libraries}
Justify language choice (C++, Python). Libraries: GLOG, Boost.

Perhaps mention coding style guides?

\subsection{Development environment}
Physical machines used: laptop, MCS, SRG cluster.

Revision control: Git.

Backup strategy: GitHub. Daily snapshots backed up to MCS, SRCF, Copy (cloud).

CMake. IDE: Eclipse with plugins, vim.

\subsection{Testing environment} \label{sec:prep-tools-testing}
GTest for unit tests. Other automated tests: Python. Note that performance evaluation part of testing requires custom development: no pre-existing benchmark suites suitable.

\section{Initial experience}
My starting point. C++: from IB Tripos. Algorithms: IA/IB. Maths: IA. No prior knowledge of flow networks.

\section{Project schedule} \label{sec:prep-project-schedule}
How I planned my time. Split into phases. Core success criteria obtained early. Testing completed. Extensions added and additional experiments performed later on.