\chapter{Preparation} \label{chap:prep}

\section{Cluster scheduling} \label{sec:prep-scheduling}

% PROOFREAD: 1. Major edits. 2. Minor edits.

% TODO: Get some references from Malte/Ionel on this?
Much as an operating system scheduler maps threads to processor cores, a cluster scheduler maps tasks to machines. Better scheduling techniques can considerably improve performance: for example, throughput increased by 40\% under Quincy ~\cite{Isard:2009}. Due to their importance, cluster schedulers are an active area of research.

I start by outlining desirable properties of cluster schedulers. Following this, I summarise the prevailing approaches to scheduling. I conclude by discussing the principles underlying Firmament, showing how it improves upon competing approaches with its use of flow scheduling.

\subsection{Design goals} \label{sec:prep-scheduling-goals}

%WORDCOUNT: Could cut design goals down considerably, although I feel this is reasonably good stuff.

% PROOFREAD: 1. Major edits. 2. Minor edits.

% SOMEDAY: Rephrase 2nd sentence: "Moreover, ... may need to analyse a large amount of data ..."
To achieve optimal performance, the scheduling policy employed must be \emph{targeted} to the requirements of the workload. The scheduler may need to analyse a large amount of data, such as the resource requirements of each task and the utilisation of each machine, to \emph{effectively} implement a particular policy. All decisions should be made as \emph{fast} as possible, to support interactive applications. Furthermore, the system must be able to \emph{scale} to the size of modern clusters. These goals are summarised in \cref{table:cluster-scheduler-design-goals}.

\begin{table}
    \centering
    \begin{tabular}{cl}
        \textbf{Goal} & \textbf{Description}\tabularnewline
        \hline
        Targeted & Application requirements are reflected in the scheduler policy. \tabularnewline
        Effective & The schedules produced are optimal given the policy objectives. \tabularnewline
        Fast and scalable & Scheduling latency remains low even for large clusters. \tabularnewline
    \end{tabular}
    \caption{Cluster scheduler design goals}
    \label{table:cluster-scheduler-design-goals}
\end{table}

%WORDCOUNT: Could cut next three paragraphs without losing much
%Scheduler policies can have a variety of objectives. For batch processing, job throughput may be the primary goal; for interactive jobs, low latency may be more important. Moreover, modern clusters are often multi-tenant: it is essential that the scheduler is \emph{fair}, providing an equitable distribution of resources between users. Naturally, objectives often conflict with each other, and a trade-off must be chosen.
%
%Getting the objectives right is hard, but implementing a scheduler which effectively achieves those objectives is still more difficult. Particularly challenging is the sheer amount of information that must be processed: as an example, consider the Whare-Map scheduler. It seeks to minimise \emph{co-location interference}: the degradation of performance occurring when tasks running on the same machine compete for shared resources~\cite{Mars:2013}\footnotemark. To achieve this objective, it monitors the performance of \emph{every} task running in the cluster, building up a profile of how applications interact.
%\footnotetext{In fact, the approach is more general than this: Whare-Map tries to take into account machine heterogeneity due to hardware differences as well.}
%
%% SOMEDAY: Terminology re: latency is a bit confused. Here you're talking about scheduling latency to do with the solver being slow. But in the next section you talk about latency due to policy: a FIFO queue.
%Finally, the system should remain responsive at scale. Today's warehouse scale computers contain tens of thousands of machines, running hundreds of thousands of tasks. Furthermore, these platforms now often run interactive applications: for example, a job may be spawned in response to a query in a data analytics platform, requiring sub-second scheduling latency~\cite{Ousterhout:2013}.

\subsection{Prevailing approaches}

\begin{table}
    \centering
    \begin{tabular}{lccc}
        \textbf{System} & \textbf{Targeted} & \textbf{Effective} & \textbf{Fast and scalable} \tabularnewline
        \hline
        \textit{Traditional schedulers} \tabularnewline % TODO: new name? or mention heuristic in body text
        Hadoop Fair Scheduler~\cite{HadoopFairSchedulerJIRA} & \mmark & \cmark & \mmark \tabularnewline 
%        YARN~\cite{YARN} & \cmark & \mmark & \mmark \tabularnewline
        Apache Mesos~\cite{Hindman:2011} & \cmark & \mmark & \mmark \tabularnewline
%        Omega~\cite{Omega} & \mmark & \cmark & \mmark \tabularnewline 
        Sparrow~\cite{Ousterhout:2013} & \xmark & \mmark & \cmark \tabularnewline 
%        Capacity Scheduler & \mmark & \cmark & \mmark \tabularnewline 
%        LATE & \xmark & \cmark & \mmark \tabularnewline 
        \hline
        \textit{Flow-based schedulers} \tabularnewline
        Quincy~\cite{Isard:2009} & \xmark & \cmark & \xmark \tabularnewline 
        Firmament~\cite[ch.~5]{Schwarzkopf:2015} & \cmark & \cmark & \xmark \tabularnewline
        Firmament using Hapi & \cmark & \cmark & \cmark \tabularnewline
        \hline
    \end{tabular}
    \caption[Scheduler feature matrix]{Scheduler feature matrix showing {\cmarkcolor full \cmark}, {\mmarkcolor partial \mmark} or {\xmarkcolor no \xmark} support.}
    \label{table:cluster-scheduler-feature-matrix}
\end{table}
% PROOFREAD: 1. Minor edits.

Commensurate with their importance, a variety of cluster scheduling designs have been investigated, summarised in \cref{table:cluster-scheduler-feature-matrix}. Traditionally, schedulers have been \emph{monolithic}, designed for a single workload. For example, the Hadoop Fair Scheduler only manages MapReduce jobs~\cite{HadoopFairSchedulerJIRA}.

Although monolithic schedulers may perform well on their intended workload, they cannot be targeted to different applications. Newer schedulers such as Mesos~\cite{Hindman:2011} adopt a \emph{two-level} design~\cite[\S3.3]{Schwarzkopf:2013}: a centralised allocator dynamically partitions resources in the cluster between \emph{scheduling frameworks}, each of which implement a policy appropriate to the applications they manage. However, as each framework sees only a subset of the resources in the cluster, they are not as effective as monolithic schedulers.

To support interactive applications on large-scale clusters, there has been research into \emph{distributed} scheduling. These methods can be very fast. For example, the Sparrow scheduler achieves sub-\SI{100}{\milli\second} scheduling latency, employing a sampling mechanism to avoid maintaining any centralised state~\cite[fig.~10]{Ousterhout:2013}. However, the lack of global state imposes limits on the effectiveness of distributed schedulers.

\subsection{Firmament and flow scheduling}
%WORDCOUNT: Firmament and flow scheduling -- could probably trim

% PROOFREAD: 1. Major edits. Need to reread.

% SOMEDAY: Add citations

% Introduce flow scheduling: Quincy monolithic scheduler. Very effective: avoid heuristic approaches.
% Firmament generalises it: targeted, effective.
% But SLOW! What if we can solve this?
% Centralised scheduling, scaling to large clusters?

% Key benefit of flow scheduling: global optimisation? Flexible policy also useful.

\emph{Flow scheduling} models scheduling as an optimisation problem over flow networks (see \cref{sec:prep-flow-scheduling}). The effectiveness of this approach was demonstrated in the pioneering Quincy system, which delivered a 40\% increase in throughput and reduced data transfer by $3.9\times$~\cite{Isard:2009}. 

Firmament generalises Quincy, allowing a variety of scheduling policies to be specified. Like with two-level schedulers, the policy can be chosen on a per-application basis. However, flow scheduling does not partition the cluster: solving the optimisation problem globally allows better scheduling decisions to be made.

Flow algorithms are notoriously difficult to parallelise, due to the global nature of the problem. Accordingly, the flow solver quickly becomes a bottleneck, leading some researchers to claim flow scheduling systems cannot scale~\cite[\S6]{Boutin:2014}. In this dissertation, I design and implement new flow algorithms which allow these systems to compete with the fastest distributed schedulers.

\section{Flow networks} \label{sec:prep-flow}

I first define flow networks and their associated optimisation problems, before discussing the details of scheduling using flow networks. Following this, I summarise properties of flow networks useful for the analysis and design of flow algorithms.

\subsection{Introduction}

% PROOFREAD: 1 minor edits.

\subsubsection{Definitions and notation}

A \emph{flow network} is a weakly connected\footnotemark directed graph $G=(V,E)$.
\footnotetext{A directed graph is weakly connected if the undirected graph formed by replacing all directed arcs with undirected edges is itself connected.}

Each arc $(i,j)\in E$ has an associated \emph{capacity} (or \emph{upper	bound}\footnotemark) $u_{ij}$, and a \emph{cost} $c_{ij}$.
\footnotetext{Some authors include a lower bound $l_{ij}$, but this does not feature in flow scheduling. Moreover, any network featuring lower bounds can be transformed into an equivalent one without~\cite[p.~39]{Ahuja:1993}.}

Each vertex $i\in V$ has an associated supply/demand $b_{i}$. Vertex $i$ is said to be a \emph{supply vertex} if $b_{i}>0$, a \emph{demand vertex} if $b_{i}<0$ and a \emph{transshipment vertex} if $b(i)=0$.

The problems we will consider involve finding a solution vector $\mathbf{x}$,
specifying the flow $x_{ij}$ at each arc $(i,j)\in E$. A solution
$\mathbf{x}$ is \emph{feasible}, and we say it is a \emph{flow}, if it satisfies capacity constraints at every arc $(i,j) \in E$:

\begin{equation} \label{eq:capacity-constraints}
0\leq x_{ij}\leq u_{ij},
\end{equation}

and mass balance constraints at each vertex $i \in V$:

\begin{equation} \label{eq:mass-balance-constraints}
\sum_{j\::\:(i,j)\in E}x_{ij}-\sum_{j\::\:(j,i)\in E}x_{ji}=b_i.
\end{equation}

That is, the net flow out of the vertex is exactly equal to the supply
at that vertex (which may be negative, in the case of demand vertices).

\subsubsection{Assumptions}

The following two assumptions hold for all flow scheduling networks, and will be used in the design and analysis of algorithms in the next chapter. \\

\begin{assumption}[Integrality] \label{assumption:integrality}
All quantities defining the flow network take on only integer values.\\
\end{assumption}

\begin{assumption}[Non-negative arc costs] \label{assumption:non-negative-arc-costs}
For all $(i,j) \in E$, $c_{ij} \geq 0$. \\
\end{assumption}

\subsubsection{The minimum-cost flow problem} \label{sec:prep-flow-mcf}

The well-known maximum flow problem involves finding a solution vector
$\mathbf{x}$ subject to constraints \cref{eq:capacity-constraints} and \cref{eq:mass-balance-constraints}, i.e.\ finding a feasible flow.

Flow scheduling requires solving a generalization of this, known as the minimum-cost flow (MCF) problem. Formally, it is:

\begin{equation} \label{eq:mcf-primal-problem}
\mbox{minimise}\ s(\mathbf{x})=\sum_{(i,j)\in E}c_{ij}x_{ij}
\end{equation}
subject to the constraint that $\mathbf{x}$ must be a feasible flow.

Note, in general, that the MCF and maximum flow problem may be infeasible. For example, the network might be imbalanced: $\sum_{i\in V}b_{i}\neq0$. All networks produced by Firmament are guaranteed to be solvable, however, so I will tend not to discuss this case\footnotemark.
\footnotetext{For robustness, all algorithms implemented as part of this project detect if a network is infeasible.}

\subsection{Scheduling with flows} \label{sec:prep-flow-scheduling}

I will now show how cluster scheduling can be formulated in terms of the MCF problem, using the Quincy model as an example. For a detailed description of Quincy, see Isard \textit{et al.}~\cite{Isard:2009}. Firmament generalises Quincy, supporting different scheduling policies via a \emph{cost model} framework. I describe how I ported Quincy to this framework in \cref{appendix:flow-scheduling}. Schwarzkopf~\cite[ch.~5]{Schwarzkopf:2015} provides several examples of representing other scheduling policies in this manner.

% FIGURE: Prep:Scheduling using flows desperately needs a flow graph

\subsubsection{Network structure}

Each machine in the cluster is represented by a vertex, $\mathbf{M}_l$. The topology of the network follows that of the cluster: each rack is represented by a rack aggregator vertex $\mathbf{R}_j$, with arcs to  machines in that rack. There is also a single cluster aggregator vertex $\mathbf{X}$, with arcs to every rack aggregator.

The scheduler manages \emph{jobs}, which consist of a number of \emph{tasks}. Each job $j$ has an \emph{unscheduled aggregator} $\mathbf{U}_j$, and a task vertex $\mathbf{T}_i^j$ for each task. Every task has an arc to the cluster aggregator. Tasks may also have arcs to rack aggregators and machines.

Each task vertex $\mathbf{T}_i^j$ has unit supply. Tasks must drain into the \emph{sink} vertex $\mathbf{S}$, passing through a machine or unscheduled aggregator along the way\footnotemark.  Supply from tasks can only drain into the sink vertex via machines and unscheduled aggregators. If the flow from $\mathbf{T}_i^j$ passes through machine $\mathbf{M}_l$, the task is scheduled there; if it passes through the unscheduled aggregator $\mathbf{U}_j$, the task is left unscheduled.
\footnotetext{In fact, unscheduled aggregators may themselves have demand, consuming the flow before it reaches the sink: see \cref{appendix:flow-scheduling:capacities}.}

\subsubsection{Capacities and costs}
Arc capacities are used to \emph{restrict} the possible scheduling assignments. In Quincy, the capacities are chosen to provide a lower and upper bound on the number of tasks that may be scheduled for each job, guaranteeing fairness (see \cref{appendix:flow-scheduling:capacities}).

The costs associated with arcs specify \emph{preferences} between possible scheduling assignments. Quincy seeks to achieve data locality, and so sets the cost of an arc to be proportional to the network resources consumed. For arcs from a task to a machine, this can be computed exactly. When the arc is to a rack (or cluster) aggregator, a conservative approximation is used, assuming the scheduler places the task on the \emph{worst} machine in that rack (or the cluster as a whole).

To reduce the size of the network, Quincy only includes arcs to machines or racks which store a substantial proportion of the input data for the task. This bounds the outdegree, guaranteeing sparseness. 

Solving the minimum-cost flow problem on the resulting network finds a scheduling assignment which maximises data locality, subject to fairness constraints.

% XXX: TBC. Give references to more detailed sections of the appendix.

\subsubsection{Properties of these networks}

% PROOFREAD: 1 minor edits.

I state below some properties which hold for all flow scheduling networks. These will be useful in the next chapter, when analysing the complexity of algorithms. \\

% WORDCOUNT: Could shift the proofs to appendix, although keep the claims here.
\begin{restatable}[Number of vertices in flow scheduling networks]{lemma}{flowschedulingnumvertices}
\label{lemma:network-num-vertices}
Let $n$ denote the number of vertices in the network. Then $n = \Theta\left(\text{\# machines} + \text{\# tasks}\right)$.
\end{restatable}
\begin{proof}
See \cref{appendix:flow-scheduling:properties}.
\end{proof}

\begin{restatable}[Number of arcs in flow scheduling networks]{lemma}{flowschedulingnumarcs} 
\label{lemma:network-num-arcs}
Let $m$ denote the number of arcs in the network. Then $m = O(n)$. That is, the network is \emph{sparse}.
\end{restatable}
\begin{proof}
See \cref{appendix:flow-scheduling:properties}.
\end{proof}

\begin{lemma}[Size of supply in flow scheduling networks] \label{lemma:network-supply}
The largest supply in the network is a unit supply.
\end{lemma}
\begin{proof}
The only vertices in the network with any supply are the task vertices, $\mathbf{T}_i^j$. These by definition have unit supply.
\end{proof}

\begin{remark}
Most vertices in the network are transshipment vertices, with no demand nor supply. The demand vertices in the network are the sink vertex $\mathbf{S}$ and unscheduled aggregators, $\mathbf{U}^j$. These may have a greater than unit demand.
\end{remark}

\subsection{Key concepts in flow algorithms}

% PROOFREAD: 1 minor edits except for Residual Networks which was major

In the preceding sections, I have formalised the notion of a flow network and outlined how cluster scheduling can be formulated in terms of the MCF problem. This section introduces some further definitions and properties of flow networks, necessary for the implementation and analysis of algorithms in the next chapter.

\subsubsection{Pseudoflows} \label{sec:prep-flow-pseudo}

% PROOFREAD: 1 minor edits

A \emph{pseudoflow} is a vector $\mathbf{x}$ which satisfies the capacity constraints
\cref{eq:capacity-constraints}, but which need not satisfy the mass balance constraints \cref{eq:mass-balance-constraints}. While all flow algorithms must return a feasible solution, many operate on pseudoflows in intermediate stages. 

The \emph{excess} at a vertex $i\in V$ is defined to be:

\begin{equation}
e_i=b_i+\sum_{\set{j | (j,i)\in E}}x_{ji}-\sum_{\set{j | (i,j)\in E}}x_{ij}.
\end{equation}

Vertex $i$ is said to be an \emph{excess vertex} if $e_{i}>0$, and a \emph{deficit vertex} if $e_{i}<0$ (with deficit $-e_{i}$). If $e_{i}=0$, $i$ is said to be \emph{balanced}. Note the mass balance constraints \cref{eq:mass-balance-constraints} hold if and only if all vertices are balanced.

\subsubsection{Residual networks}

% PROOFREAD: 1 major edits.

The notion of \emph{residual networks} is used in many flow algorithms. The residual network of $G$ is defined with respect to a (pseudo)flow $\mathbf{x}$, and is denoted by $G_{\mathbf{x}}$. Informally, it represents the actions an algorithm can take to modify the (pseudo)flow $\mathbf{x}$. In the case where $\mathbf{x=0},$ we have $G_{\mathbf{0}}=G$.

Formally, we define $G_{\mathbf{x}}=\left(V,E_{\mathbf{x}}\right)$ to be a directed graph where:
\begin{equation}
E_{\mathbf{x}}=\set{(i,j)\in V^{2} | (i,j)\in E\land x_{ij}<u_{ij}} \cup \set{(j,i)\in V^{2} | (i,j)\in E\land x_{ij}>0}. 
\end{equation}

The former set contains \emph{forward arcs}, present in the original flow network. Arcs $(i,j)$ which are \emph{saturated}, i.e.\ $x_{ij}=u_{ij}$, are omitted from the residual network: they do not allow for any additional flow to be pushed. The second set consists of \emph{backward arcs}, the reverse of arcs in the original network. Only arcs with positive flow in the original network have a corresponding backwards arc. Pushing flow along the backward arc corresponds to cancelling flow on the forward arc.

The \emph{residual capacity} of an arc $(i,j)\in E_{\mathbf{x}}$ is defined to be:

\begin{equation}
r_{ij}=\begin{cases}
u_{ij}-x_{ij} & ,\:(i,j)\:\mbox{is a forward arc;}\\
x_{ij} & ,\:(i,j)\:\mbox{is a reverse arc.}
\end{cases}
\end{equation}

The cost of forward arcs is the same as in the original network, whereas the cost of a reverse arc $(j,i)$ is defined as $-c_{ij}$.

\subsubsection{Duality and reduced cost} \label{sec:prep-flow-rc-and-dual}

% PROOFREAD: 1 minor edits.

Every linear programming problem may be viewed from two perspectives, as either a \emph{primal} or \emph{dual} problem. Solutions to the dual problem provide a lower bound on the objective value of the primal problem. The primal version of the MCF problem was stated previously in \cref{sec:prep-flow-mcf}. 

To formulate the dual problem, it is necessary to associate with each vertex $i \in V$ a \emph{dual variable} $\pi_i$, called the \emph{vertex potential}. The \emph{reduced cost} of an arc $(i,j)\in E$ with respect to a potential vector $\boldsymbol{\pi}$ is defined as:
\begin{equation} \label{eq:reduced-costs}
c_{ij}^{\boldsymbol{\pi}}=c_{ij}-\pi_{i}+\pi_{j}.
\end{equation}

The dual problem can now be stated as:
\begin{equation}
\mathrm{maximise}\; w(\boldsymbol{\pi})=\sum_{i\in V}b_{i}\pi_{i}-\sum_{(i,j)\in E}\max\left(0,-c_{ij}^{\pi}\right)u_{ij}
\end{equation}
with no constraints on $\boldsymbol{\pi}$.

Many flow algorithms seek to solve the dual problem, as this may be computationally more efficient. Others try to enjoy the best of both worlds, operating on both the primal and dual versions of the problem.

\subsubsection{Optimality conditions} \label{sec:prep-flow-optimality}

% PROOFREAD: 1 minor edits.

Below, I give conditions for a solution vector $\mathbf{x}$ to be optimal. These may suggest algorithms for solving the problem, and are needed for correctness proofs in the subsequent chapter. \\

\begin{restatable}[Negative cycle optimality conditions]{thm}{optimalitynegcycle}
\label{thm:optimality-neg-cycle}
Let $\mathbf{x}$ be a (feasible) flow. It is an optimal solution to the minimum-cost flow problem if and only if the residual network $G_\mathbf{x}$ has no negative cost (directed) cycle.
\end{restatable}
\begin{proof}
See Ahuja \textit{et al.}~\cite[p.~307]{Ahuja:1993}.
\end{proof}

\begin{thm}[Reduced cost optimality conditions] \label{thm:optimality-reduced-cost}
Let $\mathbf{x}$ be a (feasible) flow. It is an optimal solution to the minimum-cost flow problem if and only if there exists a vertex potential vector $\boldsymbol{\pi}$ such that that the reduced cost of each arc in the residual network $G_{\mathbf{x}}$ is non-negative:

\begin{equation} \label{eq:optimality-reduced-cost}
\forall(i,j)\in E_{\mathbf{x}}\cdot c_{ij}^{\boldsymbol{\pi}}\geq 0
\end{equation}
\end{thm}
\begin{proof}
See Ahuja \textit{et al.}~\cite[p.~309]{Ahuja:1993}.
\end{proof}

\begin{thm}[Complementary slackness optimality conditions] \label{thm:optimality-complementary-slackness}
Let $\mathbf{x}$ be a (feasible) flow. It is an optimal solution to the minimum-cost flow problem if and only if there exists a vertex potential vector $\boldsymbol{\pi}$ such that for every arc $(i,j)\in E$:

\normalfont % don't want text in maths italic
\begin{subequations} \label{eq:optimality-complementary-slackness}
\begin{align} 
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}>0}, } & \text{then \ensuremath{x_{ij}=0}} \\
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}<0}, } & \text{then \ensuremath{x_{ij}=u_{ij}}} \\
\text{if \ensuremath{c_{ij}^{\boldsymbol{\pi}}=0}, } & \text{then \ensuremath{0\leq x_{ij}\leq  u_{ij}}}
\end{align}
\end{subequations}
\end{thm}
\begin{proof}
The result follows immediately from expanding out \cref{eq:optimality-reduced-cost}, applying the definition of a residual network and performing a case analysis. A detailed proof is given by Ahuja \textit{et al.}~\cite[p.~310]{Ahuja:1993}.
\end{proof}

\subsubsection{Notation for complexity analysis} \label{sec:prep-flow-complexity}

% PROOFREAD: 1 minor edits.

Here, I introduce notation that will be useful for stating the asymptotic complexity of algorithms. Let $n=|V|$ and $m=|E|$ denote the number of vertices and arcs respectively.

The running time of algorithms may depend not just on the size of the network, but also on the magnitudes of input data. Let $U$ denote the largest vertex supply/demand, or arc capacity:
\begin{equation}
U=\max\left(\max\left\{ |b_{i}|\::\: i\in V\right\} ,\max\left\{ u_{ij}\::\:\left(i,j\right)\in E\right\} \right)
\end{equation}

and let $C$ denote the largest arc cost:

\begin{equation}
C=\max\left\{ c_{ij}\::\:(i,j)\in E\right\} 
\end{equation}

\section{Project management}

Before I could start to develop improved flow algorithms, it was necessary to understand the considerable body of prior work  summarised in \cref{sec:intro-related-work} and \cref{sec:prep-flow}. Consequently, reviewing the existing literature formed a key part of the preparatory stage of this project. 

Having mastered the background material, I began to refine the initial project proposal (see \cref{appendix:proposal}) into a concrete plan. To start with, I carefully reviewed the project's requirements, described in the next section. In the final two sections, I describe how I selected a development model and devised a testing strategy appropriate for the project's requirements.

\subsection{Requirements analysis} \label{sec:prep-management-requirements}

% SOMEDAY: Include dependency analysis?
% Matt did this and it was a big plus, but ultimately my dependence structure is much simpler; unsure if it's worth it?
% Implementation: strategy -> algorithms -> data structures
% Some common code, e.g. parsing.
% Evaluation: test harness -> simulator

% Requirements: based on success criteria and extensions in project proposal
% Requirements shown along with section satisfying them
% Quick summary?
% Core deliverables: 

The main goals for the project are listed in \cref{table:prep-project-requirements}, broadly following the success criteria and extensions suggested in the original project proposal.

% FIGURE: Make this prettier, not happy with "mid". Circles don't really work though.
\begin{table}
    \centering
    \begin{tabular}{clccc}
        \textbf{\#} & \textbf{Deliverable} & \textbf{Priority} & \textbf{Risk} & \textbf{Difficulty}
        \tabularnewline
        \hline
        & \textit{Success criteria} \tabularnewline
        S1 & Implement standard flow algorithms & High & Low & High \tabularnewline
        S2 & Design an approximate solver & High & Mid. & Low \tabularnewline
        S3 & Integrate system with Firmament & High & Low & Low \tabularnewline
        S4 & Develop a benchmark suite  & High & Low & Mid. \tabularnewline
        \hline
        & \textit{Optional extensions} \tabularnewline
        E1 & Design an incremental solver & Mid. & High & High \tabularnewline
        E2 & Build cluster simulator & Mid. & Mid. & Mid. \tabularnewline
        E3 & Optimise algorithm implementations & Low & Mid. & High \tabularnewline
        \hline
    \end{tabular}
    \caption{Project deliverables}
    \label{table:prep-project-requirements}
\end{table}

Core success criteria were assigned a high priority: work on extensions only began once all success criteria had been met (see \cref{sec:prep-implsched}). Most of the risk was concentrated in the approximate solver. This approach had not previously been attempted, so it was difficult to predict how it would perform.

I deemed extensions to be more risky. Although the incremental solution approach turned out to be highly successful (see \cref{sec:eval-incremental}), at the outset of the project it was unclear whether it was even possible to build such a solver. All extensions were rated medium or high difficulty, because of the considerable implementation work required.

\subsection{Development life cycle}
\label{sec:prep-management-model}

% Requirements mostly fixed
% Somewhat linear dependencies.
% But considerable uncertainty over what the best approach is: high risk.
% Want to 'fail early'.
% Build prototype of everything, including test infrastructure.
% Gradually build out algorithms, performing some early-stage testing.
% Investigate those which are most promising further, building out test code as needed.

% Different approach used for different parts of the system
% Standard algorithms: pretty much waterfall. You knew how they worked already.
% Test harness: iterative, were unclear which features were needed. But low risk. You knew how to make it work.
% Novel algorithms: evolutionary prototyping.

% Spiral model genuinely is the best option for you, and I think you can justify this.

% Intro of development models.
% Key goal: manage risk.
% Introduce Spiral process generator.
% Explain different stages of the process.
% Standard algorithms: low-risk, waterfall
% Test harness: incremental development, effectively sequences of waterfall
% New approaches: evolutionary prototyping

A software development life cycle or process provides structure to a project, dividing work into distinct phases to simplify management. This project has clearly defined requirements, suggesting a waterfall model. However, there was considerable uncertainty associated with the design of some deliverables: methods such as incremental development or evolutionary prototyping might be more appropriate in these cases. I adopted the \emph{spiral model}, a risk-driven life cycle \emph{generator}, which is able to accommodate such varying requirements.

The spiral model indicates that development of deliverables S1 and S3 in \cref{table:prep-project-requirements} should follow the \emph{waterfall} model: they are low risk, with precisely known requirements. By contrast, the test suite --- consisting of deliverables S4 and E2 --- used an \emph{incremental} development approach, with new tests added after other deliverables were completed.

Deliverables S2, E1 and E3 were all rated as medium or high risk. Their requirements were fairly well known, but there was significant uncertainty as to the appropriate design. \emph{Evolutionary prototyping} was followed in accordance with the spiral model, first constructing a robust prototype which is then repeatedly refined. Building a prototype early was invaluable for risk management: for example, it allowed the viability of the incremental solver (extension E1) to be confirmed before committing significant development time.

%FIGURE: Spiral model

\subsection{Testing strategy}
\label{sec:prep-management-testing}
I developed unit tests for each class. Where possible, I used the GTest framework (see \cref{appendix:tools-libraries}). However, testing of flow algorithms requires large external datasets: a Python test harness was developed to automate these tests. Integration tests with Firmament were also performed upon the completion of deliverables S1 and S3 (see \cref{sec:eval-test-correctness}).

Extensive performance evaluation was conducted. Anticipating this need, success criteria S4 and extension E2 deliver tools for benchmarking. Further details are provided in \cref{sec:impl-firmament}, \cref{sec:eval-benchmark-strategy} and \cref{appendix:impl-benchmark-harness}.

\section{Implementation schedule} \label{sec:prep-implsched}

%Split into phases:
%1. Standard algorithms.
%2. Testing and performance evaluation.
%3. Approximate solver.
%4. Incremental solver.
%5. Optimisations.
%Return to phase 2 after each stage. May need to extend it to support new test types. Show how this supports evolutionary prototyping, waterfall, incremental?
%
%Other ways of structuring it? Problems: phase 2 is kind of awkward. And standard algorithms weren't implemented in one pass, but rather as needed by particular solvers.
%1. Approximate solver.
%2. Testing and performance evaluation.
%3. Incremental solver.
%4. Optimisations.
%I kind of prefer this style, actually. It's more faithful to how the project actually proceeded, certainly.

% Segue in: previous section described high-level approach, now develop detailed plan for how to structure development.
% Outline the phases
% Justify the strategy
% Keep it simple

For a project of this scope, it is essential to plan the work before commencing development. An implementation schedule was devised, following the high-level strategy described in the preceding section, dividing the project into the following phases:

\begin{enumerate}
    \item \label{itm:phase-approximate}
        \textbf{Approximate solver} -- develop an algorithm to find approximate solutions to the MCF problem. Provided success criteria S1, S2 and S3; see \cref{sec:impl-approx}.
    \item \label{itm:phase-evaluation} 
        \textbf{Performance evaluation} -- run experiments to determine the speed-up offered by the system. This required development of a benchmark suite to facilitate testing, and provided deliverables S4 and E2; see \cref{chap:eval}.
    \item \label{itm:phase-incremental} 
        \textbf{Incremental solver} -- develop an algorithm solving the minimum-cost flow problem on sequences of related flow networks. The method reuses solutions to previous networks in the sequence to improve performance. Provided deliverable E1; see \cref{sec:impl-incremental}.
    \item \label{itm:phase-optimisations}
        \textbf{Optimisations} -- previous phases focused on improvements in \emph{algorithmic} techniques. In practice, performance is heavily dependent on the \emph{implementation} of the algorithm. Consequently, optimisations were necessary for the project to be competitive with existing solvers. Delivers extension E3; see \cref{sec:eval-optimisations}.
\end{enumerate}

All core success criteria were satisfied after the completion of the first two phases. This ensured the key goals of the project were met before work on extensions commenced. 

%Optimisation was postponed to the last phase. As \emph{evolutionary prototyping} was used for development of the approximate and incremental solvers (see \cref{sec:prep-management-model}), optimisation during phases \ref{itm:phase-approximate} or \ref{itm:phase-incremental} would have been premature. Leaving it to the final phase also ensures that time will only be spent on optimisation when all other goals have been met, appropriate as it is the lowest priority deliverable in \cref{table:prep-project-requirements}.
%
%Phase \ref{itm:phase-evaluation} is revisited upon the completion of phase \ref{itm:phase-incremental}, and after each optimisation in phase \ref{itm:phase-optimisations}. The development of the benchmark suite (see \cref{appendix:impl-benchmark-harness}) during phase \ref{itm:phase-evaluation} allows for the tests to be performed with little manual intervention, minimising the cost of revisiting this phase.

\section{Choice of tools}

\subsection{Languages} 

\subsubsection{C++}
I implemented the minimum-cost flow algorithms in C++. The performance of C++ is on par with low-level languages like C, with a number of excellent optimising compilers\footnotemark. At the same time, it adds support for object oriented programming and other features typical of high-level languages.
\footnotetext{Clang and GCC were used in this project.}

\subsubsection{Python and shell}
I used the Python scripting language for the development of test harnesses (see \cref{sec:eval-test-correctness} and \cref{appendix:impl-benchmark-harness}), and for generation of many of the figures appearing in \cref{chap:eval}. As a high-level language, Python aided productivity, although it was not used for any performance critical code. Shell scripting was used for particularly simple tasks, that involved plumbing together UNIX utilities. Python was favoured for anything more complicated, however, as it allows for a more structured programming style.

\subsection{Libraries} \label{sec:prep-tools-libraries}

Hapi uses two libraries developed at Google --- glog and gtest --- for logging and unit tests. I also extensively used the Boost family of libraries. See \cref{appendix:tools-libraries} for details of how I used the libraries, and a justification of my choice. 

\subsection{Flow scheduling}

The Firmament cluster scheduling platform is used for testing this project. The only other flow scheduling system in existence is Quincy, which pioneered the approach; however, Quincy is not publicly available.

\subsection{Revision control and backup strategy}

The Git distributed revision control system was used to manage the project source code, along with the {\LaTeX} code for this dissertation. Each Git working directory is a clone of the repository, complete with versioning history. The benchmark suite makes extensive use of this distributed nature of Git (see \cref{appendix:impl-benchmark-harness}).

To simplify testing, commits were automatically pushed to test machines in the SRG cluster, via a Git hook. Furthermore, changes were regularly pushed to GitHub, a popular repository hosting service.

Given the distributed nature of Git, having multiple working copies as described above automatically provides data replication. However, it would be possible for all copies of the repository to become corrupted due to user error, so this could not be relied upon as a backup strategy. Moreover, some files were not stored in Git\footnotemark.
\footnotetext{For example, test data files were often large, so had to be stored outside of Git.}

To protect against these threats, nightly backups were taken using a cron job. This took snapshots of the Git repository using \code{git-bundle}. Copies of other project files were made using \code{rdiff-backup}, which maintains a history of changes without the overhead of a full version control system like Git.

Copies of the resulting snapshots were stored on the university MCS and the Student Run Computing Facility (SRCF) in Cambridge. Additional off-site backups were made to Copy, a cloud storage service based in the US.

%FIGURE: Backup strategy

\section{Summary}

In this chapter, I have outlined the work undertaken prior to starting implementation for this project. I summarised the current state of the art cluster schedulers in \cref{sec:prep-scheduling}, and showed how flow scheduling can improve upon it. Next, I provided some elementary background on flow networks, and outlined how scheduling can be expressed in terms of a flow problem.

After having surveyed the research area, I turned to more practical considerations. I started by conducting a requirements analysis, identifying an appropriate software development life cycle taking into account the risk profile of the requirements. Next, I developed a concrete plan of implementation using the aforementioned techniques. Finally, I identified the tools I would use during implementation. In the next chapter, I describe the algorithms developed in this project.